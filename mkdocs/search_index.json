{
    "docs": [
        {
            "location": "/", 
            "text": "LeoFS, A Storage System for a Data Lake and the Web\n\n\nWhy LeoFS?\n\n\nLeoFS is a highly available, distributed, eventually consistent object/blob store. If you are searching a storage system that is able to store huge amount and various kind of raw data in its native format, LeoFS is suitable for that.\n\n\nLeoFS supports the following features:\n\n\n\n\nMulti protocols support\n\n\nRESTful Interface\n\n\nAmazon S3-API\n1\n\n\nA S3 compatible storage system\n\n\nSwitch to LeoFS to decrease your cost from more expensive public-cloud solutions.\n\n\n\n\n\n\nNFS v3\n2\n\n\n\n\n\n\nBuilt-in object cache feature\n\n\nMulti data center/cluster replication\n\n\nUser and bucket management\n\n\nData lake solution\n\n\nCloud solution integration\n\n\n\n\nAbout\n\n\nLeoFS provides High Cost Performance Ratio. It allows you to build LeoFS clusters using commodity hardware on top of the Linux operating system. LeoFS will provide very good performance even on commodity hardware. LeoFS will require a smaller cluster than other storage to achieve the same performance. LeoFS is also very easy to setup and to operate.\n\n\nLeoFS provides High Reliability thanks to its great design on top of the Erlang/OTP capabilities. Erlang/OTP is known for being used in production systems for years with a solid nine nines (99.9999999%) availability, and LeoFS is no exception. A LeoFS system will stay up regardless of software errors or hardware failures happening inside the cluster.\n\n\nLeoFS provides High Scalability. Adding and removing nodes is simple and quick, allowing you to react swiftly when your needs change. A LeoFS cluster can be thought as elastic storage that you can stretch as much and as often as you need.\n\n\n\n\nLeoFS consists of three components - LeoStorage, LeoGateway and LeoManager which depend on Erlang.\n\n\nLeoGateway\n handles http-request and http-response from any clients when using REST-API OR S3-API. Also, it is already built in the object-cache mechanism, memory and disk cache.\n\n\nLeoStorage\n handles GET, PUT and DELETE objects as well as metadata. Also, it has replicator, recoverer and queueing mechanism in order to keep running a storage node and realize eventual consistency.\n\n\nLeoManager\n always monitors LeoGateway and LeoStorage nodes. The main monitoring status are Node status and RING\u2019s checksum in order to realize to keep high availability and keep data consistency.\n\n\nGetting Started\n\n\nTo try LeoFS, see our \nGetting Started guides\n, and to learn more about LeoFS, see our \nArchitecture section\n.\n\n\nGoals\n\n\nWe have been aiming to achieve three things:\n\n\n\n\nHigh reliability and availability\n\n\nOperating ratios is 99.9999999%, nine nines\n\n\nKeeps maintaining high data and system availability while some node downed in a LeoFS' cluster\n\n\n\n\n\n\nHigh scalability\n\n\nBuilds a huge capacity cluster at low cost\n\n\nSupports a multi data center/cluster replication\n\n\n\n\n\n\nHigh cost performance ratio\n\n\nHigh performance with minimum resources\n\n\nProvides easy management and easy operation to users\n\n\n\n\n\n\n\n\nResources\n\n\n\n\nLeoFS Website\n\n\nLeoFS Repository on GitHub \n\n\nLeoFS Download Page\n\n\nLeoFS Package for CentOS and Ubuntu\n\n\nLeoFS Package for FreeBSD \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmazon S3 API\n\n\n\n\n\n\nWikipedia: Network File System", 
            "title": "Introduction"
        }, 
        {
            "location": "/#leofs-a-storage-system-for-a-data-lake-and-the-web", 
            "text": "", 
            "title": "LeoFS, A Storage System for a Data Lake and the Web"
        }, 
        {
            "location": "/#why-leofs", 
            "text": "LeoFS is a highly available, distributed, eventually consistent object/blob store. If you are searching a storage system that is able to store huge amount and various kind of raw data in its native format, LeoFS is suitable for that.  LeoFS supports the following features:   Multi protocols support  RESTful Interface  Amazon S3-API 1  A S3 compatible storage system  Switch to LeoFS to decrease your cost from more expensive public-cloud solutions.    NFS v3 2    Built-in object cache feature  Multi data center/cluster replication  User and bucket management  Data lake solution  Cloud solution integration", 
            "title": "Why LeoFS?"
        }, 
        {
            "location": "/#about", 
            "text": "LeoFS provides High Cost Performance Ratio. It allows you to build LeoFS clusters using commodity hardware on top of the Linux operating system. LeoFS will provide very good performance even on commodity hardware. LeoFS will require a smaller cluster than other storage to achieve the same performance. LeoFS is also very easy to setup and to operate.  LeoFS provides High Reliability thanks to its great design on top of the Erlang/OTP capabilities. Erlang/OTP is known for being used in production systems for years with a solid nine nines (99.9999999%) availability, and LeoFS is no exception. A LeoFS system will stay up regardless of software errors or hardware failures happening inside the cluster.  LeoFS provides High Scalability. Adding and removing nodes is simple and quick, allowing you to react swiftly when your needs change. A LeoFS cluster can be thought as elastic storage that you can stretch as much and as often as you need.   LeoFS consists of three components - LeoStorage, LeoGateway and LeoManager which depend on Erlang.  LeoGateway  handles http-request and http-response from any clients when using REST-API OR S3-API. Also, it is already built in the object-cache mechanism, memory and disk cache.  LeoStorage  handles GET, PUT and DELETE objects as well as metadata. Also, it has replicator, recoverer and queueing mechanism in order to keep running a storage node and realize eventual consistency.  LeoManager  always monitors LeoGateway and LeoStorage nodes. The main monitoring status are Node status and RING\u2019s checksum in order to realize to keep high availability and keep data consistency.", 
            "title": "About"
        }, 
        {
            "location": "/#getting-started", 
            "text": "To try LeoFS, see our  Getting Started guides , and to learn more about LeoFS, see our  Architecture section .", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#goals", 
            "text": "We have been aiming to achieve three things:   High reliability and availability  Operating ratios is 99.9999999%, nine nines  Keeps maintaining high data and system availability while some node downed in a LeoFS' cluster    High scalability  Builds a huge capacity cluster at low cost  Supports a multi data center/cluster replication    High cost performance ratio  High performance with minimum resources  Provides easy management and easy operation to users", 
            "title": "Goals"
        }, 
        {
            "location": "/#resources", 
            "text": "LeoFS Website  LeoFS Repository on GitHub   LeoFS Download Page  LeoFS Package for CentOS and Ubuntu  LeoFS Package for FreeBSD          Amazon S3 API    Wikipedia: Network File System", 
            "title": "Resources"
        }, 
        {
            "location": "/whats_new/", 
            "text": "What's New?\n\n\nThis page lists the new features and enhancements in the v1.3 series of LeoFS.\n\n\nRelease v1.3.3\n\n\nUpdated Contents of LeoFS Package for CentOS and Ubuntu\n\n\n\n\nLeoFS package is generated by \nleofs_package\n which requires the system to run LeoFS as non-privileged user \nleofs\n.\n\n\n\n\nRelated Links\n\n\n\n\nv1.3.3's Change log\n\n\nFor Administrators / Settings / Environment Configuration\n\n\nFor Administrators / System Administration / System Migration\n\n\n\n\nRelease v1.3.2\n\n\nAWS SDK for Python, Boto3\n support\n\n\n\n\nAccording to a user's request, we have added Boto3 to LeoFS' client support.\n\n\n\n\nRelated Links\n\n\n\n\nv1.3.2's Change log\n\n\nBoto3 of LeoFS' client test\n\n\n\n\nRelease v1.3.1\n\n\nUser-defined metadata support\n\n\nIn order to provide optional information as a key-value pair into a metadata when you send a PUT request to store an object, you're able to set \nuse user-defined metadata\n of the object.\n\n\nRelated Links\n\n\n\n\nv1.3.1's Change log\n\n\nS3 / Developer Guide / Working with Amazon S3 Objects/Object Key and Metadata\n\n\n\n\nRelease v1.3.0\n\n\nAWS Signature v4 support\n\n\nIn order to cover latest AWS SDKs which includes \nGo\n, \nJava\n and others, we supported AWS Signature v4 with v1.3.0.\n\n\nRelated Links\n\n\n\n\nv1.3.0's Change log\n\n\nAWS General Reference / Signing AWS API Requests / Signature Version 4 Signing Process", 
            "title": "What's New?"
        }, 
        {
            "location": "/whats_new/#whats-new", 
            "text": "This page lists the new features and enhancements in the v1.3 series of LeoFS.", 
            "title": "What's New?"
        }, 
        {
            "location": "/whats_new/#release-v133", 
            "text": "", 
            "title": "Release v1.3.3"
        }, 
        {
            "location": "/whats_new/#updated-contents-of-leofs-package-for-centos-and-ubuntu", 
            "text": "LeoFS package is generated by  leofs_package  which requires the system to run LeoFS as non-privileged user  leofs .", 
            "title": "Updated Contents of LeoFS Package for CentOS and Ubuntu"
        }, 
        {
            "location": "/whats_new/#related-links", 
            "text": "v1.3.3's Change log  For Administrators / Settings / Environment Configuration  For Administrators / System Administration / System Migration", 
            "title": "Related Links"
        }, 
        {
            "location": "/whats_new/#release-v132", 
            "text": "", 
            "title": "Release v1.3.2"
        }, 
        {
            "location": "/whats_new/#aws-sdk-for-python-boto3-support", 
            "text": "According to a user's request, we have added Boto3 to LeoFS' client support.", 
            "title": "AWS SDK for Python, Boto3 support"
        }, 
        {
            "location": "/whats_new/#related-links_1", 
            "text": "v1.3.2's Change log  Boto3 of LeoFS' client test", 
            "title": "Related Links"
        }, 
        {
            "location": "/whats_new/#release-v131", 
            "text": "", 
            "title": "Release v1.3.1"
        }, 
        {
            "location": "/whats_new/#user-defined-metadata-support", 
            "text": "In order to provide optional information as a key-value pair into a metadata when you send a PUT request to store an object, you're able to set  use user-defined metadata  of the object.", 
            "title": "User-defined metadata support"
        }, 
        {
            "location": "/whats_new/#related-links_2", 
            "text": "v1.3.1's Change log  S3 / Developer Guide / Working with Amazon S3 Objects/Object Key and Metadata", 
            "title": "Related Links"
        }, 
        {
            "location": "/whats_new/#release-v130", 
            "text": "", 
            "title": "Release v1.3.0"
        }, 
        {
            "location": "/whats_new/#aws-signature-v4-support", 
            "text": "In order to cover latest AWS SDKs which includes  Go ,  Java  and others, we supported AWS Signature v4 with v1.3.0.", 
            "title": "AWS Signature v4 support"
        }, 
        {
            "location": "/whats_new/#related-links_3", 
            "text": "v1.3.0's Change log  AWS General Reference / Signing AWS API Requests / Signature Version 4 Signing Process", 
            "title": "Related Links"
        }, 
        {
            "location": "/installation/quick/", 
            "text": "Quick Installation and Setup\n\n\nPurpose\n\n\nThis section is a step by step guide to setting up LeoFS for the first time. By following this tutorial you're able to easily build a stand-alone LeoFS system.\n\n\nNote\n\n\nIn this section, you install LeoStorage, LeoGateway and LeoManager on a single system with no clustering to quickly understand LeoFS.\n\n\nInstallation v1.3.3 or Later\n\n\nUbuntu\n\n\nFor Ubuntu distributions, perform the following steps:\n\n\n\n\nDownloads a LeoFS' package from \nLeoFS' repository\n on GitHub\n\n\nInstalls a LeoFS using \ndpkg\n\n\n\n\nFor Ubuntu 16.04\n\n\n1\n2\n3\n4\n5\n$ wget https://github.com/leo-project/leofs/releases/download/{VERSION}/leofs_{VERSION}-1_erl-18.3_ubuntu-16.04_amd64.deb\n$ sudo dpkg -i leofs_{VERSION}-1_erl-18.3_ubuntu-16.04_amd64.deb\n$ ls -l /usr/local/leofs/\ntotal 4\ndrwxr-xr-x 6 root   root   4096 Jan 20 00:00 {VERSION}\n\n\n\n\n\n\nFor Ubuntu 14.04\n\n\n1\n2\n3\n4\n5\n$ wget https://github.com/leo-project/leofs/releases/download/{VERSION}/leofs_{VERSION}-1_erl-18.3_ubuntu-14.04_amd64.deb\n$ sudo dpkg -i leofs_{VERSION}-1_erl-18.3_ubuntu-14.04_amd64.deb\n$ ls -l /usr/local/leofs/\ntotal 4\ndrwxr-xr-x 6 root   root   4096 Jan 20 00:00 {VERSION}\n\n\n\n\n\n\nCentOS\n\n\nFor CentOS distributions, perform the following steps:\n\n\n\n\nDownloads a LeoFS' package from \nLeoFS' repository\n on GitHub\n\n\nInstalls a LeoFS using \nrpm\n\n\n\n\nFor CentOS 6.x\n\n\n1\n2\n3\n4\n5\n$ wget https://github.com/leo-project/leofs/releases/download/{VERSION}/leofs-{VERSION}-1.erl-18.3.el6.x86_64.rpm\n$ sudo rpm -ivh leofs-{VERSION}-1.erl-18.3.el6.x86_64.rpm\n$ ls -l /usr/local/leofs/\ntotal 4\ndrwxr-xr-x 6 root   root   4096 Jan 20 15:37 {VERSION}\n\n\n\n\n\n\nFor CentOS 7.x\n\n\n1\n2\n3\n4\n5\n$ wget https://github.com/leo-project/leofs/releases/download/{VERSION}/leofs-{VERSION}-1.erl-18.3.el7.x86_64.rpm\n$ sudo rpm -ivh leofs-{VERSION}-1.erl-18.3.el7.x86_64.rpm\n$ ls -l /usr/local/leofs/\ntotal 4\ndrwxr-xr-x 6 root   root   4096 Jan 20 15:37 {VERSION}\n\n\n\n\n\n\nConfiguration\n\n\nTo be able to access the LeoFS storage system, you need to edit \n/etc/hosts\n which adheres to \nrules for bucket naming of S3-API\n.\n\n\nModifies \u201c/etc/hosts\u201d\n\n\n\n\nAdds a domain for the LeoFS bucket in /etc/hosts\n\n\nBucket names must follow \nthese rules\n\n\n\n\n1\n2\n3\n## Replace {BUCKET_NAME} with the name of the bucket ##\n$ sudo vi /etc/hosts\n127.0.0.1 localhost {BUCKET_NAME}.localhost\n\n\n\n\n\n\nLaunches LeoManager, LeoStorage, and LeoGateway node(s)\n\n\nYou launch the LeoFS storage system by the following steps:\n\n\n\n\nThere is only single replica by \nthe default configuration of \nconsistency.num_of_replicas\n\n\nStarts both \nLeoManager master\n and \nLeoManager slave\n\n\nStarts a \nLeoStorage\n\n\nStarts a \nLeoGateway\n\n\n\n\n1\n2\n3\n4\n5\n$ cd /usr/local/leofs/{VERSION}\n$ leo_manager_0/bin/leo_manager start\n$ leo_manager_1/bin/leo_manager start\n$ leo_storage/bin/leo_storage start\n$ leo_gateway/bin/leo_gateway start\n\n\n\n\n\n\nThen confirms whether the LeoManager nodes and the LeoStorage are running or not with the \nleofs-adm status\n command.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n$ leofs-adm status\n [System Confiuration]\n-----------------------------------+----------\n Item                              | Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version | 1.3.3\n                        cluster Id | leofs_1\n                             DC Id | dc_1\n                    Total replicas | 1\n          number of successes of R | 1\n          number of successes of W | 1\n          number of successes of D | 1\n number of rack-awareness replicas | 0\n                         ring size | 2^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n [mdcr] max number of joinable DCs | 2\n [mdcr] total replicas per a DC    | 1\n [mdcr] number of successes of R   | 1\n [mdcr] number of successes of W   | 1\n [mdcr] number of successes of D   | 1\n-----------------------------------+----------\n Manager RING hash\n-----------------------------------+----------\n                 current ring-hash | 433fe365\n                previous ring-hash | 433fe365\n-----------------------------------+----------\n\n [State of Node(s)]\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n type  |           node           |    state     |  current ring  |   prev ring    |          updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S    | storage_0@127.0.0.1      | attached     |                |                | 2017-05-01 00:43:06 +0000\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n\n\n\n\n\nStarts the LeoFS storage system\n\n\nIf there is no issue, you're able to execute the \nleofs-adm start\n commmand to launch it, then confirm the current status with the \nleof-adm status\n command.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n$ ./leofs-adm start\nGenerating RING...\nGenerated RING\nOK 100% - storage_0@127.0.0.1\nOK\n\n$ leofs-adm status\n [System Confiuration]\n-----------------------------------+----------\n Item                              | Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version | 1.3.3\n                        cluster Id | leofs_1\n                             DC Id | dc_1\n                    Total replicas | 1\n          number of successes of R | 1\n          number of successes of W | 1\n          number of successes of D | 1\n number of rack-awareness replicas | 0\n                         ring size | 2^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n [mdcr] max number of joinable DCs | 2\n [mdcr] total replicas per a DC    | 1\n [mdcr] number of successes of R   | 1\n [mdcr] number of successes of W   | 1\n [mdcr] number of successes of D   | 1\n-----------------------------------+----------\n Manager RING hash\n-----------------------------------+----------\n                 current ring-hash | 433fe365\n                previous ring-hash | 433fe365\n-----------------------------------+----------\n\n [State of Node(s)]\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n type  |           node           |    state     |  current ring  |   prev ring    |          updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S    | storage_0@127.0.0.1      | running      | 433fe365       | 433fe365       | 2017-05-01 00:43:06 +0000\n  G    | gateway_0@127.0.0.1      | running      | 433fe365       | 433fe365       | 2017-05-01 00:43:08 +0000\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n\n\n\n\n\nRetrieves both an access-key and a secret accesskey\n\n\nTo make a bucket for the test, you need to retrieve both an \naccess-key\n and a \nsecret access-key\n with the \nleofs-adm create-user\n command\n\n\n1\n2\n3\n$ leofs-adm create-user \nYOUR-ID\n\naccess-key-id: \nACCESS-KEY-ID\n\nsecret-access-key: \nSECRET-ACCESS-KEY-ID\n\n\n\n\n\n\n\nAfter that, you use S3-client(s) with those keys when you access the LeoFS storage system.\n\n\nUses\ns3cmd\n to access it\n\n\nIf you'd like to use \ns3cmd\n to access the LeoFS storage system, perform the following steps:\n\n\n\n\nInstalls \ns3cmd\n on your machine\n\n\nConfigures \ns3cmd\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n$ s3cmd --version\ns3cmd version \n1\n.6.1\n\n$ s3cmd --configure\n...\n\n## access_key = \naccess-key-id\n\n\n## secret_key = \nsecret-access-key\n\n\n## proxy_host = localhost\n\n\n## proxy_port = 8080\n\n\n\n\n\n\n\n\n\nCreates a bucket\n\n\n\n\n1\n2\n$ s3cmd mb \nBUCKET\n\nBucket \ns3://\nBUCKET\n/\n created\n\n\n\n\n\n\n\n\nPuts an object into the bucket\n\n\n\n\n1\n2\n3\n$ s3cmd put /path/to/\nOBJECT\n s3://\nBUCKET\n/\nupload: \nOBJET\n -\n \ns3://\nBUCKET\n/\nOBJECT\n  \n[\n1\n of \n1\n]\n\n \n1096\n of \n1096\n   \n100\n% in    0s   \n170\n.92 kB/s  \ndone\n\n\n\n\n\n\n\n\n\nGets an object\n\n\n\n\n1\n2\n3\n$ s3cmd get s3://\nBUCKET\n/\nOBJECT\n\ndownload: \ns3://\nBUCKET\n/\nOBJET\n -\n \n./\nOBJECT\n  \n[\n1\n of \n1\n]\n\n \n1096\n of \n1096\n   \n100\n% in    0s   \n307\n.38 kB/s  \ndone\n\n\n\n\n\n\n\n\n\nLists objects\n\n\n\n\n1\n2\n$ s3cmd ls s3://\nBUCKET\n/\n\n2017\n-01-30 \n02\n:24      \n1096\n   s3://\nBUCKET\n/\nOBJECT\n\n\n\n\n\n\n\n\n\nRemoves an object\n\n\n\n\n1\n2\n$ s3cmd del s3://\nBUCKET\n/\nOBJECT\n\ndelete: \ns3://\nBUCKET\n/\nOBJECT\n\n\n\n\n\n\n\nWrap up\n\n\nYou now know how to setup a stand-alone LeoFS storage system, and make sure to have a look at \nBuilding a LeoFS' cluster with Ansible\n to learn how to setup a LeoFS cluster.\n\n\nRelated Links\n\n\n\n\nGetting Started / Building a LeoFS' cluster with Ansible\n\n\nFor Administrators / Settings / Environment Configuration\n\n\nFor Administrators / Settings / LeoManager Settings\n\n\nFor Administrators / Settings / LeoStorage Settings\n\n\nFor Administrators / Settings / LeoGateway Settings", 
            "title": "Quick Installation and Setup"
        }, 
        {
            "location": "/installation/quick/#quick-installation-and-setup", 
            "text": "", 
            "title": "Quick Installation and Setup"
        }, 
        {
            "location": "/installation/quick/#purpose", 
            "text": "This section is a step by step guide to setting up LeoFS for the first time. By following this tutorial you're able to easily build a stand-alone LeoFS system.", 
            "title": "Purpose"
        }, 
        {
            "location": "/installation/quick/#note", 
            "text": "In this section, you install LeoStorage, LeoGateway and LeoManager on a single system with no clustering to quickly understand LeoFS.", 
            "title": "Note"
        }, 
        {
            "location": "/installation/quick/#installation-v133-or-later", 
            "text": "", 
            "title": "Installation v1.3.3 or Later"
        }, 
        {
            "location": "/installation/quick/#ubuntu", 
            "text": "For Ubuntu distributions, perform the following steps:   Downloads a LeoFS' package from  LeoFS' repository  on GitHub  Installs a LeoFS using  dpkg", 
            "title": "Ubuntu"
        }, 
        {
            "location": "/installation/quick/#for-ubuntu-1604", 
            "text": "1\n2\n3\n4\n5 $ wget https://github.com/leo-project/leofs/releases/download/{VERSION}/leofs_{VERSION}-1_erl-18.3_ubuntu-16.04_amd64.deb\n$ sudo dpkg -i leofs_{VERSION}-1_erl-18.3_ubuntu-16.04_amd64.deb\n$ ls -l /usr/local/leofs/\ntotal 4\ndrwxr-xr-x 6 root   root   4096 Jan 20 00:00 {VERSION}", 
            "title": "For Ubuntu 16.04"
        }, 
        {
            "location": "/installation/quick/#for-ubuntu-1404", 
            "text": "1\n2\n3\n4\n5 $ wget https://github.com/leo-project/leofs/releases/download/{VERSION}/leofs_{VERSION}-1_erl-18.3_ubuntu-14.04_amd64.deb\n$ sudo dpkg -i leofs_{VERSION}-1_erl-18.3_ubuntu-14.04_amd64.deb\n$ ls -l /usr/local/leofs/\ntotal 4\ndrwxr-xr-x 6 root   root   4096 Jan 20 00:00 {VERSION}", 
            "title": "For Ubuntu 14.04"
        }, 
        {
            "location": "/installation/quick/#centos", 
            "text": "For CentOS distributions, perform the following steps:   Downloads a LeoFS' package from  LeoFS' repository  on GitHub  Installs a LeoFS using  rpm", 
            "title": "CentOS"
        }, 
        {
            "location": "/installation/quick/#for-centos-6x", 
            "text": "1\n2\n3\n4\n5 $ wget https://github.com/leo-project/leofs/releases/download/{VERSION}/leofs-{VERSION}-1.erl-18.3.el6.x86_64.rpm\n$ sudo rpm -ivh leofs-{VERSION}-1.erl-18.3.el6.x86_64.rpm\n$ ls -l /usr/local/leofs/\ntotal 4\ndrwxr-xr-x 6 root   root   4096 Jan 20 15:37 {VERSION}", 
            "title": "For CentOS 6.x"
        }, 
        {
            "location": "/installation/quick/#for-centos-7x", 
            "text": "1\n2\n3\n4\n5 $ wget https://github.com/leo-project/leofs/releases/download/{VERSION}/leofs-{VERSION}-1.erl-18.3.el7.x86_64.rpm\n$ sudo rpm -ivh leofs-{VERSION}-1.erl-18.3.el7.x86_64.rpm\n$ ls -l /usr/local/leofs/\ntotal 4\ndrwxr-xr-x 6 root   root   4096 Jan 20 15:37 {VERSION}", 
            "title": "For CentOS 7.x"
        }, 
        {
            "location": "/installation/quick/#configuration", 
            "text": "To be able to access the LeoFS storage system, you need to edit  /etc/hosts  which adheres to  rules for bucket naming of S3-API .", 
            "title": "Configuration"
        }, 
        {
            "location": "/installation/quick/#modifies-etchosts", 
            "text": "Adds a domain for the LeoFS bucket in /etc/hosts  Bucket names must follow  these rules   1\n2\n3 ## Replace {BUCKET_NAME} with the name of the bucket ##\n$ sudo vi /etc/hosts\n127.0.0.1 localhost {BUCKET_NAME}.localhost", 
            "title": "Modifies \u201c/etc/hosts\u201d"
        }, 
        {
            "location": "/installation/quick/#launches-leomanager-leostorage-and-leogateway-nodes", 
            "text": "You launch the LeoFS storage system by the following steps:   There is only single replica by  the default configuration of  consistency.num_of_replicas  Starts both  LeoManager master  and  LeoManager slave  Starts a  LeoStorage  Starts a  LeoGateway   1\n2\n3\n4\n5 $ cd /usr/local/leofs/{VERSION}\n$ leo_manager_0/bin/leo_manager start\n$ leo_manager_1/bin/leo_manager start\n$ leo_storage/bin/leo_storage start\n$ leo_gateway/bin/leo_gateway start   Then confirms whether the LeoManager nodes and the LeoStorage are running or not with the  leofs-adm status  command.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37 $ leofs-adm status\n [System Confiuration]\n-----------------------------------+----------\n Item                              | Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version | 1.3.3\n                        cluster Id | leofs_1\n                             DC Id | dc_1\n                    Total replicas | 1\n          number of successes of R | 1\n          number of successes of W | 1\n          number of successes of D | 1\n number of rack-awareness replicas | 0\n                         ring size | 2^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n [mdcr] max number of joinable DCs | 2\n [mdcr] total replicas per a DC    | 1\n [mdcr] number of successes of R   | 1\n [mdcr] number of successes of W   | 1\n [mdcr] number of successes of D   | 1\n-----------------------------------+----------\n Manager RING hash\n-----------------------------------+----------\n                 current ring-hash | 433fe365\n                previous ring-hash | 433fe365\n-----------------------------------+----------\n\n [State of Node(s)]\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n type  |           node           |    state     |  current ring  |   prev ring    |          updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S    | storage_0@127.0.0.1      | attached     |                |                | 2017-05-01 00:43:06 +0000\n-------+--------------------------+--------------+----------------+----------------+----------------------------", 
            "title": "Launches LeoManager, LeoStorage, and LeoGateway node(s)"
        }, 
        {
            "location": "/installation/quick/#starts-the-leofs-storage-system", 
            "text": "If there is no issue, you're able to execute the  leofs-adm start  commmand to launch it, then confirm the current status with the  leof-adm status  command.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44 $ ./leofs-adm start\nGenerating RING...\nGenerated RING\nOK 100% - storage_0@127.0.0.1\nOK\n\n$ leofs-adm status\n [System Confiuration]\n-----------------------------------+----------\n Item                              | Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version | 1.3.3\n                        cluster Id | leofs_1\n                             DC Id | dc_1\n                    Total replicas | 1\n          number of successes of R | 1\n          number of successes of W | 1\n          number of successes of D | 1\n number of rack-awareness replicas | 0\n                         ring size | 2^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n [mdcr] max number of joinable DCs | 2\n [mdcr] total replicas per a DC    | 1\n [mdcr] number of successes of R   | 1\n [mdcr] number of successes of W   | 1\n [mdcr] number of successes of D   | 1\n-----------------------------------+----------\n Manager RING hash\n-----------------------------------+----------\n                 current ring-hash | 433fe365\n                previous ring-hash | 433fe365\n-----------------------------------+----------\n\n [State of Node(s)]\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n type  |           node           |    state     |  current ring  |   prev ring    |          updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S    | storage_0@127.0.0.1      | running      | 433fe365       | 433fe365       | 2017-05-01 00:43:06 +0000\n  G    | gateway_0@127.0.0.1      | running      | 433fe365       | 433fe365       | 2017-05-01 00:43:08 +0000\n-------+--------------------------+--------------+----------------+----------------+----------------------------", 
            "title": "Starts the LeoFS storage system"
        }, 
        {
            "location": "/installation/quick/#retrieves-both-an-access-key-and-a-secret-accesskey", 
            "text": "To make a bucket for the test, you need to retrieve both an  access-key  and a  secret access-key  with the  leofs-adm create-user  command  1\n2\n3 $ leofs-adm create-user  YOUR-ID \naccess-key-id:  ACCESS-KEY-ID \nsecret-access-key:  SECRET-ACCESS-KEY-ID    After that, you use S3-client(s) with those keys when you access the LeoFS storage system.", 
            "title": "Retrieves both an access-key and a secret accesskey"
        }, 
        {
            "location": "/installation/quick/#usess3cmd-to-access-it", 
            "text": "If you'd like to use  s3cmd  to access the LeoFS storage system, perform the following steps:   Installs  s3cmd  on your machine  Configures  s3cmd   1\n2\n3\n4\n5\n6\n7\n8\n9 $ s3cmd --version\ns3cmd version  1 .6.1\n\n$ s3cmd --configure\n... ## access_key =  access-key-id  ## secret_key =  secret-access-key  ## proxy_host = localhost  ## proxy_port = 8080     Creates a bucket   1\n2 $ s3cmd mb  BUCKET \nBucket  s3:// BUCKET /  created    Puts an object into the bucket   1\n2\n3 $ s3cmd put /path/to/ OBJECT  s3:// BUCKET /\nupload:  OBJET  -   s3:// BUCKET / OBJECT    [ 1  of  1 ] \n  1096  of  1096     100 % in    0s    170 .92 kB/s   done     Gets an object   1\n2\n3 $ s3cmd get s3:// BUCKET / OBJECT \ndownload:  s3:// BUCKET / OBJET  -   ./ OBJECT    [ 1  of  1 ] \n  1096  of  1096     100 % in    0s    307 .38 kB/s   done     Lists objects   1\n2 $ s3cmd ls s3:// BUCKET / 2017 -01-30  02 :24       1096    s3:// BUCKET / OBJECT     Removes an object   1\n2 $ s3cmd del s3:// BUCKET / OBJECT \ndelete:  s3:// BUCKET / OBJECT", 
            "title": "Usess3cmd to access it"
        }, 
        {
            "location": "/installation/quick/#wrap-up", 
            "text": "You now know how to setup a stand-alone LeoFS storage system, and make sure to have a look at  Building a LeoFS' cluster with Ansible  to learn how to setup a LeoFS cluster.", 
            "title": "Wrap up"
        }, 
        {
            "location": "/installation/quick/#related-links", 
            "text": "Getting Started / Building a LeoFS' cluster with Ansible  For Administrators / Settings / Environment Configuration  For Administrators / Settings / LeoManager Settings  For Administrators / Settings / LeoStorage Settings  For Administrators / Settings / LeoGateway Settings", 
            "title": "Related Links"
        }, 
        {
            "location": "/installation/cluster/", 
            "text": "Building a LeoFS' cluster with Ansible\n\n\nPurpose\n\n\nThis tutorial teaches you how to easily build a LeoFS cluster. All steps will not be explained in detail, it is assumed you already know \nhow to setup a stand-alone LeoFS system\n. This guide exists to help you get a cluster up and running quickly. We recommend that you read the LeoFS Configuration and Administration Guide to learn how to administer your LeoFS cluster. We hope that by reading this tutorial you will be able to get a cluster started as quickly as possible.\n\n\nInstalls and Launches LeoFS with \nleofs_ansible\n\n\nYou can easily install LeoFS into your servers with using \nleofs_ansible\n, perform following steps:\n\n\nUses \nleofs_ansible\n\n\n\n\nInstalls and sets up \nAnsible\n\n\nleofs_ansible's documentation\n is already published, you can follow it to install and launch a LeoFS storage system.\n\n\n\n\nAn example of \nhosts\n\n\nManager\n\n\n\n\nA number of nodes: 2\n\n\nIP: 10.0.0.101, 10.0.0.102\n\n\nName: M0@10.0.0.101, M1@10.0.0.102\n\n\n\n\nGateway\n\n\n\n\nA number of nodes: 1\n\n\nIP: 10.0.0.103\n\n\nName: G0@10.0.0.103\n\n\n\n\nStorage\n\n\n\n\nA number of nodes: 3\n\n\nIP: 10.0.0.104 .. 10.0.0.106\n\n\nName: S0@10.0.0.104 .. S2@10.0.0.106\n\n\n\n\nIn this case, we configure basic properties and nodes of LeoManager, LeoStorage an LeoGateway. You need to configure those properties to suit your environment.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n##\n## Please check roles/common/vars/leofs_releases for available versions\n##\n[all:vars]\nleofs_version=1.3.2\nbuild_temp_path=\n/tmp/leofs_builder\n\nbuild_install_path=\n/tmp/\n\nbuild_branch=\nmaster\n\nsource=\npackage\n\n\n[builder]\n10.0.0.100\n\n# nodename of leo_manager_0 and leo_manager_1 are set at group_vars/all\n[leo_manager_0]\n10.0.0.101\n\n# nodename of leo_manager_0 and leo_manager_1 are set at group_vars/all\n[leo_manager_1]\n10.0.0.102\n\n[leo_storage]\n10.0.0.104 leofs_module_nodename=S0@10.0.0.104\n10.0.0.105 leofs_module_nodename=S1@10.0.0.105\n10.0.0.106 leofs_module_nodename=S2@10.0.0.106\n\n[leo_gateway]\n10.0.0.103 leofs_module_nodename=G0@10.0.0.103\n\n[leofs_nodes:children]\nleo_manager_0\nleo_manager_1\nleo_gateway\nleo_storage\n\n\n\n\n\n\nConfirmation\n\n\nUses the \nleofs-adm status\n command to confirm current its status.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n$\n \nleofs\n-\nadm\n \nstatus\n\n \n[\nSystem\n \nConfiuration\n]\n\n\n-----------------------------------+----------\n\n \nItem\n                              \n|\n \nValue\n\n\n-----------------------------------+----------\n\n \nBasic\n/\nConsistency\n \nlevel\n\n\n-----------------------------------+----------\n\n                    \nsystem\n \nversion\n \n|\n \n1.3.2\n\n                        \ncluster\n \nId\n \n|\n \nleofs_1\n\n                             \nDC\n \nId\n \n|\n \ndc_1\n\n                    \nTotal\n \nreplicas\n \n|\n \n2\n\n          \nnumber\n \nof\n \nsuccesses\n \nof\n \nR\n \n|\n \n1\n\n          \nnumber\n \nof\n \nsuccesses\n \nof\n \nW\n \n|\n \n1\n\n          \nnumber\n \nof\n \nsuccesses\n \nof\n \nD\n \n|\n \n1\n\n \nnumber\n \nof\n \nrack\n-\nawareness\n \nreplicas\n \n|\n \n0\n\n                         \nring\n \nsize\n \n|\n \n2\n^\n128\n\n\n-----------------------------------+----------\n\n \nMulti\n \nDC\n \nreplication\n \nsettings\n\n\n-----------------------------------+----------\n\n        \nmax\n \nnumber\n \nof\n \njoinable\n \nDCs\n \n|\n \n2\n\n           \nnumber\n \nof\n \nreplicas\n \na\n \nDC\n \n|\n \n1\n\n\n-----------------------------------+----------\n\n \nManager\n \nRING\n \nhash\n\n\n-----------------------------------+----------\n\n                 \ncurrent\n \nring\n-\nhash\n \n|\n \n3923\nd007\n\n                \nprevious\n \nring\n-\nhash\n \n|\n \n3923\nd007\n\n\n-----------------------------------+----------\n\n\n \n[\nState\n \nof\n \nNode\n(\ns\n)]\n\n\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n \ntype\n  \n|\n           \nnode\n           \n|\n    \nstate\n     \n|\n  \ncurrent\n \nring\n  \n|\n   \nprev\n \nring\n    \n|\n          \nupdated\n \nat\n\n\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n  \nS\n    \n|\n \nS0\n@10.0.0.104\n            \n|\n \nrunning\n      \n|\n \n3923\nd007\n       \n|\n \n3923\nd007\n       \n|\n \n2017\n-\n01\n-\n30\n \n12\n:\n32\n:\n48\n \n+\n0900\n\n  \nS\n    \n|\n \nS1\n@10.0.0.105\n            \n|\n \nrunning\n      \n|\n \n3923\nd007\n       \n|\n \n3923\nd007\n       \n|\n \n2017\n-\n01\n-\n30\n \n12\n:\n32\n:\n48\n \n+\n0900\n\n  \nS\n    \n|\n \nS2\n@10.0.0.106\n            \n|\n \nrunning\n      \n|\n \n3923\nd007\n       \n|\n \n3923\nd007\n       \n|\n \n2017\n-\n01\n-\n30\n \n12\n:\n32\n:\n48\n \n+\n0900\n\n  \nG\n    \n|\n \nG0\n@10.0.0.103\n            \n|\n \nrunning\n      \n|\n \n3923\nd007\n       \n|\n \n3923\nd007\n       \n|\n \n2017\n-\n01\n-\n30\n \n12\n:\n32\n:\n52\n \n+\n0900\n\n\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n\n\n\n\n\n\nRetrieves both an access-key and a secret accesskey\n\n\nTo make a bucket for the test, you need to retrieve both an \naccess-key\n and a \nsecret access-key\n with the \nleofs-adm create-user\n command\n\n\n1\n2\n3\n$ leofs-adm create-user \nYOUR-ID\n\naccess-key-id: \nACCESS-KEY-ID\n\nsecret-access-key: \nSECRET-ACCESS-KEY-ID\n\n\n\n\n\n\n\nAfter that, you use S3-client(s) with those keys when you access the LeoFS storage system.\n\n\nUses\ns3cmd\n to access it\n\n\nIf you'd like to use \ns3cmd\n to access the LeoFS storage system, perform the following steps:\n\n\n\n\nInstalls \ns3cmd\n on your machine\n\n\nConfigures \ns3cmd\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n$ s3cmd --version\ns3cmd version \n1\n.6.1\n\n$ s3cmd --configure\n...\n\n## access_key = \naccess-key-id\n\n\n## secret_key = \nsecret-access-key\n\n\n## proxy_host = localhost\n\n\n## proxy_port = 8080\n\n\n\n\n\n\n\n\n\nCreates a bucket\n\n\n\n\n1\n2\n$ s3cmd mb \nBUCKET\n\nBucket \ns3://\nBUCKET\n/\n created\n\n\n\n\n\n\n\n\nPuts an object into the bucket\n\n\n\n\n1\n2\n3\n$ s3cmd put /path/to/\nOBJECT\n s3://\nBUCKET\n/\nupload: \nOBJET\n -\n \ns3://\nBUCKET\n/\nOBJECT\n  \n[\n1\n of \n1\n]\n\n \n1096\n of \n1096\n   \n100\n% in    0s   \n170\n.92 kB/s  \ndone\n\n\n\n\n\n\n\n\n\nGets an object\n\n\n\n\n1\n2\n3\n$ s3cmd get s3://\nBUCKET\n/\nOBJECT\n\ndownload: \ns3://\nBUCKET\n/\nOBJET\n -\n \n./\nOBJECT\n  \n[\n1\n of \n1\n]\n\n \n1096\n of \n1096\n   \n100\n% in    0s   \n307\n.38 kB/s  \ndone\n\n\n\n\n\n\n\n\n\nLists objects\n\n\n\n\n1\n2\n$ s3cmd ls s3://\nBUCKET\n/\n\n2017\n-01-30 \n02\n:24      \n1096\n   s3://\nBUCKET\n/\nOBJECT\n\n\n\n\n\n\n\n\n\nRemoves an object\n\n\n\n\n1\n2\n$ s3cmd del s3://\nBUCKET\n/\nOBJECT\n\ndelete: \ns3://\nBUCKET\n/\nOBJECT\n\n\n\n\n\n\n\nWrap up\n\n\nYou now have a working LeoFS cluster. Make sure to have a look at \nAdministrators / Settings\n to learn more about administration and settings of a LeoFS storage system.\n\n\nRelated Links\n\n\n\n\nGetting Started / Quick Installation and Setup\n\n\nFor Administrators / Setup / Planning for Production\n\n\nFor Administrators / Settings / Environment Configuration\n\n\nFor Administrators / Settings / Cluster Settings\n\n\nFor Administrators / Settings / LeoManager Settings\n\n\nFor Administrators / Settings / LeoStorage Settings\n\n\nFor Administrators / Settings / LeoGateway Settings\n\n\nFor Administrators / System Operations / Cluster Operations", 
            "title": "Building a LeoFS' cluster with Ansible"
        }, 
        {
            "location": "/installation/cluster/#building-a-leofs-cluster-with-ansible", 
            "text": "", 
            "title": "Building a LeoFS' cluster with Ansible"
        }, 
        {
            "location": "/installation/cluster/#purpose", 
            "text": "This tutorial teaches you how to easily build a LeoFS cluster. All steps will not be explained in detail, it is assumed you already know  how to setup a stand-alone LeoFS system . This guide exists to help you get a cluster up and running quickly. We recommend that you read the LeoFS Configuration and Administration Guide to learn how to administer your LeoFS cluster. We hope that by reading this tutorial you will be able to get a cluster started as quickly as possible.", 
            "title": "Purpose"
        }, 
        {
            "location": "/installation/cluster/#installs-and-launches-leofs-with-leofs_ansible", 
            "text": "You can easily install LeoFS into your servers with using  leofs_ansible , perform following steps:", 
            "title": "Installs and Launches LeoFS with leofs_ansible"
        }, 
        {
            "location": "/installation/cluster/#uses-leofs_ansible", 
            "text": "Installs and sets up  Ansible  leofs_ansible's documentation  is already published, you can follow it to install and launch a LeoFS storage system.", 
            "title": "Uses leofs_ansible"
        }, 
        {
            "location": "/installation/cluster/#an-example-of-hosts", 
            "text": "", 
            "title": "An example of hosts"
        }, 
        {
            "location": "/installation/cluster/#manager", 
            "text": "A number of nodes: 2  IP: 10.0.0.101, 10.0.0.102  Name: M0@10.0.0.101, M1@10.0.0.102", 
            "title": "Manager"
        }, 
        {
            "location": "/installation/cluster/#gateway", 
            "text": "A number of nodes: 1  IP: 10.0.0.103  Name: G0@10.0.0.103", 
            "title": "Gateway"
        }, 
        {
            "location": "/installation/cluster/#storage", 
            "text": "A number of nodes: 3  IP: 10.0.0.104 .. 10.0.0.106  Name: S0@10.0.0.104 .. S2@10.0.0.106   In this case, we configure basic properties and nodes of LeoManager, LeoStorage an LeoGateway. You need to configure those properties to suit your environment.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34 ##\n## Please check roles/common/vars/leofs_releases for available versions\n##\n[all:vars]\nleofs_version=1.3.2\nbuild_temp_path= /tmp/leofs_builder \nbuild_install_path= /tmp/ \nbuild_branch= master \nsource= package \n\n[builder]\n10.0.0.100\n\n# nodename of leo_manager_0 and leo_manager_1 are set at group_vars/all\n[leo_manager_0]\n10.0.0.101\n\n# nodename of leo_manager_0 and leo_manager_1 are set at group_vars/all\n[leo_manager_1]\n10.0.0.102\n\n[leo_storage]\n10.0.0.104 leofs_module_nodename=S0@10.0.0.104\n10.0.0.105 leofs_module_nodename=S1@10.0.0.105\n10.0.0.106 leofs_module_nodename=S2@10.0.0.106\n\n[leo_gateway]\n10.0.0.103 leofs_module_nodename=G0@10.0.0.103\n\n[leofs_nodes:children]\nleo_manager_0\nleo_manager_1\nleo_gateway\nleo_storage", 
            "title": "Storage"
        }, 
        {
            "location": "/installation/cluster/#confirmation", 
            "text": "Uses the  leofs-adm status  command to confirm current its status.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37 $   leofs - adm   status \n  [ System   Confiuration ]  -----------------------------------+---------- \n  Item                                |   Value  -----------------------------------+---------- \n  Basic / Consistency   level  -----------------------------------+---------- \n                     system   version   |   1.3.2 \n                         cluster   Id   |   leofs_1 \n                              DC   Id   |   dc_1 \n                     Total   replicas   |   2 \n           number   of   successes   of   R   |   1 \n           number   of   successes   of   W   |   1 \n           number   of   successes   of   D   |   1 \n  number   of   rack - awareness   replicas   |   0 \n                          ring   size   |   2 ^ 128  -----------------------------------+---------- \n  Multi   DC   replication   settings  -----------------------------------+---------- \n         max   number   of   joinable   DCs   |   2 \n            number   of   replicas   a   DC   |   1  -----------------------------------+---------- \n  Manager   RING   hash  -----------------------------------+---------- \n                  current   ring - hash   |   3923 d007 \n                 previous   ring - hash   |   3923 d007  -----------------------------------+---------- \n\n  [ State   of   Node ( s )]  -------+--------------------------+--------------+----------------+----------------+---------------------------- \n  type    |             node             |      state       |    current   ring    |     prev   ring      |            updated   at  -------+--------------------------+--------------+----------------+----------------+---------------------------- \n   S      |   S0 @10.0.0.104              |   running        |   3923 d007         |   3923 d007         |   2017 - 01 - 30   12 : 32 : 48   + 0900 \n   S      |   S1 @10.0.0.105              |   running        |   3923 d007         |   3923 d007         |   2017 - 01 - 30   12 : 32 : 48   + 0900 \n   S      |   S2 @10.0.0.106              |   running        |   3923 d007         |   3923 d007         |   2017 - 01 - 30   12 : 32 : 48   + 0900 \n   G      |   G0 @10.0.0.103              |   running        |   3923 d007         |   3923 d007         |   2017 - 01 - 30   12 : 32 : 52   + 0900  -------+--------------------------+--------------+----------------+----------------+----------------------------", 
            "title": "Confirmation"
        }, 
        {
            "location": "/installation/cluster/#retrieves-both-an-access-key-and-a-secret-accesskey", 
            "text": "To make a bucket for the test, you need to retrieve both an  access-key  and a  secret access-key  with the  leofs-adm create-user  command  1\n2\n3 $ leofs-adm create-user  YOUR-ID \naccess-key-id:  ACCESS-KEY-ID \nsecret-access-key:  SECRET-ACCESS-KEY-ID    After that, you use S3-client(s) with those keys when you access the LeoFS storage system.", 
            "title": "Retrieves both an access-key and a secret accesskey"
        }, 
        {
            "location": "/installation/cluster/#usess3cmd-to-access-it", 
            "text": "If you'd like to use  s3cmd  to access the LeoFS storage system, perform the following steps:   Installs  s3cmd  on your machine  Configures  s3cmd   1\n2\n3\n4\n5\n6\n7\n8\n9 $ s3cmd --version\ns3cmd version  1 .6.1\n\n$ s3cmd --configure\n... ## access_key =  access-key-id  ## secret_key =  secret-access-key  ## proxy_host = localhost  ## proxy_port = 8080     Creates a bucket   1\n2 $ s3cmd mb  BUCKET \nBucket  s3:// BUCKET /  created    Puts an object into the bucket   1\n2\n3 $ s3cmd put /path/to/ OBJECT  s3:// BUCKET /\nupload:  OBJET  -   s3:// BUCKET / OBJECT    [ 1  of  1 ] \n  1096  of  1096     100 % in    0s    170 .92 kB/s   done     Gets an object   1\n2\n3 $ s3cmd get s3:// BUCKET / OBJECT \ndownload:  s3:// BUCKET / OBJET  -   ./ OBJECT    [ 1  of  1 ] \n  1096  of  1096     100 % in    0s    307 .38 kB/s   done     Lists objects   1\n2 $ s3cmd ls s3:// BUCKET / 2017 -01-30  02 :24       1096    s3:// BUCKET / OBJECT     Removes an object   1\n2 $ s3cmd del s3:// BUCKET / OBJECT \ndelete:  s3:// BUCKET / OBJECT", 
            "title": "Usess3cmd to access it"
        }, 
        {
            "location": "/installation/cluster/#wrap-up", 
            "text": "You now have a working LeoFS cluster. Make sure to have a look at  Administrators / Settings  to learn more about administration and settings of a LeoFS storage system.", 
            "title": "Wrap up"
        }, 
        {
            "location": "/installation/cluster/#related-links", 
            "text": "Getting Started / Quick Installation and Setup  For Administrators / Setup / Planning for Production  For Administrators / Settings / Environment Configuration  For Administrators / Settings / Cluster Settings  For Administrators / Settings / LeoManager Settings  For Administrators / Settings / LeoStorage Settings  For Administrators / Settings / LeoGateway Settings  For Administrators / System Operations / Cluster Operations", 
            "title": "Related Links"
        }, 
        {
            "location": "/architecture/README/", 
            "text": "Architecture\n\n\nConcept\n\n\nWhat we\u2019re focused on is \nhigh availability\n, \nhigh scalability\n, and \nhigh-cost performance ratio\n because unstructured data which have been exponentially increasing day by day, and we needed to more efficiently manage objects to find values from tons of raw data.\n\n\nArchitecture Overview\n\n\nWe succeeded in designing and implementing LeoFS as simple as possible. LeoFS consists of three components, \nLeoManager\n, \nLeoStorage\n, and \nLeoGateway\n. The role of each component is clearly defined.\n\n\n\n\nWhat we also carefully desined LeoFS is three things:\n\n\n\n\nTo keep running LeoFS without SPOF\n\n\nTo keep maintaining high performance regardless of the kind of data and amount data\n\n\nTo provide easy administration by the LeoFS' CLI, \nleofs-adm\n\n\n\n\nRelated Links\n\n\n\n\nConcept and Architecture / LeoGateway's Architecture\n\n\nConcept and Architecture / LeoStorage's Architecture\n\n\nConcept and Architecture / LeoManager's Architecture", 
            "title": "LeoFS' Architecture"
        }, 
        {
            "location": "/architecture/README/#architecture", 
            "text": "", 
            "title": "Architecture"
        }, 
        {
            "location": "/architecture/README/#concept", 
            "text": "What we\u2019re focused on is  high availability ,  high scalability , and  high-cost performance ratio  because unstructured data which have been exponentially increasing day by day, and we needed to more efficiently manage objects to find values from tons of raw data.", 
            "title": "Concept"
        }, 
        {
            "location": "/architecture/README/#architecture-overview", 
            "text": "We succeeded in designing and implementing LeoFS as simple as possible. LeoFS consists of three components,  LeoManager ,  LeoStorage , and  LeoGateway . The role of each component is clearly defined.   What we also carefully desined LeoFS is three things:   To keep running LeoFS without SPOF  To keep maintaining high performance regardless of the kind of data and amount data  To provide easy administration by the LeoFS' CLI,  leofs-adm", 
            "title": "Architecture Overview"
        }, 
        {
            "location": "/architecture/README/#related-links", 
            "text": "Concept and Architecture / LeoGateway's Architecture  Concept and Architecture / LeoStorage's Architecture  Concept and Architecture / LeoManager's Architecture", 
            "title": "Related Links"
        }, 
        {
            "location": "/architecture/leo_gateway/", 
            "text": "LeoGateway's Architecture\n\n\nLeoGateway consists of the fast HTTP server which is \nCowboy\n, the multi-protocols handler, and the object cache. It provides multi-protocols which are the RESTful API, \nAmazon S3-API\n, and NFS v3. If you adopt using Amazon S3-API, you can easily access LeoFS with S3 clients which include \ns3cmd\n, \nDragonDisk\n and AWS SDKs - \nJava\n, \nRuby\n, \nGo\n, \nPython (Boto3)\n and others.\n\n\n\n\nA client requests an object or a bucket operation to a LeoGateway node, then it requests the message of an operation to a LeoStorage node.\n\n\nA destination LeoStorage node is decided by RING \n(distributed hash table)\n, which is generated and distributed at LeoManager nodes.\n\n\nLeoGateway also provides built-in support for the object cache to realize keeping high performance and reduction of traffic between LeoGateway nodes and LeoStorage nodes.\n\n\nRelated Links\n\n\n\n\nFor Administrators / Settings / LeoGateway Settings\n\n\nFor Administrators / Interface / S3-API\n\n\nFor Administrators / Interface / REST-API\n\n\nFor Administrators / Interface / NFS v3\n\n\nFor Administrators / System Operations / S3-API related Operations", 
            "title": "LeoGateway's Architecture"
        }, 
        {
            "location": "/architecture/leo_gateway/#leogateways-architecture", 
            "text": "LeoGateway consists of the fast HTTP server which is  Cowboy , the multi-protocols handler, and the object cache. It provides multi-protocols which are the RESTful API,  Amazon S3-API , and NFS v3. If you adopt using Amazon S3-API, you can easily access LeoFS with S3 clients which include  s3cmd ,  DragonDisk  and AWS SDKs -  Java ,  Ruby ,  Go ,  Python (Boto3)  and others.   A client requests an object or a bucket operation to a LeoGateway node, then it requests the message of an operation to a LeoStorage node.  A destination LeoStorage node is decided by RING  (distributed hash table) , which is generated and distributed at LeoManager nodes.  LeoGateway also provides built-in support for the object cache to realize keeping high performance and reduction of traffic between LeoGateway nodes and LeoStorage nodes.", 
            "title": "LeoGateway's Architecture"
        }, 
        {
            "location": "/architecture/leo_gateway/#related-links", 
            "text": "For Administrators / Settings / LeoGateway Settings  For Administrators / Interface / S3-API  For Administrators / Interface / REST-API  For Administrators / Interface / NFS v3  For Administrators / System Operations / S3-API related Operations", 
            "title": "Related Links"
        }, 
        {
            "location": "/architecture/leo_storage/", 
            "text": "LeoStorage's Architecture\n\n\nFundamentals\n\n\nLeoStorage consists of \nthe object storage\n and \nthe metadata storage\n, and it includes replicator and repairer realize eventual consistency.\n\n\nWRITE-Request Handling\n\n\nA LeoStorage node accepts a request from a LeoGateway node then automatically replicate an object into the LeoStorage cluster. Finally, its LeoStorage node confirms whether a stored object satisfies the consistency rule.\n\n\nREAD-Request Handling\n\n\nA LeoGateway node requests a LeoStorage node; then its LeoStorage node retrieves an object from the local object-storage or a remote LeoStorage node. Finally, its LeoStorage node responds an object to its LeoGateway node. Also, its LeoStorage node checks the consistency with the asynchronous processing. Please note that \nLeoGateway cache settings\n can affect requests handling.\n\n\nIf its LeoStorage node finds inconsistency of an object, its node fixes the inconsistent object with the backend process. Its object eventually keeps consistency with the functions.\n\n\n\n\nData Structure\n\n\nLeoFS\u2019 object consists of three layers which are \nmetadata\n, \nneedle\n and \nobject-container\n.\n\n\n\n\nLeoObjectStorage\n manages and stores both an object and metadata which stores as a needle.\n\n\nLeoObjectStorage's metadata-storage handles and stores attributes of an object which includes filename, size, checksum, and others, and it depends on \nLeveldb\n.\n\n\nLeoObjectStorage's object-container adopts a log structured file format, which is robust and high performance because an effect of the local file system is just a little part, and LeoStorage is necessary to remove unnecessary objects from the object containers, which is realized by the data compaction feature.\n\n\n\n\n\n\nLarge Object Support\n\n\nLeoFS supports handling a large size object since v0.12. The purpose of this feature is two things:\n\n\n\n\nTo equalize disk usage of each LeoStorage node.\n\n\nTo realize high I/O efficiency and high availability.\n\n\n\n\nWRITE-Request Handling\n\n\nA LeoGateway node divides a large size object into plural objects, then those chunks are replicated into a LeoStorage cluster which is similar to handling small size objects, and the default chunk size is 5MB, the configuration of which can change a custom chunked object size.\n\n\nREAD-Request Handling\n\n\nA LeoGateway node retrieves a metadata of a requested object, then if it's a large size object, its LeoGateway node retrieves the chunked objects in order of the fragment object number from the LeoStorage cluster. Finally, its LeoGateway node responds the objects to the client.\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Settings / LeoStroage Settings\n\n\nFor Administrators / System Operations / Cluster Operations", 
            "title": "LeoStorage's Architecture"
        }, 
        {
            "location": "/architecture/leo_storage/#leostorages-architecture", 
            "text": "", 
            "title": "LeoStorage's Architecture"
        }, 
        {
            "location": "/architecture/leo_storage/#fundamentals", 
            "text": "LeoStorage consists of  the object storage  and  the metadata storage , and it includes replicator and repairer realize eventual consistency.", 
            "title": "Fundamentals"
        }, 
        {
            "location": "/architecture/leo_storage/#write-request-handling", 
            "text": "A LeoStorage node accepts a request from a LeoGateway node then automatically replicate an object into the LeoStorage cluster. Finally, its LeoStorage node confirms whether a stored object satisfies the consistency rule.", 
            "title": "WRITE-Request Handling"
        }, 
        {
            "location": "/architecture/leo_storage/#read-request-handling", 
            "text": "A LeoGateway node requests a LeoStorage node; then its LeoStorage node retrieves an object from the local object-storage or a remote LeoStorage node. Finally, its LeoStorage node responds an object to its LeoGateway node. Also, its LeoStorage node checks the consistency with the asynchronous processing. Please note that  LeoGateway cache settings  can affect requests handling.  If its LeoStorage node finds inconsistency of an object, its node fixes the inconsistent object with the backend process. Its object eventually keeps consistency with the functions.", 
            "title": "READ-Request Handling"
        }, 
        {
            "location": "/architecture/leo_storage/#data-structure", 
            "text": "LeoFS\u2019 object consists of three layers which are  metadata ,  needle  and  object-container .   LeoObjectStorage  manages and stores both an object and metadata which stores as a needle.  LeoObjectStorage's metadata-storage handles and stores attributes of an object which includes filename, size, checksum, and others, and it depends on  Leveldb .  LeoObjectStorage's object-container adopts a log structured file format, which is robust and high performance because an effect of the local file system is just a little part, and LeoStorage is necessary to remove unnecessary objects from the object containers, which is realized by the data compaction feature.", 
            "title": "Data Structure"
        }, 
        {
            "location": "/architecture/leo_storage/#large-object-support", 
            "text": "LeoFS supports handling a large size object since v0.12. The purpose of this feature is two things:   To equalize disk usage of each LeoStorage node.  To realize high I/O efficiency and high availability.", 
            "title": "Large Object Support"
        }, 
        {
            "location": "/architecture/leo_storage/#write-request-handling_1", 
            "text": "A LeoGateway node divides a large size object into plural objects, then those chunks are replicated into a LeoStorage cluster which is similar to handling small size objects, and the default chunk size is 5MB, the configuration of which can change a custom chunked object size.", 
            "title": "WRITE-Request Handling"
        }, 
        {
            "location": "/architecture/leo_storage/#read-request-handling_1", 
            "text": "A LeoGateway node retrieves a metadata of a requested object, then if it's a large size object, its LeoGateway node retrieves the chunked objects in order of the fragment object number from the LeoStorage cluster. Finally, its LeoGateway node responds the objects to the client.", 
            "title": "READ-Request Handling"
        }, 
        {
            "location": "/architecture/leo_storage/#related-links", 
            "text": "For Administrators / Settings / LeoStroage Settings  For Administrators / System Operations / Cluster Operations", 
            "title": "Related Links"
        }, 
        {
            "location": "/architecture/leo_manager/", 
            "text": "LeoManager's Architecture\n\n\nLeoManager monitors the state of LeoGateway and LeoStorage nodes to keep the high availability of a LeoFS system. A consistency of RING (\ndistributed hash table\n) of LeoGateway and LeoStorage nodes are always monitored by LeoManager to prevent \nsplit-brain\n.\n\n\n\n\nBoth LeoManager nodes manage configurations of a system and information of every node to be able to recover a system reliability, and the data are replicated by \nErlang Mnesia\n to avoid data loss.\n\n\nLeoManager provides \nleofs-adm as a LeoFS administration commands\n to be able to operate LeoFS quickly. The administration commands already cover entire LeoFS features.\n\n\nRelated Links\n\n\n\n\nFor Administrators / Index of LeoFS' Commands\n\n\nFor Administrators / Settings / LeoManager Settings", 
            "title": "LeoManager's Architecture"
        }, 
        {
            "location": "/architecture/leo_manager/#leomanagers-architecture", 
            "text": "LeoManager monitors the state of LeoGateway and LeoStorage nodes to keep the high availability of a LeoFS system. A consistency of RING ( distributed hash table ) of LeoGateway and LeoStorage nodes are always monitored by LeoManager to prevent  split-brain .   Both LeoManager nodes manage configurations of a system and information of every node to be able to recover a system reliability, and the data are replicated by  Erlang Mnesia  to avoid data loss.  LeoManager provides  leofs-adm as a LeoFS administration commands  to be able to operate LeoFS quickly. The administration commands already cover entire LeoFS features.", 
            "title": "LeoManager's Architecture"
        }, 
        {
            "location": "/architecture/leo_manager/#related-links", 
            "text": "For Administrators / Index of LeoFS' Commands  For Administrators / Settings / LeoManager Settings", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/setup/planning_for_production/", 
            "text": "Planning for Production\n\n\nThis section provides information to set up LeoFS on the production environment. Planning for production involves ensuring that the hardware and software requirements are met, and the deployment and security consideration are taken into account before installing LeoFS. You can choose to install LeoFS on-premises on \nsupported platforms\n or virtualization platforms.\n\n\nBefore you begin the actual installation, ensure that you go through the section.\nFor installation instructions on different platforms:\n\n\n\n\nOn-premises\n\n\nVirtualization platforms:\n\n\nAmazon Web Services\n and \nDocker\n \n\n\n\n\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Setup / Supported Platforms\n\n\nFor Administrators / Setup / Hardware Requirements\n\n\nFor Administrators / Setup / Network Configuration", 
            "title": "Planning for Production"
        }, 
        {
            "location": "/admin/setup/planning_for_production/#planning-for-production", 
            "text": "This section provides information to set up LeoFS on the production environment. Planning for production involves ensuring that the hardware and software requirements are met, and the deployment and security consideration are taken into account before installing LeoFS. You can choose to install LeoFS on-premises on  supported platforms  or virtualization platforms.  Before you begin the actual installation, ensure that you go through the section.\nFor installation instructions on different platforms:   On-premises  Virtualization platforms:  Amazon Web Services  and  Docker", 
            "title": "Planning for Production"
        }, 
        {
            "location": "/admin/setup/planning_for_production/#related-links", 
            "text": "For Administrators / Setup / Supported Platforms  For Administrators / Setup / Hardware Requirements  For Administrators / Setup / Network Configuration", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/setup/supported_platforms/", 
            "text": "Supported Platforms\n\n\nLeoFS supports popular operating systems and virtual environments.\n\n\n\n\n\n\n\n\nPlatform\n\n\nVersion\n\n\n32/64 bit\n\n\nSupported\n\n\n\n\n\n\n\n\n\n\nCentOS\n\n\n7\n\n\n64 bit\n\n\nDevelopment, testing and production\n\n\n\n\n\n\nCentOS\n\n\n6\n\n\n64 bit\n\n\nDevelopment, testing and production\n\n\n\n\n\n\nUbuntu Linux\n\n\n16.04\n\n\n64 bit\n\n\nDevelopment, testing and production\n\n\n\n\n\n\nUbuntu Linux\n\n\n14.04\n\n\n64 bit\n\n\nDevelopment, testing and production\n\n\n\n\n\n\nAWS.EC2 Red Hat Enterprise Linux\n\n\n7\n\n\n64 bit\n\n\nDevelopment, testing\n\n\n\n\n\n\nAWS.EC2 Red Hat Enterprise Linux\n\n\n6\n\n\n64 bit\n\n\nDevelopment, testing\n\n\n\n\n\n\nAWS.EC2 Ubnutu\n\n\n16.04\n\n\n64 bit\n\n\nDevelopment, testing\n\n\n\n\n\n\nAWS.EC2 Ubnutu\n\n\n14.04\n\n\n64 bit\n\n\nDevelopment, testing\n\n\n\n\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Setup / Planning for Production", 
            "title": "Supported Platforms"
        }, 
        {
            "location": "/admin/setup/supported_platforms/#supported-platforms", 
            "text": "LeoFS supports popular operating systems and virtual environments.     Platform  Version  32/64 bit  Supported      CentOS  7  64 bit  Development, testing and production    CentOS  6  64 bit  Development, testing and production    Ubuntu Linux  16.04  64 bit  Development, testing and production    Ubuntu Linux  14.04  64 bit  Development, testing and production    AWS.EC2 Red Hat Enterprise Linux  7  64 bit  Development, testing    AWS.EC2 Red Hat Enterprise Linux  6  64 bit  Development, testing    AWS.EC2 Ubnutu  16.04  64 bit  Development, testing    AWS.EC2 Ubnutu  14.04  64 bit  Development, testing", 
            "title": "Supported Platforms"
        }, 
        {
            "location": "/admin/setup/supported_platforms/#related-links", 
            "text": "For Administrators / Setup / Planning for Production", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/setup/hardware_requirements/", 
            "text": "Hardware Requirements\n\n\nMinimum Requirements\n\n\n\n\n\n\n\n\n\n\nCPU\n\n\nMemory\n\n\nDisk\n\n\n\n\n\n\n\n\n\n\nManager\n\n\n1\n\n\n512 MB\n\n\n20 GB\n\n\n\n\n\n\nGateway\n\n\n2\n\n\n1 GB\n\n\n20 GB\n\n\n\n\n\n\nStorage\n\n\n2\n\n\n1 GB\n\n\n100 GB\n\n\n\n\n\n\n\n\nHardware Recommendation\n\n\n\n\nWorkload on Manager is low\n, it does not consume many resources\n\n\nBetter CPU allows LeoGateway\n to process more operations (OPS, \nOperation Per Second\n)\n\n\nLeoGateway utilizes \nmemory and disk as cache\n, adding those resources can reduce the workload to LeoStorage\n\n\nSSD\n on a LeoStoage node significantly improves small object read performance\n\n\n10Gbps network\n is recommended\n\n\n\n\nReference Platform\n\n\n\n\n\n\n\n\nHardware\n\n\nDetail\n\n\n\n\n\n\n\n\n\n\nCPU\n\n\n16C32T (Intel E5-2630 v3)\n\n\n\n\n\n\nMemory\n\n\n32 GB\n\n\n\n\n\n\nNetwork\n\n\n10 GbE\n\n\n\n\n\n\nDisk\n\n\nSSD (Crucial BX100)\n\n\n\n\n\n\n\n\nPerformance\n\n\n\n\n\n\n\n\nData Set\n\n\nRead\n\n\nWrite\n\n\nResource Usage\n\n\n\n\n\n\n\n\n\n\nImage (32KB)\n\n\n20,000 OPS\n\n\n20,000 OPS\n\n\nHigh CPU Usage\n\n\n\n\n\n\nSmall Mixed (\n2MB)\n\n\n1,200 OPS\n\n\n1,500 OPS\n\n\nHigh Disk I/O\n\n\n\n\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Setup / Planning for Production\n\n\nleo-project/notes - LeoFS Benchmark Report", 
            "title": "Hardware Requirements"
        }, 
        {
            "location": "/admin/setup/hardware_requirements/#hardware-requirements", 
            "text": "", 
            "title": "Hardware Requirements"
        }, 
        {
            "location": "/admin/setup/hardware_requirements/#minimum-requirements", 
            "text": "CPU  Memory  Disk      Manager  1  512 MB  20 GB    Gateway  2  1 GB  20 GB    Storage  2  1 GB  100 GB", 
            "title": "Minimum Requirements"
        }, 
        {
            "location": "/admin/setup/hardware_requirements/#hardware-recommendation", 
            "text": "Workload on Manager is low , it does not consume many resources  Better CPU allows LeoGateway  to process more operations (OPS,  Operation Per Second )  LeoGateway utilizes  memory and disk as cache , adding those resources can reduce the workload to LeoStorage  SSD  on a LeoStoage node significantly improves small object read performance  10Gbps network  is recommended", 
            "title": "Hardware Recommendation"
        }, 
        {
            "location": "/admin/setup/hardware_requirements/#reference-platform", 
            "text": "Hardware  Detail      CPU  16C32T (Intel E5-2630 v3)    Memory  32 GB    Network  10 GbE    Disk  SSD (Crucial BX100)", 
            "title": "Reference Platform"
        }, 
        {
            "location": "/admin/setup/hardware_requirements/#performance", 
            "text": "Data Set  Read  Write  Resource Usage      Image (32KB)  20,000 OPS  20,000 OPS  High CPU Usage    Small Mixed ( 2MB)  1,200 OPS  1,500 OPS  High Disk I/O", 
            "title": "Performance"
        }, 
        {
            "location": "/admin/setup/hardware_requirements/#related-links", 
            "text": "For Administrators / Setup / Planning for Production  leo-project/notes - LeoFS Benchmark Report", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/setup/network_config/", 
            "text": "Network Configurations\n\n\nFirewall Rules\n\n\nIn order for a LeoFS system to operate correctly, it is necessary to set and check the firewall rules in your environment. LeoFS depends on \nErlang/OTP\n's RPC, which uses specified ports and provides LeoFS' SNMP agent which also uses a port per a LeoFS' component node - LeoStorage, LeoGateway, and LeoManager.\n\n\n\n\n\n\n\n\nSubsystem\n\n\nDirection\n\n\nPorts\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nLeoManager (master)\n\n\n\n\n\n\n\n\n\n\n\n\nLeoManager (master)\n\n\nIncoming\n\n\n10010/*\n\n\nLeoManager console (text)\n\n\n\n\n\n\nLeoManager (master)\n\n\nIncoming\n\n\n10020/*\n\n\nLeoManager console (json)\n\n\n\n\n\n\nLeoManager (master)\n\n\nIncoming\n\n\n4369/*\n\n\nErlang Port Mapper\n\n\n\n\n\n\nLeoManager (master)\n\n\nIncoming\n\n\n4020/*\n\n\nSNMP Listen Port\n\n\n\n\n\n\nLeoManager (master)\n\n\nOutgoing\n\n\n*/4369\n\n\nErlang Port Mapper\n\n\n\n\n\n\nLeoManager (slave)\n\n\n\n\n\n\n\n\n\n\n\n\nLeoManager (slave)\n\n\nIncoming\n\n\n10011/*\n\n\nLeoManager console (text)\n\n\n\n\n\n\nLeoManager (slave)\n\n\nIncoming\n\n\n10021/*\n\n\nLeoManager console (json)\n\n\n\n\n\n\nLeoManager (slave)\n\n\nIncoming\n\n\n4369/*\n\n\nErlang Port Mapper\n\n\n\n\n\n\nLeoManager (slave)\n\n\nIncoming\n\n\n4021/*\n\n\nSNMP Listen Port\n\n\n\n\n\n\nLeoManager (slave)\n\n\nOutgoing\n\n\n*/4369\n\n\nErlang Port Mapper\n\n\n\n\n\n\nLeoStorage\n\n\n\n\n\n\n\n\n\n\n\n\nLeoStorage\n\n\nIncoming\n\n\n4369/*\n\n\nErlang Port Mapper\n\n\n\n\n\n\nLeoStorage\n\n\nIncoming\n\n\n4010/*\n\n\nSNMP Listen Port\n\n\n\n\n\n\nLeoStorage\n\n\nOutgoing\n\n\n*/4369\n\n\nErlang Port Mapper\n\n\n\n\n\n\nLeoGateway\n\n\n\n\n\n\n\n\n\n\n\n\nLeoGateway\n\n\nIncoming\n\n\n8080/*\n\n\nHTTP listen port\n\n\n\n\n\n\nLeoGateway\n\n\nIncoming\n\n\n8443/*\n\n\nHTTPS listen port\n\n\n\n\n\n\nLeoGateway\n\n\nIncoming\n\n\n4369/*\n\n\nErlang Port Mapper\n\n\n\n\n\n\nLeoGateway\n\n\nIncoming\n\n\n4000/*\n\n\nSNMP Listen Port\n\n\n\n\n\n\nLeoGateway\n\n\nOutgoing\n\n\n*/4369\n\n\nErlang Port Mapper\n\n\n\n\n\n\n\n\nHow to Change Erlang's Port Range\n\n\nPort range can be specified by setting Erlang's kernel variables \ninet_dist_listen_min\n and \ninet_dist_listen_max\n. If you need to change those configuration items, you need to enter the Erlang console of a target node.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n## Example:\n\n\n##   - This forces Erlang to use only ports 9100--9105 for distributed Erlang traffic.\n\n$ bin/leo_storage remote_console\n\n\n(\nstorage_0@127.0.0.1\n)\n1\n application:set_env\n(\nkernel, inet_dist_listen_min, \n9100\n)\n.\n\n(\nstorage_0@127.0.0.1\n)\n2\n application:set_env\n(\nkernel, inet_dist_listen_max, \n9105\n)\n.\n\n\n## Press [CTRL+c] to exit the Erlang console\n\n\n\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Settings / LeoManager Settings\n\n\nFor Administrators / Settings / LeoStorage Settings\n\n\nFor Administrators / Settings / LeoGateway Settings", 
            "title": "Network Configurations"
        }, 
        {
            "location": "/admin/setup/network_config/#network-configurations", 
            "text": "", 
            "title": "Network Configurations"
        }, 
        {
            "location": "/admin/setup/network_config/#firewall-rules", 
            "text": "In order for a LeoFS system to operate correctly, it is necessary to set and check the firewall rules in your environment. LeoFS depends on  Erlang/OTP 's RPC, which uses specified ports and provides LeoFS' SNMP agent which also uses a port per a LeoFS' component node - LeoStorage, LeoGateway, and LeoManager.     Subsystem  Direction  Ports  Description      LeoManager (master)       LeoManager (master)  Incoming  10010/*  LeoManager console (text)    LeoManager (master)  Incoming  10020/*  LeoManager console (json)    LeoManager (master)  Incoming  4369/*  Erlang Port Mapper    LeoManager (master)  Incoming  4020/*  SNMP Listen Port    LeoManager (master)  Outgoing  */4369  Erlang Port Mapper    LeoManager (slave)       LeoManager (slave)  Incoming  10011/*  LeoManager console (text)    LeoManager (slave)  Incoming  10021/*  LeoManager console (json)    LeoManager (slave)  Incoming  4369/*  Erlang Port Mapper    LeoManager (slave)  Incoming  4021/*  SNMP Listen Port    LeoManager (slave)  Outgoing  */4369  Erlang Port Mapper    LeoStorage       LeoStorage  Incoming  4369/*  Erlang Port Mapper    LeoStorage  Incoming  4010/*  SNMP Listen Port    LeoStorage  Outgoing  */4369  Erlang Port Mapper    LeoGateway       LeoGateway  Incoming  8080/*  HTTP listen port    LeoGateway  Incoming  8443/*  HTTPS listen port    LeoGateway  Incoming  4369/*  Erlang Port Mapper    LeoGateway  Incoming  4000/*  SNMP Listen Port    LeoGateway  Outgoing  */4369  Erlang Port Mapper", 
            "title": "Firewall Rules"
        }, 
        {
            "location": "/admin/setup/network_config/#how-to-change-erlangs-port-range", 
            "text": "Port range can be specified by setting Erlang's kernel variables  inet_dist_listen_min  and  inet_dist_listen_max . If you need to change those configuration items, you need to enter the Erlang console of a target node.  1\n2\n3\n4\n5\n6\n7\n8 ## Example:  ##   - This forces Erlang to use only ports 9100--9105 for distributed Erlang traffic. \n$ bin/leo_storage remote_console ( storage_0@127.0.0.1 ) 1  application:set_env ( kernel, inet_dist_listen_min,  9100 ) . ( storage_0@127.0.0.1 ) 2  application:set_env ( kernel, inet_dist_listen_max,  9105 ) . ## Press [CTRL+c] to exit the Erlang console", 
            "title": "How to Change Erlang's Port Range"
        }, 
        {
            "location": "/admin/setup/network_config/#related-links", 
            "text": "For Administrators / Settings / LeoManager Settings  For Administrators / Settings / LeoStorage Settings  For Administrators / Settings / LeoGateway Settings", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/settings/cluster/", 
            "text": "Cluster Settings\n\n\nThis document outlines the various configuration items to keep in mind when planning a LeoFS system's cluster, and this documentation leads you to be able to configure its cluster when planning and launching it correctly.\n\n\nPrior Knowledge\n\n\nLeoFS adopts \neventual consistency\n of the consistency model; it takes priority over AP \n(Availability and Partition tolerance)\n over C \n(consistency)\n which depends on \nCAP theorem\n.\n\n\nTo keep the consistency of objects eventually, LeoFS delivers the replication and recovery feature to automatically fix consistency of objects. You can configure the consistency level of a LeoFS system, and it is affected by the configuration.\n\n\nHow to Keep RING's Consistency\n\n\nCase 1: Both LeoManager nodes are unavailable\n\n\nIf both LeoManager nodes are unavailable, LeoStorage and LeoGateway nodes don't update the RING to keep its consistency into the LeoFS system.\n\n\nCase 2: One LeoManager node is unavailable\n\n\nIf a LeoManager node is unavailable, LeoFS can update the RING, and synchronize it with the LeoFS' system eventually. After restarting another LeoManager node, LeoManager automatically synchronizes the RING between the manager nodes.\n\n\nConsistency Level\n\n\nConfigure the consistency level of a LeoFS system at \nLeoManager's configuration file - leo_manager_0.conf\n. You need to carefully configure the consistency level because it is not able to change some items after starting the system.\n\n\nThere are four configuration items at \nleo_manager_0.conf\n, items of which have a great impact on \ndata availability\n and \nstorage performance\n.\n\n\n\n\n\n\n\n\nItem\n\n\nAbbr\n\n\nModifiable\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nconsistency.num_of_replicas\n\n\nn\n\n\nNo\n\n\n1\n\n\nA number of replicas\n\n\n\n\n\n\nconsistency.write\n\n\nw\n\n\nYes\n\n\n1\n\n\nA number of replicas needed for a successful WRITE operation\n\n\n\n\n\n\nconsistency.read\n\n\nr\n\n\nYes\n\n\n1\n\n\nA number of replicas needed for a successful READ operation\n\n\n\n\n\n\nconsistency.delete\n\n\nd\n\n\nYes\n\n\n1\n\n\nA number of replicas needed for a successful DELETE operation\n\n\n\n\n\n\nconsistency.rack_aware_replicas\n\n\n\n\nNo\n\n\n0\n\n\nA number of rack-aware replicas\n\n\n\n\n\n\n\n\nData Availability of Consistency Level\n\n\nThis document delivers the relationship of \ndata availability\n and \nconfiguration level\n as below:\n\n\n\n\n\n\n\n\nData Availability\n\n\nConfiguration Level\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nExtremely Low\n\n\nn=2, r=1\nw=1, d=1\n\n\nData can not be acquired even if two nodes goes down \n(for personal use)\n\n\n\n\n\n\nLow\n\n\nn=3, r=1\nw=1, d=1\n\n\nLow data consistency\n\n\n\n\n\n\nMiddle(1)\n\n\nn=3, r=1\nw=2, d=2\n\n\nTypical settings\n\n\n\n\n\n\nMiddle(2)\n\n\nn=3, r=2\nw=2, d=2\n\n\nHigh data consistency than \nMiddle(1)\n\n\n\n\n\n\nHigh\n\n\nn=3, r=2\nw=3, d=3\n\n\nData can not be input and removed even if one node goes down\n\n\n\n\n\n\nExtremely High\n\n\nn=3, r=3\nw=3, d=3\n\n\nData can not be acquired even if one node goes down \n(can not be recommended)\n\n\n\n\n\n\n\n\nHow To Change Consistency Level\n\n\nYou can change \nconsistency.write\n, \nconsistency.read\n and \nconsistency.delete\n of the consistency level that you use the \nleofs-adm update-consistency-level\n command, but you cannot update \nnum_of_replicas\n and \nrack_aware_replicas\n.\n\n\n1\n2\n## Changes the consistency level to [w:2, d:2, r:1]\n\n$ leofs-adm update-consistency-level \n2\n \n2\n \n1\n\n\n\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Settings / LeoManager Settings", 
            "title": "Cluster Settings"
        }, 
        {
            "location": "/admin/settings/cluster/#cluster-settings", 
            "text": "This document outlines the various configuration items to keep in mind when planning a LeoFS system's cluster, and this documentation leads you to be able to configure its cluster when planning and launching it correctly.", 
            "title": "Cluster Settings"
        }, 
        {
            "location": "/admin/settings/cluster/#prior-knowledge", 
            "text": "LeoFS adopts  eventual consistency  of the consistency model; it takes priority over AP  (Availability and Partition tolerance)  over C  (consistency)  which depends on  CAP theorem .  To keep the consistency of objects eventually, LeoFS delivers the replication and recovery feature to automatically fix consistency of objects. You can configure the consistency level of a LeoFS system, and it is affected by the configuration.", 
            "title": "Prior Knowledge"
        }, 
        {
            "location": "/admin/settings/cluster/#how-to-keep-rings-consistency", 
            "text": "", 
            "title": "How to Keep RING's Consistency"
        }, 
        {
            "location": "/admin/settings/cluster/#case-1-both-leomanager-nodes-are-unavailable", 
            "text": "If both LeoManager nodes are unavailable, LeoStorage and LeoGateway nodes don't update the RING to keep its consistency into the LeoFS system.", 
            "title": "Case 1: Both LeoManager nodes are unavailable"
        }, 
        {
            "location": "/admin/settings/cluster/#case-2-one-leomanager-node-is-unavailable", 
            "text": "If a LeoManager node is unavailable, LeoFS can update the RING, and synchronize it with the LeoFS' system eventually. After restarting another LeoManager node, LeoManager automatically synchronizes the RING between the manager nodes.", 
            "title": "Case 2: One LeoManager node is unavailable"
        }, 
        {
            "location": "/admin/settings/cluster/#consistency-level", 
            "text": "Configure the consistency level of a LeoFS system at  LeoManager's configuration file - leo_manager_0.conf . You need to carefully configure the consistency level because it is not able to change some items after starting the system.  There are four configuration items at  leo_manager_0.conf , items of which have a great impact on  data availability  and  storage performance .     Item  Abbr  Modifiable  Default  Description      consistency.num_of_replicas  n  No  1  A number of replicas    consistency.write  w  Yes  1  A number of replicas needed for a successful WRITE operation    consistency.read  r  Yes  1  A number of replicas needed for a successful READ operation    consistency.delete  d  Yes  1  A number of replicas needed for a successful DELETE operation    consistency.rack_aware_replicas   No  0  A number of rack-aware replicas", 
            "title": "Consistency Level"
        }, 
        {
            "location": "/admin/settings/cluster/#data-availability-of-consistency-level", 
            "text": "This document delivers the relationship of  data availability  and  configuration level  as below:     Data Availability  Configuration Level  Description      Extremely Low  n=2, r=1 w=1, d=1  Data can not be acquired even if two nodes goes down  (for personal use)    Low  n=3, r=1 w=1, d=1  Low data consistency    Middle(1)  n=3, r=1 w=2, d=2  Typical settings    Middle(2)  n=3, r=2 w=2, d=2  High data consistency than  Middle(1)    High  n=3, r=2 w=3, d=3  Data can not be input and removed even if one node goes down    Extremely High  n=3, r=3 w=3, d=3  Data can not be acquired even if one node goes down  (can not be recommended)", 
            "title": "Data Availability of Consistency Level"
        }, 
        {
            "location": "/admin/settings/cluster/#how-to-change-consistency-level", 
            "text": "You can change  consistency.write ,  consistency.read  and  consistency.delete  of the consistency level that you use the  leofs-adm update-consistency-level  command, but you cannot update  num_of_replicas  and  rack_aware_replicas .  1\n2 ## Changes the consistency level to [w:2, d:2, r:1] \n$ leofs-adm update-consistency-level  2   2   1", 
            "title": "How To Change Consistency Level"
        }, 
        {
            "location": "/admin/settings/cluster/#related-links", 
            "text": "For Administrators / Settings / LeoManager Settings", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/settings/environment_config/", 
            "text": "Environment Config Files\n\n\nOverview\n\n\nStarting from v1.3.3, some environment variables used by launch scripts can be redefined in\n\n\"environment config files\"\n. They have shell syntax and are read by launch scripts.\n\n\n[Environment Files]\n\n\n1\n2\n3\n4\nleo_manager_0/etc/leo_manager.environment\nleo_manager_1/etc/leo_manager.environment\nleo_gateway/etc/leo_gateway.environment\nleo_storage/etc/leo_storage.environment\n\n\n\n\n\n\nChanging settings in these files is completely optional, but can be used to better organize directories used by LeoFS nodes and simplify upgrades. Here is highly customized example of \n.environment\n and \n.config\n files that allow LeoFS to store all work information and logs outside of default installation tree (\n/usr/local/leofs/\nversion\n).\n\n\nAs a result, upgrade process to newer version becomes as simple as placing \nleo_*.environment\n files in \netc\n directory of new version, for example (for \nleo_manager_0\n):\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n## Stop the process of LeoManager\n\n$ /usr/local/leofs/\nold_version\n/leo_manager_0/bin/leo_manager stop\n\n\n## Overwrite the environemnt file\n\n$ cp /usr/local/leofs/\nold_version\n/leo_manager_0/etc/leo_manager.environment \n\\\n\n     /usr/local/leofs/\nnew_version\n/leo_manager_0/etc/\n\n\n## Restart the process of LeoManager\n\n$ /usr/local/leofs/\nnew_version\n/leo_manager_0/bin/leo_manager start\n\n\n\n\n\n\nWith this, users can place actual config files (like \nleo_manager.conf\n) to the directory of their choice and change them independently of version upgrades, and the \n.environment\n files that need to be placed into installation tree don't need to be changed between versions. With the correct setup, since no work/temporary files will be kept in the installation tree, old version can be removed cleanly.\n\n\nExample Configuration\n\n\nContents of \n/usr/local/leofs/\nversion\n/leo_manager_0/etc/leo_manager.environment\n:\n\n\n1\n2\n3\n4\n# pick config file from fixed place\n\n\nRUNNER_ETC_DIR\n=\n/etc/leofs/leo_manager_0\n\n\n# store erlang.log.* and run_erl.log in this directory\n\n\nRUNNER_LOG_DIR\n=\n/var/log/leofs/leo_manager_0\n\n\n\n\n\n\n\nDirectories defined in \nRUNNER_ETC_DIR\n and \nRUNNER_LOG_DIR\n \n(in this example, \n/etc/leofs/leo_manager_0\n and \n/var/log/leofs/leo_manager_0\n)\n must be writable by \nleofs\n user, also \n$RUNNER_LOG_DIR/sasl\n \n(here \n/var/log/leofs/leo_manager_0/sasl\n)\n must exist:\n\n\n1\n2\n3\ndrwxr-xr-x. 2 leofs leofs 4096 Apr  4 20:40 /etc/leofs/leo_manager_0/\ndrwxr-xr-x. 4 leofs leofs 4096 Apr  5 20:00 /var/log/leofs/leo_manager_0/\ndrwxr-xr-x. 2 leofs leofs 4096 Apr  4 20:40 /var/log/leofs/leo_manager_0/sasl/\n\n\n\n\n\n\nIn \nleo_manager.conf\n, all options related to directories should point to external paths:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nsasl.sasl_error_log\n \n=\n \n/var/log/leofs/leo_manager_0/sasl/sasl-error.log\n\n\nsasl.error_logger_mf_dir\n \n=\n \n/var/log/leofs/leo_manager_0/sasl\n\n\nmnesia.dir\n \n=\n \n/var/local/leofs/leo_manager_0/work/mnesia/127.0.0.1\n\n\nqueue_dir\n \n=\n \n/var/local/leofs/leo_manager_0/work/queue\n\n\nlog.erlang\n \n=\n \n/var/log/leofs/leo_manager_0/erlang\n\n\nlog.app\n \n=\n \n/var/log/leofs/leo_manager_0/app\n\n\nlog.member_dir\n \n=\n \n/var/log/leofs/leo_manager_0/ring\n\n\nlog.ring_dir\n \n=\n \n/var/log/leofs/leo_manager_0/ring\n\n\nerlang.crash_dump\n \n=\n \n/var/log/leofs/leo_manager_0/erl_crash.dump\n\n\n\n\n\n\n\nFor \nleo_storage.conf\n it will be:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nsasl.sasl_error_log\n \n=\n \n/var/log/leofs/leo_storage/sasl/sasl-error.log\n\n\nsasl.error_logger_mf_dir\n \n=\n \n/var/log/leofs/leo_storage/sasl\n\n\nobj_containers.path\n \n=\n \n[/mnt/avs]\n\n\nlog.erlang\n \n=\n \n/var/log/leofs/leo_storage/erlang\n\n\nlog.app\n \n=\n \n/var/log/leofs/leo_storage/app\n\n\nlog.member_dir\n \n=\n \n/var/log/leofs/leo_storage/ring\n\n\nlog.ring_dir\n \n=\n \n/var/log/leofs/leo_storage/ring\n\n\nqueue_dir\n  \n=\n \n/var/local/leofs/leo_storage/work/queue\n\n\nleo_ordning_reda.temp_stacked_dir\n \n=\n \n/var/local/leofs/leo_storage/work/ord_reda/\n\n\nerlang.crash_dump\n \n=\n \n/var/log/leofs/leo_storage/erl_crash.dump\n\n\n\n\n\n\n\nFor \nleo_gateway.conf\n:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nsasl.sasl_error_log\n \n=\n \n/var/log/leofs/leo_gateway/sasl/sasl-error.log\n\n\nsasl.error_logger_mf_dir\n \n=\n \n/var/log/leofs/leo_gateway/sasl\n\n\nlog.erlang\n \n=\n \n/var/log/leofs/leo_gateway/erlang\n\n\nlog.app\n \n=\n \n/var/log/leofs/leo_gateway/app\n\n\nlog.member_dir\n \n=\n \n/var/log/leofs/leo_gateway/ring\n\n\nlog.ring_dir\n \n=\n \n/var/log/leofs/leo_gateway/ring\n\n\ncache.cache_disc_dir_data\n \n=\n \n/var/local/leofs/leo_gateway/cache/data\n\n\ncache.cache_disc_dir_journal\n \n=\n \n/var/local/leofs/leo_gateway/cache/journal\n\n\nqueue_dir\n \n=\n \n/var/local/leofs/leo_gateway/work/queue\n\n\nerlang.crash_dump\n \n=\n \n/var/log/leofs/leo_gateway/erl_crash.dump\n\n\n\n\n\n\n\nAll these directories must exist and have correct \nownership/permissions\n \n(writable by \nleofs\n user, unless set up otherwise)\n\n\nAdditional settings - SNMP config\n\n\nWhen pursuing \"pure\" system which keeps all the data out of installation tree, one might also decide to move SNMP agent config and \nSNMP db directories\n to external paths, by setting it in \nleo_manager.config\n:\n\n\n1\n2\n## leo_manager_0.conf\n\n\nsnmp_conf\n \n=\n \n/etc/leofs/leo_manager_0/leo_manager_snmp\n\n\n\n\n\n\n\nthen copying \n/usr/local/leofs/\nversion\n/leo_manager_0/snmp/snmpa_manager_0/leo_manager_snmp.config\n to \n/etc/leofs/leo_manager_0/leo_manager_snmp\n and setting\n\n\n1\n{\ndb_dir\n,\n \n/var/local/leofs/leo_manager_0/snmp_db\n},\n\n\n\n\n\n\n\nin \n/etc/leofs/leo_manager_0/leo_manager_snmp.config\n to make sure that absolutely no temporary files are created in \n/usr/local/leofs\n tree. It shouldn't matter otherwise since there is no need to keep contents of \nSNMP db directory\n between upgrades.\n\n\n(Here, copy of leo_manager_snmp.config was made so that original config would be untouched; while it is possible to change \ndb_dir\n in original \n/usr/local/leofs/\nversion\n/leo_manager_0/snmp/snmpa_manager_0/leo_manager_snmp.config\n as well, doing so would mean that this file needs to be replaced after each upgrade, reducing benefit of only changing \n.environment\n file after upgrade)\n\n\nNotice\n\n\nNote that this configuration is just an example of how to use \n.environment\n config features to move all the log files and config files out of the tree so they reside at fixed paths, to simplify configuration changes and upgrades as much as possible.\n\n\nThe resulting upgrade process can be less safe than original one suggested at \nFor Administrators / System Administration / System Migration\n, because the new version changes working \nmnesia\n and \nqueue\n directories upon launch and going back to the older version might be not always possible.\n\n\nUsers should consider making backups of work directories (\n/var/local/leofs\n in this example) before launching the newer version of a node.\n\n\nRelated Links\n\n\n\n\nFor Administrators / System Administration / System Migration", 
            "title": "Environment Configuration"
        }, 
        {
            "location": "/admin/settings/environment_config/#environment-config-files", 
            "text": "", 
            "title": "Environment Config Files"
        }, 
        {
            "location": "/admin/settings/environment_config/#overview", 
            "text": "Starting from v1.3.3, some environment variables used by launch scripts can be redefined in \"environment config files\" . They have shell syntax and are read by launch scripts.  [Environment Files]  1\n2\n3\n4 leo_manager_0/etc/leo_manager.environment\nleo_manager_1/etc/leo_manager.environment\nleo_gateway/etc/leo_gateway.environment\nleo_storage/etc/leo_storage.environment   Changing settings in these files is completely optional, but can be used to better organize directories used by LeoFS nodes and simplify upgrades. Here is highly customized example of  .environment  and  .config  files that allow LeoFS to store all work information and logs outside of default installation tree ( /usr/local/leofs/ version ).  As a result, upgrade process to newer version becomes as simple as placing  leo_*.environment  files in  etc  directory of new version, for example (for  leo_manager_0 ):  1\n2\n3\n4\n5\n6\n7\n8\n9 ## Stop the process of LeoManager \n$ /usr/local/leofs/ old_version /leo_manager_0/bin/leo_manager stop ## Overwrite the environemnt file \n$ cp /usr/local/leofs/ old_version /leo_manager_0/etc/leo_manager.environment  \\ \n     /usr/local/leofs/ new_version /leo_manager_0/etc/ ## Restart the process of LeoManager \n$ /usr/local/leofs/ new_version /leo_manager_0/bin/leo_manager start   With this, users can place actual config files (like  leo_manager.conf ) to the directory of their choice and change them independently of version upgrades, and the  .environment  files that need to be placed into installation tree don't need to be changed between versions. With the correct setup, since no work/temporary files will be kept in the installation tree, old version can be removed cleanly.", 
            "title": "Overview"
        }, 
        {
            "location": "/admin/settings/environment_config/#example-configuration", 
            "text": "Contents of  /usr/local/leofs/ version /leo_manager_0/etc/leo_manager.environment :  1\n2\n3\n4 # pick config file from fixed place  RUNNER_ETC_DIR = /etc/leofs/leo_manager_0  # store erlang.log.* and run_erl.log in this directory  RUNNER_LOG_DIR = /var/log/leofs/leo_manager_0    Directories defined in  RUNNER_ETC_DIR  and  RUNNER_LOG_DIR   (in this example,  /etc/leofs/leo_manager_0  and  /var/log/leofs/leo_manager_0 )  must be writable by  leofs  user, also  $RUNNER_LOG_DIR/sasl   (here  /var/log/leofs/leo_manager_0/sasl )  must exist:  1\n2\n3 drwxr-xr-x. 2 leofs leofs 4096 Apr  4 20:40 /etc/leofs/leo_manager_0/\ndrwxr-xr-x. 4 leofs leofs 4096 Apr  5 20:00 /var/log/leofs/leo_manager_0/\ndrwxr-xr-x. 2 leofs leofs 4096 Apr  4 20:40 /var/log/leofs/leo_manager_0/sasl/   In  leo_manager.conf , all options related to directories should point to external paths:  1\n2\n3\n4\n5\n6\n7\n8\n9 sasl.sasl_error_log   =   /var/log/leofs/leo_manager_0/sasl/sasl-error.log  sasl.error_logger_mf_dir   =   /var/log/leofs/leo_manager_0/sasl  mnesia.dir   =   /var/local/leofs/leo_manager_0/work/mnesia/127.0.0.1  queue_dir   =   /var/local/leofs/leo_manager_0/work/queue  log.erlang   =   /var/log/leofs/leo_manager_0/erlang  log.app   =   /var/log/leofs/leo_manager_0/app  log.member_dir   =   /var/log/leofs/leo_manager_0/ring  log.ring_dir   =   /var/log/leofs/leo_manager_0/ring  erlang.crash_dump   =   /var/log/leofs/leo_manager_0/erl_crash.dump    For  leo_storage.conf  it will be:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 sasl.sasl_error_log   =   /var/log/leofs/leo_storage/sasl/sasl-error.log  sasl.error_logger_mf_dir   =   /var/log/leofs/leo_storage/sasl  obj_containers.path   =   [/mnt/avs]  log.erlang   =   /var/log/leofs/leo_storage/erlang  log.app   =   /var/log/leofs/leo_storage/app  log.member_dir   =   /var/log/leofs/leo_storage/ring  log.ring_dir   =   /var/log/leofs/leo_storage/ring  queue_dir    =   /var/local/leofs/leo_storage/work/queue  leo_ordning_reda.temp_stacked_dir   =   /var/local/leofs/leo_storage/work/ord_reda/  erlang.crash_dump   =   /var/log/leofs/leo_storage/erl_crash.dump    For  leo_gateway.conf :   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 sasl.sasl_error_log   =   /var/log/leofs/leo_gateway/sasl/sasl-error.log  sasl.error_logger_mf_dir   =   /var/log/leofs/leo_gateway/sasl  log.erlang   =   /var/log/leofs/leo_gateway/erlang  log.app   =   /var/log/leofs/leo_gateway/app  log.member_dir   =   /var/log/leofs/leo_gateway/ring  log.ring_dir   =   /var/log/leofs/leo_gateway/ring  cache.cache_disc_dir_data   =   /var/local/leofs/leo_gateway/cache/data  cache.cache_disc_dir_journal   =   /var/local/leofs/leo_gateway/cache/journal  queue_dir   =   /var/local/leofs/leo_gateway/work/queue  erlang.crash_dump   =   /var/log/leofs/leo_gateway/erl_crash.dump    All these directories must exist and have correct  ownership/permissions   (writable by  leofs  user, unless set up otherwise)", 
            "title": "Example Configuration"
        }, 
        {
            "location": "/admin/settings/environment_config/#additional-settings-snmp-config", 
            "text": "When pursuing \"pure\" system which keeps all the data out of installation tree, one might also decide to move SNMP agent config and  SNMP db directories  to external paths, by setting it in  leo_manager.config :  1\n2 ## leo_manager_0.conf  snmp_conf   =   /etc/leofs/leo_manager_0/leo_manager_snmp    then copying  /usr/local/leofs/ version /leo_manager_0/snmp/snmpa_manager_0/leo_manager_snmp.config  to  /etc/leofs/leo_manager_0/leo_manager_snmp  and setting  1 { db_dir ,   /var/local/leofs/leo_manager_0/snmp_db },    in  /etc/leofs/leo_manager_0/leo_manager_snmp.config  to make sure that absolutely no temporary files are created in  /usr/local/leofs  tree. It shouldn't matter otherwise since there is no need to keep contents of  SNMP db directory  between upgrades.  (Here, copy of leo_manager_snmp.config was made so that original config would be untouched; while it is possible to change  db_dir  in original  /usr/local/leofs/ version /leo_manager_0/snmp/snmpa_manager_0/leo_manager_snmp.config  as well, doing so would mean that this file needs to be replaced after each upgrade, reducing benefit of only changing  .environment  file after upgrade)", 
            "title": "Additional settings - SNMP config"
        }, 
        {
            "location": "/admin/settings/environment_config/#notice", 
            "text": "Note that this configuration is just an example of how to use  .environment  config features to move all the log files and config files out of the tree so they reside at fixed paths, to simplify configuration changes and upgrades as much as possible.  The resulting upgrade process can be less safe than original one suggested at  For Administrators / System Administration / System Migration , because the new version changes working  mnesia  and  queue  directories upon launch and going back to the older version might be not always possible.  Users should consider making backups of work directories ( /var/local/leofs  in this example) before launching the newer version of a node.", 
            "title": "Notice"
        }, 
        {
            "location": "/admin/settings/environment_config/#related-links", 
            "text": "For Administrators / System Administration / System Migration", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/settings/leo_manager/", 
            "text": "LeoManager Settings\n\n\nPrior Knowledge\n\n\nThe current version, v1.3 of LeoManager depends on \nErlang Mnesia, A distributed telecommunications DBMS\n to manage configurations of a LeoFS system and information of all nodes. LeoManager nodes must keep running to replicate the data for preventing data loss. You need to configure both LeoManager master and the slave.\n\n\nOther Configurations\n\n\nIf you want to modify settings like where to place \nleo_manager.conf\n, what user is starting a LeoManager process and so on, refer \nFor Administrators / Settings / Environment Configuration\n for more information.\n\n\nConfiguration\n\n\nThere are some configuration differences between LeoManager-master and LeoManager-slave. LeoManager-master only has \nthe consistency level\n and \nthe multi datacenter replication\n.\n\n\nThe default setting is to launch a LeoFS system on one node, whose setting cannot replicate data because the total number of the replica is one, and data loss could happen with high probability. You need to modify the configuration suitably before launching the LeoFS system on your production or other environments.\n\n\nLeoManager Configurations\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nBasic\n\n\n\n\n\n\n\n\nmanager.partner\n\n\nThe partner of manager's alias. This configuration is necessary for communicationg between \nLeoManager\ns master\n and \nLeoManager\ns slave\n. \n( Default: manager_1@127.0.0.1 )\n\n\n\n\n\n\nconsole.port.cui\n\n\nThe port number of LeoManager's console for text format\n( Default: 10010 )\n\n\n\n\n\n\nconsole.port.json\n\n\nThe port number of LeoManager's console for JSON format\n( Default: 10020 )\n\n\n\n\n\n\nconsole.acceptors.cui\n\n\nThe maximum number of acceptors of LeoManager's console for text format\n( Default: 3 )\n\n\n\n\n\n\nconsole.acceptors.json\n\n\nThe maximum number of acceptors of LeoManager's console for JSON format\n( Default:16 )\n\n\n\n\n\n\nSystem\n\n\n\n\n\n\n\n\nsystem.dc_id\n\n\nDatacenter ID\n is necessary for using the data center replication\n( Default: dc_1 )\n\n\n\n\n\n\nsystem.cluster_id\n\n\nCluster ID\n is also necessary for using the data center replication\n( Default: leofs_1 )\n\n\n\n\n\n\nConsistency Level\n\n\n\n\n\n\n\n\nconsistency.num_of_replicas\n\n\nonly LeoManager\ns master\n The total number of object copies\n( Default: 1 )\n\n\n\n\n\n\nconsistency.write\n\n\nonly LeoManager\ns master\n The total number of object copies needed for a successful WRITE operation\n( Default: 1 )\n\n\n\n\n\n\nconsistency.read\n\n\nonly LeoManager\ns master\n The total number of object copies needed for a successful READ operation\n( Default: 1 )\n\n\n\n\n\n\nconsistency.delete\n\n\nonly LeoManager\ns master\n The total number of object copies needed for a successful DELETE operation\n( Default: 1 )\n\n\n\n\n\n\nconsistency.rack_aware_replicas\n\n\nonly LeoManager\ns master\n The total number of object copies of rack-aware\n( Default: 0 )\n\n\n\n\n\n\nMulti Data Center Replication\n\n\n\n\n\n\n\n\nmdc_replication.max_targets\n\n\nonly LeoManager\ns master\n The maximum number of replication targets of clusters OR data centers\n( Default: 2 )\n\n\n\n\n\n\nmdc_replication.num_of_replicas_a_dc\n\n\nonly LeoManager\ns master\n A remote cluster of a LeoFS system which receives this cluster's objects, and then replicates them, which adhere to a replication method of each object\n( Default: 1 )\n\n\n\n\n\n\nmdc_replication.consistency.write\n\n\nonly LeoManager\ns master\n \n[since 1.3.3]\n \n A number of replicas needed for a successful WRITE-operation\n( Default: 1 )\n\n\n\n\n\n\nmdc_replication.consistency.read\n\n\nonly LeoManager\ns master\n \n[since 1.3.3]\n \n A number of replicas needed for a successful READ-operation\n( Default: 1 )\n\n\n\n\n\n\nmdc_replication.consistency.delete\n\n\nonly LeoManager\ns master\n \n[since 1.3.3]\n \n A number of replicas needed for a successful DELETE-operation\n( Default: 1 )\n\n\n\n\n\n\nRPC for Multi Datacenter Replication\n\n\n\n\n\n\n\n\nrpc.server.acceptors\n\n\nThe total number of acceptor of the RPC server\n( Default: 16 )\n\n\n\n\n\n\nrpc.server.listen_port\n\n\nThe listening port of the RPC server\n( Default: 13075 )\n\n\n\n\n\n\nrpc.server.listen_timeout\n\n\nThe listening timeout\n( Default: 5000 )\n\n\n\n\n\n\nrpc.client.connection_pool_size\n\n\nA client is able to keep connections of a remote LeoFS up to the pool size\n( Default: 16 )\n\n\n\n\n\n\nrpc.client.connection_buffer_size\n\n\nA client is able to increase connections of a remote LeoFS up to the buffer size\n( Default: 16 )\n\n\n\n\n\n\nMnesia\n\n\n\n\n\n\n\n\nmnesia.dir\n\n\nThe directory of the database file of Mnesia*(Erlang distributed DB)*\n( Default: ./work/mnesia/127.0.0.1 )\n\n\n\n\n\n\nmnesia.dump_log_write_threshold\n\n\nThe maximum number of writes allowed to the transaction log before a new dump of the log is performed. Default is 100 log writes.\n- See also: \nErlang Mnesia dump_log_write_threshold\n( Default: 50000 )\n\n\n\n\n\n\nmnesia.dc_dump_limit\n\n\nMnesia's tables are dumped when \nfilesize(Log) \n (filesize(Tab)/Dc_dump_limit)\n. Lower values reduce CPU overhead but increase disk space and startup times. Default is 4.\n- See also: \nErlang Mnesia\n( Default: 40 )\n\n\n\n\n\n\nLog\n\n\n\n\n\n\n\n\nlog.log_level\n\n\nLeoManager's logger controls outputting logs by the log level:\n1: Info\n2: Warn\n3: Error\n( Default: 1 )\n\n\n\n\n\n\nlog.erlang\n\n\nThe output destination of Erlang's logs\n( Default: ./log/erlang )\n\n\n\n\n\n\nlog.app\n\n\nThe output destination of LeoManager's logs\n( Default: ./log/app )\n\n\n\n\n\n\nlog.member_dir\n\n\nThe output destination of the member's dump file\n( Default: ./log/ring )\n\n\n\n\n\n\nlog.ring_dir\n\n\nThe output destination of the RING's dump file\n( Default: ./log/ring )\n\n\n\n\n\n\nOther Directories\n\n\n\n\n\n\n\n\nqueue_dir\n\n\nThe directory of the data file of LeoFS' MQ\n( Default: ./work/queue )\n\n\n\n\n\n\nsnmp_agent\n\n\nThe directory of the snmp agent file of LeoFS\n( Default: ./snmp/snmpa_manager_0/LEO-MANAGER )\n\n\n\n\n\n\n\n\nErlang VM's Related Configurations\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nnodename\n\n\nThe format of the node name is \nNAME\n@\nIP-ADDRESS\n, which must be unique always in a LeoFS system\n( Default: manager_0@127.0.0.1 )\n\n\n\n\n\n\ndistributed_cookie\n\n\nSets the magic cookie of the node to \nCookie\n. \n- See also: \nDistributed Erlang\n( Default: 401321b4 )\n\n\n\n\n\n\nerlang.kernel_poll\n\n\nKernel poll reduces LeoFS' CPU usage when it has hundreds (or more) network connections\n( Default: true )\n\n\n\n\n\n\nerlang.asyc_threads\n\n\nThe total number of Erlang aynch threads for the async thread pool. \nThe asynchronous thread pool\n are OS threads which are userd for I/O operations.\n( Default: 32 )\n\n\n\n\n\n\nerlang.max_ports\n\n\nThe max_ports sets the default value of maximum number of ports.\n- See also: \nErlang erlang:open_port/2\n( Default: 64000 )\n\n\n\n\n\n\nerlang.crash_dump\n\n\nThe output destination of an Erlang crash dump\n( Default: ./log/erl_crash.dump )\n\n\n\n\n\n\nerlang.max_ets_tables\n\n\nThe maxinum number of \nErlagn ETS\n tables\n( Default: 256000 )\n\n\n\n\n\n\nerlang.smp\n\n\n-smp\n enable and \n-smp\n start the Erlang runtime system with \nSMP\n support enabled\n( Default: enable )\n\n\n\n\n\n\nprocess_limit\n\n\nThe maxinum number of Erlang processes. Sets the maximum number of simultaneously existing processes for this system if a Number is passed as value. Valid range for Number is [1024-134217727]\n( Default: 1048576 )\n\n\n\n\n\n\n\n\nRelated Links\n\n\n\n\nConcept and Architecture / LeoManager's Architecture\n\n\nFor Administrators / Settings / Cluster Settings\n\n\nFor Administrators / Settings / Environment Configuration\n\n\nFor Administrators / System Operations / Multi Data Center Replication", 
            "title": "LeoManager Settings"
        }, 
        {
            "location": "/admin/settings/leo_manager/#leomanager-settings", 
            "text": "", 
            "title": "LeoManager Settings"
        }, 
        {
            "location": "/admin/settings/leo_manager/#prior-knowledge", 
            "text": "The current version, v1.3 of LeoManager depends on  Erlang Mnesia, A distributed telecommunications DBMS  to manage configurations of a LeoFS system and information of all nodes. LeoManager nodes must keep running to replicate the data for preventing data loss. You need to configure both LeoManager master and the slave.", 
            "title": "Prior Knowledge"
        }, 
        {
            "location": "/admin/settings/leo_manager/#other-configurations", 
            "text": "If you want to modify settings like where to place  leo_manager.conf , what user is starting a LeoManager process and so on, refer  For Administrators / Settings / Environment Configuration  for more information.", 
            "title": "Other Configurations"
        }, 
        {
            "location": "/admin/settings/leo_manager/#configuration", 
            "text": "There are some configuration differences between LeoManager-master and LeoManager-slave. LeoManager-master only has  the consistency level  and  the multi datacenter replication .  The default setting is to launch a LeoFS system on one node, whose setting cannot replicate data because the total number of the replica is one, and data loss could happen with high probability. You need to modify the configuration suitably before launching the LeoFS system on your production or other environments.", 
            "title": "Configuration"
        }, 
        {
            "location": "/admin/settings/leo_manager/#leomanager-configurations", 
            "text": "Item  Description      Basic     manager.partner  The partner of manager's alias. This configuration is necessary for communicationg between  LeoManager s master  and  LeoManager s slave .  ( Default: manager_1@127.0.0.1 )    console.port.cui  The port number of LeoManager's console for text format ( Default: 10010 )    console.port.json  The port number of LeoManager's console for JSON format ( Default: 10020 )    console.acceptors.cui  The maximum number of acceptors of LeoManager's console for text format ( Default: 3 )    console.acceptors.json  The maximum number of acceptors of LeoManager's console for JSON format ( Default:16 )    System     system.dc_id  Datacenter ID  is necessary for using the data center replication ( Default: dc_1 )    system.cluster_id  Cluster ID  is also necessary for using the data center replication ( Default: leofs_1 )    Consistency Level     consistency.num_of_replicas  only LeoManager s master  The total number of object copies ( Default: 1 )    consistency.write  only LeoManager s master  The total number of object copies needed for a successful WRITE operation ( Default: 1 )    consistency.read  only LeoManager s master  The total number of object copies needed for a successful READ operation ( Default: 1 )    consistency.delete  only LeoManager s master  The total number of object copies needed for a successful DELETE operation ( Default: 1 )    consistency.rack_aware_replicas  only LeoManager s master  The total number of object copies of rack-aware ( Default: 0 )    Multi Data Center Replication     mdc_replication.max_targets  only LeoManager s master  The maximum number of replication targets of clusters OR data centers ( Default: 2 )    mdc_replication.num_of_replicas_a_dc  only LeoManager s master  A remote cluster of a LeoFS system which receives this cluster's objects, and then replicates them, which adhere to a replication method of each object ( Default: 1 )    mdc_replication.consistency.write  only LeoManager s master   [since 1.3.3]    A number of replicas needed for a successful WRITE-operation ( Default: 1 )    mdc_replication.consistency.read  only LeoManager s master   [since 1.3.3]    A number of replicas needed for a successful READ-operation ( Default: 1 )    mdc_replication.consistency.delete  only LeoManager s master   [since 1.3.3]    A number of replicas needed for a successful DELETE-operation ( Default: 1 )    RPC for Multi Datacenter Replication     rpc.server.acceptors  The total number of acceptor of the RPC server ( Default: 16 )    rpc.server.listen_port  The listening port of the RPC server ( Default: 13075 )    rpc.server.listen_timeout  The listening timeout ( Default: 5000 )    rpc.client.connection_pool_size  A client is able to keep connections of a remote LeoFS up to the pool size ( Default: 16 )    rpc.client.connection_buffer_size  A client is able to increase connections of a remote LeoFS up to the buffer size ( Default: 16 )    Mnesia     mnesia.dir  The directory of the database file of Mnesia*(Erlang distributed DB)* ( Default: ./work/mnesia/127.0.0.1 )    mnesia.dump_log_write_threshold  The maximum number of writes allowed to the transaction log before a new dump of the log is performed. Default is 100 log writes. - See also:  Erlang Mnesia dump_log_write_threshold ( Default: 50000 )    mnesia.dc_dump_limit  Mnesia's tables are dumped when  filesize(Log)   (filesize(Tab)/Dc_dump_limit) . Lower values reduce CPU overhead but increase disk space and startup times. Default is 4. - See also:  Erlang Mnesia ( Default: 40 )    Log     log.log_level  LeoManager's logger controls outputting logs by the log level: 1: Info 2: Warn 3: Error ( Default: 1 )    log.erlang  The output destination of Erlang's logs ( Default: ./log/erlang )    log.app  The output destination of LeoManager's logs ( Default: ./log/app )    log.member_dir  The output destination of the member's dump file ( Default: ./log/ring )    log.ring_dir  The output destination of the RING's dump file ( Default: ./log/ring )    Other Directories     queue_dir  The directory of the data file of LeoFS' MQ ( Default: ./work/queue )    snmp_agent  The directory of the snmp agent file of LeoFS ( Default: ./snmp/snmpa_manager_0/LEO-MANAGER )", 
            "title": "LeoManager Configurations"
        }, 
        {
            "location": "/admin/settings/leo_manager/#erlang-vms-related-configurations", 
            "text": "Item  Description      nodename  The format of the node name is  NAME @ IP-ADDRESS , which must be unique always in a LeoFS system ( Default: manager_0@127.0.0.1 )    distributed_cookie  Sets the magic cookie of the node to  Cookie .  - See also:  Distributed Erlang ( Default: 401321b4 )    erlang.kernel_poll  Kernel poll reduces LeoFS' CPU usage when it has hundreds (or more) network connections ( Default: true )    erlang.asyc_threads  The total number of Erlang aynch threads for the async thread pool.  The asynchronous thread pool  are OS threads which are userd for I/O operations. ( Default: 32 )    erlang.max_ports  The max_ports sets the default value of maximum number of ports. - See also:  Erlang erlang:open_port/2 ( Default: 64000 )    erlang.crash_dump  The output destination of an Erlang crash dump ( Default: ./log/erl_crash.dump )    erlang.max_ets_tables  The maxinum number of  Erlagn ETS  tables ( Default: 256000 )    erlang.smp  -smp  enable and  -smp  start the Erlang runtime system with  SMP  support enabled ( Default: enable )    process_limit  The maxinum number of Erlang processes. Sets the maximum number of simultaneously existing processes for this system if a Number is passed as value. Valid range for Number is [1024-134217727] ( Default: 1048576 )", 
            "title": "Erlang VM's Related Configurations"
        }, 
        {
            "location": "/admin/settings/leo_manager/#related-links", 
            "text": "Concept and Architecture / LeoManager's Architecture  For Administrators / Settings / Cluster Settings  For Administrators / Settings / Environment Configuration  For Administrators / System Operations / Multi Data Center Replication", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/settings/leo_storage/", 
            "text": "LeoStroage Settings\n\n\nPrior Knowledge\n\n\n\n\nNote: Configuration\n\n\nLeoStroage's features depend on its configuration. If once a LeoFS system is launched, you cannot modify the following LeoStorage's configurations because the algorithm of the data operation strictly adheres to the settings.\n\n\n\n\nIrrevocable and Attention Required Items:\n\n\n\n\n\n\n\n\nItem\n\n\nIrrevocable?\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nLeoStorage Basic\n\n\n\n\n\n\n\n\n\n\nobj_containers.path\n\n\nModifiable with condition\n\n\nAble to change the directory of the container(s) but not able to add or remove the directory(s). You need to move the data files which are \nobj_containers.path\n/avs/object\n and \nobj_containers.path\n/avs/metadata\n, which adhere to this configuration.\n\n\n\n\n\n\nobj_containers.num_of_containers\n\n\nYes\n\n\nNot able to change the configuration because LeoStorage cannot retrieve objects or metadatas.\n\n\n\n\n\n\nobj_containers.metadata_storage\n\n\nYes\n\n\nAs above\n\n\n\n\n\n\nnum_of_vnodes`\n\n\nYes\n\n\nAs above\n\n\n\n\n\n\nMQ\n\n\n\n\n\n\n\n\n\n\nmq.backend_db\n\n\nModifiable with condition\n\n\nLose all the MQ's data after changing\n\n\n\n\n\n\nmq.num_of_mq_procs\n\n\nModifiable with condition\n\n\nAs above\n\n\n\n\n\n\nReplication and Recovery object(s)\n\n\n\n\n\n\n\n\n\n\nreplication.rack_awareness.rack_id\n\n\nYes\n\n\nNot able to change the configuration because LeoFS cannot retrieve objects or metadatas.\n\n\n\n\n\n\nOther Directories Settings\n\n\n\n\n\n\n\n\n\n\nqueue_dir\n\n\nModifiable with condition\n\n\nAble to change the MQ's directory but you need to move the MQ's data, which adhere to this configuration.\n\n\n\n\n\n\n\n\nOther Configurations\n\n\nIf you want to modify settings like where to place \nleo_storage.conf\n, what user is starting a LeoStorage process and so on, refer \nFor Administrators / Settings / Environment Configuration\n for more information.\n\n\nConfiguration\n\n\nLeoStorage Configurations\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nLeoManager Nodes\n\n\n\n\n\n\n\n\nmanagers\n\n\nName of LeoManager nodes. This configuration is necessary for communicating with \nLeoManager\ns master\n and \nLeoManager\ns slave\n.\n( Default: [manager_0@127.0.0.1, manager_1@127.0.0.1] )\n\n\n\n\n\n\nLeoStorage Basic\n\n\n\n\n\n\n\n\nobj_containers.path\n\n\nDirectories of object-containers\n( Default: [./avs] )\n\n\n\n\n\n\nobj_containers.num_of_containers\n\n\nA number of object-containers of each directory\n( Default: [8] )\n\n\n\n\n\n\nobj_containers.sync_mode\n\n\nMode of the data synchronization. There're three modes:\nnone\n: Not synchronization every time \n(default)\nperiodic\n: Periodic synchronization which depends on \nobj_containers.sync_interval_in_ms\nwritethrough\n: Ensures that any buffers kept by the OS are written to disk every time\n( Default: none )\n\n\n\n\n\n\nobj_containers.sync_interval_in_ms\n\n\nInterval in ms of the data synchronization\n( Default: 1000, Unit: \nmsec\n )\n\n\n\n\n\n\nobj_containers.metadata_storage\n\n\nThe metadata storage feature is pluggable which depends on \nbitcask\n and \nleveldb\n.\n( Default: leveldb )\n\n\n\n\n\n\nnum_of_vnodes\n\n\nThe total number of virtual-nodes of a LeoStroage node for generating the distributed hashtable (RING)\n( Default: 168 )\n\n\n\n\n\n\nobject_storage.is_strict_check\n\n\nEnable strict check between checksum of a metadata and checksum of an object.\n( Default: false )\n\n\n\n\n\n\nobject_storage.threshold_of_slow_processing\n\n\nThreshold of slow processing\n( Default: 1000, Unit: \nmsec\n )\n\n\n\n\n\n\nseeking_timeout_per_metadata\n\n\nTimeout of seeking metadatas per a metadata\n( Default: 10, Unit: \nmsec\n )\n\n\n\n\n\n\nmax_num_of_procs\n\n\nMaximum number of processes for both write and read operation\n( Default: 3000 )\n\n\n\n\n\n\nnum_of_obj_storage_read_procs\n\n\nTotal number of obj-storage-read processes per object-container, AVS\nRange: [1..100]\n( Default: 3 )\n\n\n\n\n\n\nWatchdog\n\n\n\n\n\n\n\n\nwatchdog.common.loosen_control_at_safe_count\n\n\nWhen reach a number of safe \n(clear watchdog)\n, a watchdog loosen the control\n( Default: 1 )\n\n\n\n\n\n\nWatchdog / REX\n\n\n\n\n\n\n\n\nwatchdog.rex.is_enabled\n\n\nEnables or disables the rex-watchdog which monitors the memory usage of \nErlang's RPC component\n.\n( Default: true )\n\n\n\n\n\n\nwatchdog.rex.interval\n\n\nAn interval of executing the watchdog processing\n( Default: 10, Unit: \nsec\n )\n\n\n\n\n\n\nwatchdog.rex.threshold_mem_capacity\n\n\nThreshold of memory capacity of binary for Erlang rex\n( Default: 33554432, Unit: \nbyte\n )\n\n\n\n\n\n\nWatchdog / CPU\n\n\n\n\n\n\n\n\nwatchdog.cpu.is_enabled\n\n\nEnables or disables the CPU-watchdog which monitors both \nCPU load average\n and \nCPU utilization\n( Default: false )\n\n\n\n\n\n\nwatchdog.cpu.raised_error_times\n\n\nTimes of raising error to a client\n( Default: 5 )\n\n\n\n\n\n\nwatchdog.cpu.interval\n\n\nAn interval of executing the watchdog processing\n( Default: 10, Unit: \nsec\n )\n\n\n\n\n\n\nwatchdog.cpu.threshold_cpu_load_avg\n\n\nThreshold of CPU load average\n( Default: 5.0 )\n\n\n\n\n\n\nwatchdog.cpu.threshold_cpu_util\n\n\nThreshold of CPU utilization\n( Default: 100 )\n\n\n\n\n\n\nWatchdog / DISK\n\n\n\n\n\n\n\n\nwatchdog.disk.is_enabled\n\n\nEnables or disables the \n( Default: false )\n\n\n\n\n\n\nwatchdog.disk.raised_error_times\n\n\nTimes of raising error to a client\n( Default: 5 )\n\n\n\n\n\n\nwatchdog.disk.interval\n\n\nAn interval of executing the watchdog processing\n( Default: 10, Unit: \nsec\n )\n\n\n\n\n\n\nwatchdog.disk.threshold_disk_use\n\n\nThreshold of Disk use(%) of a target disk's capacity\n( Default: 85, Unit: \npercent\n )\n\n\n\n\n\n\nwatchdog.disk.threshold_disk_util\n\n\nThreshold of Disk utilization\n( Default: 90, Unit: \npercent\n )\n\n\n\n\n\n\nwatchdog.disk.threshold_disk_rkb\n\n\nThreshold of disk read KB/sec\n( Default: 98304, Unit: \nKB\n )\n\n\n\n\n\n\nwatchdog.disk.threshold_disk_wkb\n\n\nThreshold of disk write KB/sec\n( Default: 98304, Unit: \nKB\n )\n\n\n\n\n\n\nwatchdog.disk.target_devices\n\n\nTarget devices for checking disk utilization\n( Default: [] )\n\n\n\n\n\n\nWatchdog / CLUSTER\n\n\n\n\n\n\n\n\nwatchdog.cluster.is_enabled\n\n\nEnables or disables the \n( Default: false )\n\n\n\n\n\n\nwatchdog.cluster.interval\n\n\nAn interval of executing the watchdog processing\n( Default: 10 )\n\n\n\n\n\n\nWatchdog / ERRORS\n\n\n\n\n\n\n\n\nwatchdog.error.is_enabled\n\n\nEnables or disables the \n( Default: false )\n\n\n\n\n\n\nwatchdog.error.interval\n\n\nAn interval of executing the watchdog processing\n( Default: 60 )\n\n\n\n\n\n\nwatchdog.error.threshold_count\n\n\nTotal counts of raising error to a client\n( Default: 100 )\n\n\n\n\n\n\nData Compaction\n\n\n\n\n\n\n\n\nData Compaction / Basic\n\n\n\n\n\n\n\n\ncompaction.limit_num_of_compaction_procs\n\n\nLimit of a number of procs to execute data-compaction in parallel\n( Default: 4 )\n\n\n\n\n\n\ncompaction.skip_prefetch_size\n\n\nPerfetch size when skipping garbage\n( Default: 512 )\n\n\n\n\n\n\ncompaction.waiting_time_regular\n\n\nRegular value of compaction-proc waiting time/batch-proc\n( Default: 500, Unit: \nmsec\n )\n\n\n\n\n\n\ncompaction.waiting_time_max\n\n\nMaximum value of compaction-proc waiting time/batch-proc\n( Default: 3000, Unit: \nmsec\n )\n\n\n\n\n\n\ncompaction.batch_procs_regular\n\n\nTotal number of regular compaction batch processes\n( Default: 1000 )\n\n\n\n\n\n\ncompaction.batch_procs_max\n\n\nMaximum number of compaction batch processes\n( Default: 1500 )\n\n\n\n\n\n\nData Compaction / Automated Data Compaction\n\n\n\n\n\n\n\n\nautonomic_op.compaction.is_enabled\n\n\nEnables or disables the auto-compaction\n( Default: false )\n\n\n\n\n\n\nautonomic_op.compaction.parallel_procs\n\n\nTotal number of parallel processes\n( Default: 1 )\n\n\n\n\n\n\nautonomic_op.compaction.interval\n\n\nAn interval time of between auto-comcations\n( Default: 3600, Unit: \nsec\n )\n\n\n\n\n\n\nautonomic_op.compaction.warn_active_size_ratio\n\n\nWarning ratio of active size\n( Default: 70, Unit: \npercent\n )\n\n\n\n\n\n\nautonomic_op.compaction.threshold_active_size_ratio\n\n\nThreshold ratio of active size. LeoStroage start data-comaction after reaching it\n( Default: 60, \npercent\n )\n\n\n\n\n\n\nMQ\n\n\n\n\n\n\n\n\nmq.backend_db\n\n\nThe MQ storage feature is pluggable which depends on \nbitcask\n and \nleveldb\n.\n( Default: leveldb )\n\n\n\n\n\n\nmq.num_of_mq_procs\n\n\nA number of mq-server's processes\n( Default: 8 )\n\n\n\n\n\n\nmq.num_of_batch_process_max\n\n\nMaximum number of bach processes of message\n( Default: 3000 )\n\n\n\n\n\n\nmq.num_of_batch_process_regular\n\n\nRegular value of bach processes of message\n( Default: 1600 )\n\n\n\n\n\n\nmq.interval_between_batch_procs_max\n\n\nMaximum value of interval between batch-procs\n( Default: 3000, Unit: \nmsec\n )\n\n\n\n\n\n\nmq.interval_between_batch_procs_regular\n\n\nRegular value of interval between batch-procs\n( Default: 500, Unit: \nmsec\n )\n\n\n\n\n\n\nBackend DB / eleveldb\n\n\n\n\n\n\n\n\nbackend_db.eleveldb.write_buf_size\n\n\nWrite Buffer Size. Larger values increase performance, especially during bulk loads.\nUp to two write buffers may be held in memory at the same time, so you may wish to adjust this parameter to control memory usage.Also, a larger write buffer will result in a longer recovery time the next time the database is opened.\n( Default: 62914560 )\n\n\n\n\n\n\nbackend_db.eleveldb.max_open_files\n\n\nMax Open Files. Number of open files that can be used by the DB. You may need to increase this if your database has a large working set \n(budget one open file per 2MB of working set)\n.\n( Default: 1000 )\n\n\n\n\n\n\nbackend_db.eleveldb.sst_block_size\n\n\nThe size of a data block is controlled by the SST block size. The size represents a threshold, not a fixed count. Whenever a newly created block reaches this uncompressed size, leveldb considers it full and writes the block with its metadata to disk. The number of keys contained in the block depends upon the size of the values and keys.\n( Default: 4096 )\n\n\n\n\n\n\nReplication and Recovery object(s)\n\n\n\n\n\n\n\n\nreplication.rack_awareness.rack_id\n\n\nRack-Id\n for the rack-awareness replica placement feature\n\n\n\n\n\n\nreplication.recovery.size_of_stacked_objs\n\n\nSize of stacked objects. Objects are stacked to send as a bulked object to remote nodes.\n( Default: 5242880, Unit: \nbyte\n )\n\n\n\n\n\n\nreplication.recovery.stacking_timeout\n\n\nStacking timeout. A bulked object are sent to a remote node after reaching the timeout.\n( Default: 1, Unit: \nsec\n )\n\n\n\n\n\n\nMulti Data Center Replication / Basic\n\n\n\n\n\n\n\n\nmdc_replication.size_of_stacked_objs\n\n\nSize of stacked objects. Objects are stacked to send as a bulked object to a remote cluster.\n( Default: 33554432, Unit: \nbyte\n )\n\n\n\n\n\n\nmdc_replication.stacking_timeout\n\n\nStacking timeout. A bulked object are sent to a remote cluster after reaching the timeout.\n( Default: 30, Unit: \nsec\n )\n\n\n\n\n\n\nmdc_replication.req_timeout\n\n\nRequest timeout between clusters\n( Default: 30000, Unit: \nmsec\n )\n\n\n\n\n\n\nLog\n\n\n\n\n\n\n\n\nlog.log_level\n\n\nLog level:\n0:debug\n1:info\n2:warn\n3:error\n( Default: 1 )\n\n\n\n\n\n\nlog.is_enable_access_log\n\n\nEnables or disables the access-log feature\n( Default: false )\n\n\n\n\n\n\nlog.access_log_level\n\n\nAccess log's level:\n0: only regular case\n1: includes error cases\n( Default: 0 )\n\n\n\n\n\n\nlog.erlang\n\n\nDestination of log file(s) of Erlang's log\n( Default: ./log/erlang )\n\n\n\n\n\n\nlog.app\n\n\nDestination of log file(s) of LeoStorage\n( Default: ./log/app )\n\n\n\n\n\n\nlog.member_dir\n\n\nDestination of log file(s) of members of storage-cluster\n( Default: ./log/ring )\n\n\n\n\n\n\nlog.ring_dir\n\n\nDestination of log file(s) of RING\n( Default: ./log/ring )\n\n\n\n\n\n\nlog.is_enable_diagnosis_log\n\n\nDestination of data-diagnosis log(s)\n( Default: true )\n\n\n\n\n\n\nOther Directories Settings\n\n\n\n\n\n\n\n\nqueue_dir\n\n\nDirectory of queue for monitoring \"RING\"\n( Default: ./work/queue )\n\n\n\n\n\n\nsnmp_agent\n\n\nDirectory of SNMP agent configuration\n( Default: ./snmp/snmpa_storage_0/LEO-STORAGE )\n\n\n\n\n\n\n\n\nErlang VM's Related Configurations\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nnodename\n\n\nThe format of the node name is \nNAME\n@\nIP-ADDRESS\n, which must be unique always in a LeoFS system\n( Default: storage_0@127.0.0.1 )\n\n\n\n\n\n\ndistributed_cookie\n\n\nSets the magic cookie of the node to \nCookie\n. \n- See also: \nDistributed Erlang\n( Default: 401321b4 )\n\n\n\n\n\n\nerlang.kernel_poll\n\n\nKernel poll reduces LeoFS' CPU usage when it has hundreds (or more) network connections.\n( Default: true )\n\n\n\n\n\n\nerlang.asyc_threads\n\n\nThe total number of Erlang aynch threads\n( Default: 32 )\n\n\n\n\n\n\nerlang.max_ports\n\n\nThe max_ports sets the default value of maximum number of ports.\n- See also: \nErlang erlang:open_port/2\n( Default: 64000 )\n\n\n\n\n\n\nerlang.crash_dump\n\n\nThe output destination of an Erlang crash dump\n( Default: ./log/erl_crash.dump )\n\n\n\n\n\n\nerlang.max_ets_tables\n\n\nThe maxinum number of \nErlagn ETS\n tables\n( Default: 256000 )\n\n\n\n\n\n\nerlang.smp\n\n\n-smp\n enable and \n-smp\n start the Erlang runtime system with \nSMP\n support enabled.\n( Default: enable )\n\n\n\n\n\n\nerlang.schedulers.compaction_of_load\n\n\nEnables or disables scheduler compaction of load. If it's enabled, the Erlang VM will attempt to fully load as many scheduler threads as mush as possible.\n( Default: true )\n\n\n\n\n\n\nerlang.schedulers.utilization_balancing\n\n\nEnables or disables scheduler utilization balancing of load. By default scheduler utilization balancing is disabled and instead scheduler compaction of load is enabled, which strives for a load distribution that causes as many scheduler threads as possible to be fully loaded (that is, not run out of work).\n( Default: false )\n\n\n\n\n\n\nerlang.distribution_buffer_size\n\n\nSender-side network distribution buffer size \n(unit: KB)\n( Default: 32768 )\n\n\n\n\n\n\nerlang.fullsweep_after\n\n\nOption fullsweep_after makes it possible to specify the maximum number of generational collections before forcing a fullsweep, even if there is room on the old heap. Setting the number to zero disables the general collection algorithm, that is, all live data is copied at every garbage collection.\n( Default: 0 )\n\n\n\n\n\n\nerlang.secio\n\n\nEnables or disables eager check I/O scheduling. The flag effects when schedulers will check for I/O operations possible to execute, and when such I/O operations will execute.\n( Default: true )\n\n\n\n\n\n\nprocess_limit\n\n\nThe maxinum number of Erlang processes. Sets the maximum number of simultaneously existing processes for this system if a Number is passed as value. Valid range for Number is [1024-134217727]\n( Default: 1048576 )\n\n\n\n\n\n\n\n\nNotes and Tips of the Configuration\n\n\nobj_containers.path, obj_containers.num_of_containers\n\n\nYou can configure plural object containers with comma separated value of \nobj_containers.path\n and \nobj_containers.num_of_containers\n.\n\n\n1\n2\nobj_containers.path\n \n=\n \n[/var/leofs/avs/1, /var/leofs/avs/2]\n\n\nobj_containers.num_of_containers\n \n=\n \n[32, 64]\n\n\n\n\n\n\n\nobject_storage.is_strict_check\n\n\nWithout setting \nobject_storage.is_strict_check\n to true, there is a little possibility your data could be broken without any caution even if a LeoFS system is running on a filesystem like ZFS\n1\n that protect both the metadata and the data blocks through the checksum when bugs of any unexpected or unknown software got AVS files broken.\n\n\nConfiguration related to MQ\n\n\nLeoStorage's MQ mechanism depends on the watchdog mechanism to reduce costs of a message consumption. The MQ dynamically updates \na number of batch processes\n and \nan interval of a message consumption\n.\n\n\nFigure: Number-of-batch-processes and interval:\n\n\n\n\nAs of Figure: Relationship of Watchdog and MQ, the watchdog can automatically adjust a value of \na number of batch processes\n between \nmq.num_of_batch_process_min\n and \nmq.num_of_batch_process_max\n, which is increased or decreased with \nmq.num_of_batch_process_step\n.\n\n\nOn the other hands, a value of an interval is adjusted between \nmq.interval_between_batch_procs_min\n and \nmq.interval_between_batch_procs_max\n, which is increased or decreased with \nmq.interval_between_batch_procs_step\n.\n\n\nWhen the each value reached the min value, the MQ changes the status to \nsuspending\n, after that the node\u2019s processing costs is changed to low, the MQ updates the status to \nrunning\n, again.\n\n\n\n\nConfiguration related to the auto-compaction\n\n\nLeoStorage's auto-compaction mechanism also depends on the watchdog mechanism to reduce costs of processing. The Auto-compaction can dynamically update \na number of batch processes\n and \nan interval of a processing of seeking an object\n. The basic design of the relationship with the watchdog is similar to the MQ.\n\n\nFigure: Number-of-batch-processes and interval\n\n\n\n\nAs of \nFigure: Relationship of the watchdog and the auto-compaction\n, the watchdog automatically adjusts the value of \na number of batch processes\n between \ncompaction.batch_procs_min\n and \ncompaction.batch_procs_max\n, which is increased or decreased with \ncompaction.batch_procs_step\n.\n\n\nOn the other hand, the value of an interval is adjusted between \ncompaction.waiting_time_min\n and \ncompaction.waiting_time_max\n, which is increased or decreased with \ncompaction.waiting_time_step\n.\n\n\nWhen the each value reached the min value, the auto-compaction changes the status to \nsuspending\n, after that the node\u2019s processing costs is changed to low, the auto-compaction updates the status to \nrunning\n, again.\n\n\nFigure: Relationship of the watchdog and the auto-compaction\n\n\n\n\nRelated Links\n\n\n\n\nConcept and Architecture / LeoStorage's Architecture\n\n\nFor Administrators / System Operations / Cluster Operations\n\n\nFor Administrators / System Operations / Data Operations\n\n\nFor Administrators / Settings / Environment Configuration\n\n\n\n\n\n\n\n\n\n\n\n\nZFS", 
            "title": "LeoStroage Settings"
        }, 
        {
            "location": "/admin/settings/leo_storage/#leostroage-settings", 
            "text": "", 
            "title": "LeoStroage Settings"
        }, 
        {
            "location": "/admin/settings/leo_storage/#prior-knowledge", 
            "text": "Note: Configuration  LeoStroage's features depend on its configuration. If once a LeoFS system is launched, you cannot modify the following LeoStorage's configurations because the algorithm of the data operation strictly adheres to the settings.", 
            "title": "Prior Knowledge"
        }, 
        {
            "location": "/admin/settings/leo_storage/#irrevocable-and-attention-required-items", 
            "text": "Item  Irrevocable?  Description      LeoStorage Basic      obj_containers.path  Modifiable with condition  Able to change the directory of the container(s) but not able to add or remove the directory(s). You need to move the data files which are  obj_containers.path /avs/object  and  obj_containers.path /avs/metadata , which adhere to this configuration.    obj_containers.num_of_containers  Yes  Not able to change the configuration because LeoStorage cannot retrieve objects or metadatas.    obj_containers.metadata_storage  Yes  As above    num_of_vnodes`  Yes  As above    MQ      mq.backend_db  Modifiable with condition  Lose all the MQ's data after changing    mq.num_of_mq_procs  Modifiable with condition  As above    Replication and Recovery object(s)      replication.rack_awareness.rack_id  Yes  Not able to change the configuration because LeoFS cannot retrieve objects or metadatas.    Other Directories Settings      queue_dir  Modifiable with condition  Able to change the MQ's directory but you need to move the MQ's data, which adhere to this configuration.", 
            "title": "Irrevocable and Attention Required Items:"
        }, 
        {
            "location": "/admin/settings/leo_storage/#other-configurations", 
            "text": "If you want to modify settings like where to place  leo_storage.conf , what user is starting a LeoStorage process and so on, refer  For Administrators / Settings / Environment Configuration  for more information.", 
            "title": "Other Configurations"
        }, 
        {
            "location": "/admin/settings/leo_storage/#configuration", 
            "text": "", 
            "title": "Configuration"
        }, 
        {
            "location": "/admin/settings/leo_storage/#leostorage-configurations", 
            "text": "Item  Description      LeoManager Nodes     managers  Name of LeoManager nodes. This configuration is necessary for communicating with  LeoManager s master  and  LeoManager s slave . ( Default: [manager_0@127.0.0.1, manager_1@127.0.0.1] )    LeoStorage Basic     obj_containers.path  Directories of object-containers ( Default: [./avs] )    obj_containers.num_of_containers  A number of object-containers of each directory ( Default: [8] )    obj_containers.sync_mode  Mode of the data synchronization. There're three modes: none : Not synchronization every time  (default) periodic : Periodic synchronization which depends on  obj_containers.sync_interval_in_ms writethrough : Ensures that any buffers kept by the OS are written to disk every time ( Default: none )    obj_containers.sync_interval_in_ms  Interval in ms of the data synchronization ( Default: 1000, Unit:  msec  )    obj_containers.metadata_storage  The metadata storage feature is pluggable which depends on  bitcask  and  leveldb . ( Default: leveldb )    num_of_vnodes  The total number of virtual-nodes of a LeoStroage node for generating the distributed hashtable (RING) ( Default: 168 )    object_storage.is_strict_check  Enable strict check between checksum of a metadata and checksum of an object. ( Default: false )    object_storage.threshold_of_slow_processing  Threshold of slow processing ( Default: 1000, Unit:  msec  )    seeking_timeout_per_metadata  Timeout of seeking metadatas per a metadata ( Default: 10, Unit:  msec  )    max_num_of_procs  Maximum number of processes for both write and read operation ( Default: 3000 )    num_of_obj_storage_read_procs  Total number of obj-storage-read processes per object-container, AVS Range: [1..100] ( Default: 3 )    Watchdog     watchdog.common.loosen_control_at_safe_count  When reach a number of safe  (clear watchdog) , a watchdog loosen the control ( Default: 1 )    Watchdog / REX     watchdog.rex.is_enabled  Enables or disables the rex-watchdog which monitors the memory usage of  Erlang's RPC component . ( Default: true )    watchdog.rex.interval  An interval of executing the watchdog processing ( Default: 10, Unit:  sec  )    watchdog.rex.threshold_mem_capacity  Threshold of memory capacity of binary for Erlang rex ( Default: 33554432, Unit:  byte  )    Watchdog / CPU     watchdog.cpu.is_enabled  Enables or disables the CPU-watchdog which monitors both  CPU load average  and  CPU utilization ( Default: false )    watchdog.cpu.raised_error_times  Times of raising error to a client ( Default: 5 )    watchdog.cpu.interval  An interval of executing the watchdog processing ( Default: 10, Unit:  sec  )    watchdog.cpu.threshold_cpu_load_avg  Threshold of CPU load average ( Default: 5.0 )    watchdog.cpu.threshold_cpu_util  Threshold of CPU utilization ( Default: 100 )    Watchdog / DISK     watchdog.disk.is_enabled  Enables or disables the  ( Default: false )    watchdog.disk.raised_error_times  Times of raising error to a client ( Default: 5 )    watchdog.disk.interval  An interval of executing the watchdog processing ( Default: 10, Unit:  sec  )    watchdog.disk.threshold_disk_use  Threshold of Disk use(%) of a target disk's capacity ( Default: 85, Unit:  percent  )    watchdog.disk.threshold_disk_util  Threshold of Disk utilization ( Default: 90, Unit:  percent  )    watchdog.disk.threshold_disk_rkb  Threshold of disk read KB/sec ( Default: 98304, Unit:  KB  )    watchdog.disk.threshold_disk_wkb  Threshold of disk write KB/sec ( Default: 98304, Unit:  KB  )    watchdog.disk.target_devices  Target devices for checking disk utilization ( Default: [] )    Watchdog / CLUSTER     watchdog.cluster.is_enabled  Enables or disables the  ( Default: false )    watchdog.cluster.interval  An interval of executing the watchdog processing ( Default: 10 )    Watchdog / ERRORS     watchdog.error.is_enabled  Enables or disables the  ( Default: false )    watchdog.error.interval  An interval of executing the watchdog processing ( Default: 60 )    watchdog.error.threshold_count  Total counts of raising error to a client ( Default: 100 )    Data Compaction     Data Compaction / Basic     compaction.limit_num_of_compaction_procs  Limit of a number of procs to execute data-compaction in parallel ( Default: 4 )    compaction.skip_prefetch_size  Perfetch size when skipping garbage ( Default: 512 )    compaction.waiting_time_regular  Regular value of compaction-proc waiting time/batch-proc ( Default: 500, Unit:  msec  )    compaction.waiting_time_max  Maximum value of compaction-proc waiting time/batch-proc ( Default: 3000, Unit:  msec  )    compaction.batch_procs_regular  Total number of regular compaction batch processes ( Default: 1000 )    compaction.batch_procs_max  Maximum number of compaction batch processes ( Default: 1500 )    Data Compaction / Automated Data Compaction     autonomic_op.compaction.is_enabled  Enables or disables the auto-compaction ( Default: false )    autonomic_op.compaction.parallel_procs  Total number of parallel processes ( Default: 1 )    autonomic_op.compaction.interval  An interval time of between auto-comcations ( Default: 3600, Unit:  sec  )    autonomic_op.compaction.warn_active_size_ratio  Warning ratio of active size ( Default: 70, Unit:  percent  )    autonomic_op.compaction.threshold_active_size_ratio  Threshold ratio of active size. LeoStroage start data-comaction after reaching it ( Default: 60,  percent  )    MQ     mq.backend_db  The MQ storage feature is pluggable which depends on  bitcask  and  leveldb . ( Default: leveldb )    mq.num_of_mq_procs  A number of mq-server's processes ( Default: 8 )    mq.num_of_batch_process_max  Maximum number of bach processes of message ( Default: 3000 )    mq.num_of_batch_process_regular  Regular value of bach processes of message ( Default: 1600 )    mq.interval_between_batch_procs_max  Maximum value of interval between batch-procs ( Default: 3000, Unit:  msec  )    mq.interval_between_batch_procs_regular  Regular value of interval between batch-procs ( Default: 500, Unit:  msec  )    Backend DB / eleveldb     backend_db.eleveldb.write_buf_size  Write Buffer Size. Larger values increase performance, especially during bulk loads. Up to two write buffers may be held in memory at the same time, so you may wish to adjust this parameter to control memory usage.Also, a larger write buffer will result in a longer recovery time the next time the database is opened. ( Default: 62914560 )    backend_db.eleveldb.max_open_files  Max Open Files. Number of open files that can be used by the DB. You may need to increase this if your database has a large working set  (budget one open file per 2MB of working set) . ( Default: 1000 )    backend_db.eleveldb.sst_block_size  The size of a data block is controlled by the SST block size. The size represents a threshold, not a fixed count. Whenever a newly created block reaches this uncompressed size, leveldb considers it full and writes the block with its metadata to disk. The number of keys contained in the block depends upon the size of the values and keys. ( Default: 4096 )    Replication and Recovery object(s)     replication.rack_awareness.rack_id  Rack-Id  for the rack-awareness replica placement feature    replication.recovery.size_of_stacked_objs  Size of stacked objects. Objects are stacked to send as a bulked object to remote nodes. ( Default: 5242880, Unit:  byte  )    replication.recovery.stacking_timeout  Stacking timeout. A bulked object are sent to a remote node after reaching the timeout. ( Default: 1, Unit:  sec  )    Multi Data Center Replication / Basic     mdc_replication.size_of_stacked_objs  Size of stacked objects. Objects are stacked to send as a bulked object to a remote cluster. ( Default: 33554432, Unit:  byte  )    mdc_replication.stacking_timeout  Stacking timeout. A bulked object are sent to a remote cluster after reaching the timeout. ( Default: 30, Unit:  sec  )    mdc_replication.req_timeout  Request timeout between clusters ( Default: 30000, Unit:  msec  )    Log     log.log_level  Log level: 0:debug 1:info 2:warn 3:error ( Default: 1 )    log.is_enable_access_log  Enables or disables the access-log feature ( Default: false )    log.access_log_level  Access log's level: 0: only regular case 1: includes error cases ( Default: 0 )    log.erlang  Destination of log file(s) of Erlang's log ( Default: ./log/erlang )    log.app  Destination of log file(s) of LeoStorage ( Default: ./log/app )    log.member_dir  Destination of log file(s) of members of storage-cluster ( Default: ./log/ring )    log.ring_dir  Destination of log file(s) of RING ( Default: ./log/ring )    log.is_enable_diagnosis_log  Destination of data-diagnosis log(s) ( Default: true )    Other Directories Settings     queue_dir  Directory of queue for monitoring \"RING\" ( Default: ./work/queue )    snmp_agent  Directory of SNMP agent configuration ( Default: ./snmp/snmpa_storage_0/LEO-STORAGE )", 
            "title": "LeoStorage Configurations"
        }, 
        {
            "location": "/admin/settings/leo_storage/#erlang-vms-related-configurations", 
            "text": "Item  Description      nodename  The format of the node name is  NAME @ IP-ADDRESS , which must be unique always in a LeoFS system ( Default: storage_0@127.0.0.1 )    distributed_cookie  Sets the magic cookie of the node to  Cookie .  - See also:  Distributed Erlang ( Default: 401321b4 )    erlang.kernel_poll  Kernel poll reduces LeoFS' CPU usage when it has hundreds (or more) network connections. ( Default: true )    erlang.asyc_threads  The total number of Erlang aynch threads ( Default: 32 )    erlang.max_ports  The max_ports sets the default value of maximum number of ports. - See also:  Erlang erlang:open_port/2 ( Default: 64000 )    erlang.crash_dump  The output destination of an Erlang crash dump ( Default: ./log/erl_crash.dump )    erlang.max_ets_tables  The maxinum number of  Erlagn ETS  tables ( Default: 256000 )    erlang.smp  -smp  enable and  -smp  start the Erlang runtime system with  SMP  support enabled. ( Default: enable )    erlang.schedulers.compaction_of_load  Enables or disables scheduler compaction of load. If it's enabled, the Erlang VM will attempt to fully load as many scheduler threads as mush as possible. ( Default: true )    erlang.schedulers.utilization_balancing  Enables or disables scheduler utilization balancing of load. By default scheduler utilization balancing is disabled and instead scheduler compaction of load is enabled, which strives for a load distribution that causes as many scheduler threads as possible to be fully loaded (that is, not run out of work). ( Default: false )    erlang.distribution_buffer_size  Sender-side network distribution buffer size  (unit: KB) ( Default: 32768 )    erlang.fullsweep_after  Option fullsweep_after makes it possible to specify the maximum number of generational collections before forcing a fullsweep, even if there is room on the old heap. Setting the number to zero disables the general collection algorithm, that is, all live data is copied at every garbage collection. ( Default: 0 )    erlang.secio  Enables or disables eager check I/O scheduling. The flag effects when schedulers will check for I/O operations possible to execute, and when such I/O operations will execute. ( Default: true )    process_limit  The maxinum number of Erlang processes. Sets the maximum number of simultaneously existing processes for this system if a Number is passed as value. Valid range for Number is [1024-134217727] ( Default: 1048576 )", 
            "title": "Erlang VM's Related Configurations"
        }, 
        {
            "location": "/admin/settings/leo_storage/#notes-and-tips-of-the-configuration", 
            "text": "", 
            "title": "Notes and Tips of the Configuration"
        }, 
        {
            "location": "/admin/settings/leo_storage/#obj95containerspath-obj95containersnum95of95containers", 
            "text": "You can configure plural object containers with comma separated value of  obj_containers.path  and  obj_containers.num_of_containers .  1\n2 obj_containers.path   =   [/var/leofs/avs/1, /var/leofs/avs/2]  obj_containers.num_of_containers   =   [32, 64]", 
            "title": "obj_containers.path, obj_containers.num_of_containers"
        }, 
        {
            "location": "/admin/settings/leo_storage/#object95storageis95strict95check", 
            "text": "Without setting  object_storage.is_strict_check  to true, there is a little possibility your data could be broken without any caution even if a LeoFS system is running on a filesystem like ZFS 1  that protect both the metadata and the data blocks through the checksum when bugs of any unexpected or unknown software got AVS files broken.", 
            "title": "object_storage.is_strict_check"
        }, 
        {
            "location": "/admin/settings/leo_storage/#configuration-related-to-mq", 
            "text": "LeoStorage's MQ mechanism depends on the watchdog mechanism to reduce costs of a message consumption. The MQ dynamically updates  a number of batch processes  and  an interval of a message consumption .  Figure: Number-of-batch-processes and interval:   As of Figure: Relationship of Watchdog and MQ, the watchdog can automatically adjust a value of  a number of batch processes  between  mq.num_of_batch_process_min  and  mq.num_of_batch_process_max , which is increased or decreased with  mq.num_of_batch_process_step .  On the other hands, a value of an interval is adjusted between  mq.interval_between_batch_procs_min  and  mq.interval_between_batch_procs_max , which is increased or decreased with  mq.interval_between_batch_procs_step .  When the each value reached the min value, the MQ changes the status to  suspending , after that the node\u2019s processing costs is changed to low, the MQ updates the status to  running , again.", 
            "title": "Configuration related to MQ"
        }, 
        {
            "location": "/admin/settings/leo_storage/#configuration-related-to-the-auto-compaction", 
            "text": "LeoStorage's auto-compaction mechanism also depends on the watchdog mechanism to reduce costs of processing. The Auto-compaction can dynamically update  a number of batch processes  and  an interval of a processing of seeking an object . The basic design of the relationship with the watchdog is similar to the MQ.  Figure: Number-of-batch-processes and interval   As of  Figure: Relationship of the watchdog and the auto-compaction , the watchdog automatically adjusts the value of  a number of batch processes  between  compaction.batch_procs_min  and  compaction.batch_procs_max , which is increased or decreased with  compaction.batch_procs_step .  On the other hand, the value of an interval is adjusted between  compaction.waiting_time_min  and  compaction.waiting_time_max , which is increased or decreased with  compaction.waiting_time_step .  When the each value reached the min value, the auto-compaction changes the status to  suspending , after that the node\u2019s processing costs is changed to low, the auto-compaction updates the status to  running , again.  Figure: Relationship of the watchdog and the auto-compaction", 
            "title": "Configuration related to the auto-compaction"
        }, 
        {
            "location": "/admin/settings/leo_storage/#related-links", 
            "text": "Concept and Architecture / LeoStorage's Architecture  For Administrators / System Operations / Cluster Operations  For Administrators / System Operations / Data Operations  For Administrators / Settings / Environment Configuration       ZFS", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/settings/leo_gateway/", 
            "text": "LeoGateway Settings\n\n\nPrior Knowledge\n\n\nLeoGateway is a multi-protocols storage proxy, which supports REST-API over HTTP, Amazon S3-API\n1\n and NFS v3\n2\n. LeoGateway provides the object cache feature to handle requests efficiently and to keep the high performance of your storage system.\n\n\nOther Configurations\n\n\nIf you want to customize settings like where to place \nleo_gateway.conf\n, what user is starting a LeoGateway process and so on, refer \nFor Administrators / Settings / Environment Configuration\n for more information.\n\n\nConfiguration\n\n\nLeoGateway Configurations\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nLeoManager Nodes\n\n\n\n\n\n\n\n\nmanagers\n\n\nName of LeoManager nodes. This configuration is necessary for communicating with \nLeoManager\ns master\n and \nLeoManager\ns slave\n.\n( Default: [manager_0@127.0.0.1, manager_1@127.0.0.1] )\n\n\n\n\n\n\nLeoGateway Basic\n\n\n\n\n\n\n\n\nprotocol\n\n\nGateway Protocol - [s3/rest/embed/nfs] \n( Default: s3 )\n\n\n\n\n\n\nHTTP Related (S3/REST)\n\n\n\n\n\n\n\n\nhttp.port\n\n\nPort number the Gateway uses for HTTP connections \n( Default: 8080 )\n\n\n\n\n\n\nhttp.num_of_acceptors\n\n\nNumbers of processes listening for connections \n( Default: 128 )\n\n\n\n\n\n\nhttp.max_keepalive\n\n\nMaximum number of requests allowed in a single keep-alive session \n( Default: 4096 )\n\n\n\n\n\n\nhttp.layer_of_dirs\n\n\nMaximum number of virtual directory levels \n( Default: 12 )\n\n\n\n\n\n\nhttp.ssl_port\n\n\nPort number the Gateway uses for HTTPS connections \n( Default: 8443 )\n\n\n\n\n\n\nhttp.ssl_certfile\n\n\nSSL Certificate file \n( Default: ./etc/server_cert.pem )\n\n\n\n\n\n\nhttp.ssl_keyfile\n\n\nSSL key file \n( Default: ./etc/server_key.pem )\n\n\n\n\n\n\nhttp.headers_config_file\n\n\nHTTP custom header configuration file \n( Default: ./etc/http_custom_header.conf )\n\n\n\n\n\n\nhttp.timeout_for_header\n\n\nHTTP timeout for reading header\n( Default: 5000, Unit: \nmsec\n)\n\n\n\n\n\n\nhttp.timeout_for_body\n\n\nHTTP timeout for reading body\n( Default: 15000, Unit: \nmsec\n)\n\n\n\n\n\n\nBucket Related\n\n\n\n\n\n\n\n\nbucket_prop_sync_interval\n\n\nSynchronization Interval of Bucket Properties\n( Default: 300, Unit: \nsec\n )\n\n\n\n\n\n\nNFS-related configurations\n\n\n\n\n\n\n\n\nnfs.mountd.port\n\n\nMountd\u2019s port number \n( Default: 22050 )\n\n\n\n\n\n\nnfs.mountd.acceptors\n\n\nMountd\u2019s the number of acceptors \n( Default: 128 )\n\n\n\n\n\n\nnfs.nfsd.port\n\n\nNFSd\u2019s port number \n( Default: 2049 )\n\n\n\n\n\n\nnfs.nfsd.acceptors\n\n\nNFSd\u2019s the number of acceptors \n( Default: 128 )\n\n\n\n\n\n\nLarge object processing configuration\n\n\n\n\n\n\n\n\nlarge_object.max_chunked_objs\n\n\nMaximum number of chunked objects \n( Default: 1000 )\n\n\n\n\n\n\nlarge_object.chunked_obj_len\n\n\nLength of a chunked object. This value must be \n= \nlarge_object.reading_chunked_obj_len\n \n( Default: 5242880, Unit: \nbyte\n )\n\n\n\n\n\n\nlarge_object.threshold_of_chunk_len\n\n\nThreshold when object is chunked \n( Default: 5767168, Unit: \nbyte\n )\n\n\n\n\n\n\nlarge_object.reading_chunked_obj_len\n\n\nRead length of a chunked object. This value must be \n= \nlarge_object.chunked_obj_len\n \n( Default: 5242880, Unit: \nbyte\n )\n\n\n\n\n\n\nCache configuration\n\n\n\n\n\n\n\n\ncache.http_cache\n\n\nEnable HTTP-Cache mode, working like Varnish/Squid. Otherwise as Object Cache\n( Default: false )\n\n\n\n\n\n\ncache.cache_workers\n\n\nNumber of cache workers \n( Default: 16 )\n\n\n\n\n\n\ncache.cache_ram_capacity\n\n\nMemory Cache Capacity, divide across workers. This has to satisfy \n(8 * 1024 * 1024) * cache.cache_workers \n= cache.cache_ram_capacity\n \n( Default: 268435456, Unit: \nbyte\n )\n\n\n\n\n\n\ncache.cache_disc_capacity\n\n\nDisk Cache Capacity, divide across workers. This has to satisfy \n(8 * 1024 * 1024) * cache.cache_workers \n= cache.cache_disc_capacity\n \n( Default: 524288000, Unit: \nbyte\n )\n\n\n\n\n\n\ncache.cache_disc_threshold_len\n\n\nThreshold when object is stored in disk cache \n( Default: 1048576, Unit: \nbyte\n )\n\n\n\n\n\n\ncache.cache_disc_dir_data\n\n\nDirectory for disk cache data \n( Default: ./cache/data )\n\n\n\n\n\n\ncache.cache_disc_dir_journal\n\n\nDirectory for disk cache journal \n( Default: ./cache/journal )\n\n\n\n\n\n\nHTTP-Cache related\n\n\n\n\n\n\n\n\ncache.cache_expire\n\n\nCache expiry time \n( Default: 300, Unit: \nsec\n)\n\n\n\n\n\n\ncache.cache_max_content_len\n\n\nMaximum length of cached object \n( Default: 1048576, Unit: \nbyte\n )\n\n\n\n\n\n\ncache.cachable_content_type\n\n\nObject types to be cached\n\n\n\n\n\n\ncache.cachable_path_pattern\n\n\nPath pattern(s) to be cached (regular expression)\n\n\n\n\n\n\nWatchdog / REX\n\n\n\n\n\n\n\n\nwatchdog.rex.is_enabled\n\n\nEnables or disables the rex-watchdog which monitors the memory usage of \nErlangs RPC component\n.\n( Default: true )\n\n\n\n\n\n\nwatchdog.rex.interval\n\n\nAn interval of executing the watchdog processing\n( Default: 10, Unit: \nsec\n )\n\n\n\n\n\n\nWatchdog / CPU\n\n\n\n\n\n\n\n\nwatchdog.cpu.is_enabled\n\n\nEnables or disables the CPU-watchdog which monitors both \nCPU load average\n and \nCPU utilization\n( Default: false )\n\n\n\n\n\n\nwatchdog.cpu.raised_error_times\n\n\nTimes of raising error to a client\n( Default: 5 )\n\n\n\n\n\n\nwatchdog.cpu.interval\n\n\nAn interval of executing the watchdog processing\n( Default: 10, Unit: \nsec\n )\n\n\n\n\n\n\nwatchdog.cpu.threshold_cpu_load_avg\n\n\nThreshold of CPU load average\n( Default: 5.0 )\n\n\n\n\n\n\nwatchdog.cpu.threshold_cpu_util\n\n\nThreshold of CPU utilization\n( Default: 100 )\n\n\n\n\n\n\nWatchdog / IO (Erlang VM Internal Traffic)\n\n\n\n\n\n\n\n\nwatchdog.io.is_enabled\n\n\nEnables or disables the IO-watchdog which monitors the \nErlang VM Internal Traffic\n \n( Default: false )\n\n\n\n\n\n\nwatchdog.io.interval\n\n\nWatchdog interval \n( Default: 1, Unit: \nsec\n )\n\n\n\n\n\n\nwatchdog.io.threshold_input_per_sec\n\n\nThreshold input per second \n( Default: 134217728, Unit: \nbyte\n )\n\n\n\n\n\n\nwatchdog.io.threshold_output_per_sec\n\n\nThreshold output per second \n( Default: 134217728, Unit: \nbyte\n )\n\n\n\n\n\n\nTimeout\n\n\n\n\n\n\n\n\ntimeout.level_1\n\n\nTimeout when put object to LeoStorage \n(~65536 bytes)\n \n ( Default: 5000, Unit: \nmsec\n )\n\n\n\n\n\n\ntimeout.level_2\n\n\nTimeout when put object to LeoStorage \n(~131071 bytes)\n \n ( Default: 7000, Unit: \nmsec\n )\n\n\n\n\n\n\ntimeout.level_3\n\n\nTimeout when put object to LeoStorage \n(~524287 bytes)\n \n ( Default: 10000, Unit: \nmsec\n )\n\n\n\n\n\n\ntimeout.level_4\n\n\nTimeout when put object to LeoStorage \n(~1048576 bytes)\n \n ( Default: 20000, Unit: \nmsec\n )\n\n\n\n\n\n\ntimeout.level_5\n\n\nTimeout when put object to LeoStorage \n(1048576~ bytes)\n \n ( Default: 30000, Unit: \nmsec\n )\n\n\n\n\n\n\ntimeout.get\n\n\nTimeout when get object from LeoStorage\n ( Default: 30000, Unit: \nmsec\n )\n\n\n\n\n\n\ntimeout.ls\n\n\nTimeout when list object from LeoStorage\n ( Default: 30000, Unit: \nmsec\n )\n\n\n\n\n\n\nLog\n\n\n\n\n\n\n\n\nlog.log_level\n\n\nLog level:\n0:debug\n1:info\n2:warn\n3:error\n( Default: 1 )\n\n\n\n\n\n\nlog.is_enable_access_log\n\n\nEnables or disables the access-log feature\n( Default: false )\n\n\n\n\n\n\nlog.erlang\n\n\nDestination of log file(s) of Erlang's log\n( Default: ./log/erlang )\n\n\n\n\n\n\nlog.app\n\n\nDestination of log file(s) of LeoStorage\n( Default: ./log/app )\n\n\n\n\n\n\nlog.member_dir\n\n\nDestination of log file(s) of members of storage-cluster\n( Default: ./log/ring )\n\n\n\n\n\n\nlog.ring_dir\n\n\nDestination of log file(s) of RING\n( Default: ./log/ring )\n\n\n\n\n\n\nOther Directories Settings\n\n\n\n\n\n\n\n\nqueue_dir\n\n\nDirectory of queue for monitoring \"RING\"\n( Default: ./work/queue )\n\n\n\n\n\n\nsnmp_agent\n\n\nDirectory of SNMP agent configuration\n( Default: ./snmp/snmpa_gateway_0/LEO-GATEWAY )\n\n\n\n\n\n\n\n\nErlang VM's Related Configurations\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nnodename\n\n\nThe format of the node name is \nNAME\n@\nIP-ADDRESS\n, which must be unique always in a LeoFS system\n( Default: storage_0@127.0.0.1 )\n\n\n\n\n\n\ndistributed_cookie\n\n\nSets the magic cookie of the node to \nCookie\n. \n- See also: \nDistributed Erlang\n( Default: 401321b4 )\n\n\n\n\n\n\nerlang.kernel_poll\n\n\nKernel poll reduces LeoFS' CPU usage when it has hundreds (or more) network connections.\n( Default: true )\n\n\n\n\n\n\nerlang.asyc_threads\n\n\nThe total number of Erlang aynch threads\n( Default: 32 )\n\n\n\n\n\n\nerlang.max_ports\n\n\nThe max_ports sets the default value of maximum number of ports.\n- See also: \nErlang erlang:open_port/2\n( Default: 64000 )\n\n\n\n\n\n\nerlang.crash_dump\n\n\nThe output destination of an Erlang crash dump\n( Default: ./log/erl_crash.dump )\n\n\n\n\n\n\nerlang.max_ets_tables\n\n\nThe maxinum number of \nErlagn ETS\n tables\n( Default: 256000 )\n\n\n\n\n\n\nerlang.smp\n\n\n-smp\n enable and \n-smp\n start the Erlang runtime system with \nSMP\n support enabled.\n( Default: enable )\n\n\n\n\n\n\nerlang.schedulers.compaction_of_load\n\n\nEnables or disables scheduler compaction of load. If it's enabled, the Erlang VM will attempt to fully load as many scheduler threads as mush as possible.\n( Default: true )\n\n\n\n\n\n\nerlang.schedulers.utilization_balancing\n\n\nEnables or disables scheduler utilization balancing of load. By default scheduler utilization balancing is disabled and instead scheduler compaction of load is enabled, which strives for a load distribution that causes as many scheduler threads as possible to be fully loaded (that is, not run out of work).\n( Default: false )\n\n\n\n\n\n\nerlang.distribution_buffer_size\n\n\nSender-side network distribution buffer size \n(unit: KB)\n( Default: 32768 )\n\n\n\n\n\n\nerlang.fullsweep_after\n\n\nOption fullsweep_after makes it possible to specify the maximum number of generational collections before forcing a fullsweep, even if there is room on the old heap. Setting the number to zero disables the general collection algorithm, that is, all live data is copied at every garbage collection.\n( Default: 0 )\n\n\n\n\n\n\nerlang.secio\n\n\nEnables or disables eager check I/O scheduling. The flag effects when schedulers will check for I/O operations possible to execute, and when such I/O operations will execute.\n( Default: true )\n\n\n\n\n\n\nprocess_limit\n\n\nThe maxinum number of Erlang processes. Sets the maximum number of simultaneously existing processes for this system if a Number is passed as value. Valid range for Number is [1024-134217727]\n( Default: 1048576 )\n\n\n\n\n\n\n\n\nNotes and Tips of the Configuration\n\n\nCache Consistency between LeoGateway and LeoStorage\n\n\nLeoGateway's cache feature does not depend on the consistency level of a cluster. There is a possibility of object inconsistency.\n\n\nLeoGateway requests a storage node to compare a cached object's hash value with its stored object's hash value. LeoGateway selects a LeoStorage's node from RING, \na distributed hash table\n by a target object name, then LeoGateway requests a LeoStorage node of the redundant node. If the requested object is inconsistent in the replicas and LeoGateway cached it, a client may get inconsistent objects.\n\n\nIf you need strong consistency on a LeoFS system, you can disable the cache setting.\n\n\n1\n2\ncache.cache_ram_capacity\n \n=\n \n0\n\n\ncache.cache_disc_capacity\n \n=\n \n0\n\n\n\n\n\n\n\nConfiguration related to Disk Cache\n\n\nA total number of directories to store cache files is equal to \ncache.cache_workers\n. A maximum size of a cacheable object per a directory has been determined by \ncache.cache_disc_capacity / cache.cache_workers\n. If the size of a requested object more than the maximum size, LeoGateway avoids storing the object into the disk cache.\n\n\nAnd also, when size of a requested object more than \ncache.cache_max_content_len\n, LeoGateway similarly refuses to store the object into the disk cache.\n\n\n\n\nRelated Links\n\n\n\n\nConcept and Architecture / LeoGateway's Architecture\n\n\nFor Administrators / Interface / S3-API\n\n\nFor Administrators / Interface / REST-API\n\n\nFor Administrators / Interface / NFS v3\n\n\nFor Administrators / System Operations / S3-API related Operations\n\n\nFor Administrators / Settings / Environment Configuration\n\n\n\n\n\n\n\n\n\n\n\n\nAmazon S3 API\n\n\n\n\n\n\nWikipedia: Network File System", 
            "title": "LeoGateway Settings"
        }, 
        {
            "location": "/admin/settings/leo_gateway/#leogateway-settings", 
            "text": "", 
            "title": "LeoGateway Settings"
        }, 
        {
            "location": "/admin/settings/leo_gateway/#prior-knowledge", 
            "text": "LeoGateway is a multi-protocols storage proxy, which supports REST-API over HTTP, Amazon S3-API 1  and NFS v3 2 . LeoGateway provides the object cache feature to handle requests efficiently and to keep the high performance of your storage system.", 
            "title": "Prior Knowledge"
        }, 
        {
            "location": "/admin/settings/leo_gateway/#other-configurations", 
            "text": "If you want to customize settings like where to place  leo_gateway.conf , what user is starting a LeoGateway process and so on, refer  For Administrators / Settings / Environment Configuration  for more information.", 
            "title": "Other Configurations"
        }, 
        {
            "location": "/admin/settings/leo_gateway/#configuration", 
            "text": "", 
            "title": "Configuration"
        }, 
        {
            "location": "/admin/settings/leo_gateway/#leogateway-configurations", 
            "text": "Item  Description      LeoManager Nodes     managers  Name of LeoManager nodes. This configuration is necessary for communicating with  LeoManager s master  and  LeoManager s slave . ( Default: [manager_0@127.0.0.1, manager_1@127.0.0.1] )    LeoGateway Basic     protocol  Gateway Protocol - [s3/rest/embed/nfs]  ( Default: s3 )    HTTP Related (S3/REST)     http.port  Port number the Gateway uses for HTTP connections  ( Default: 8080 )    http.num_of_acceptors  Numbers of processes listening for connections  ( Default: 128 )    http.max_keepalive  Maximum number of requests allowed in a single keep-alive session  ( Default: 4096 )    http.layer_of_dirs  Maximum number of virtual directory levels  ( Default: 12 )    http.ssl_port  Port number the Gateway uses for HTTPS connections  ( Default: 8443 )    http.ssl_certfile  SSL Certificate file  ( Default: ./etc/server_cert.pem )    http.ssl_keyfile  SSL key file  ( Default: ./etc/server_key.pem )    http.headers_config_file  HTTP custom header configuration file  ( Default: ./etc/http_custom_header.conf )    http.timeout_for_header  HTTP timeout for reading header ( Default: 5000, Unit:  msec )    http.timeout_for_body  HTTP timeout for reading body ( Default: 15000, Unit:  msec )    Bucket Related     bucket_prop_sync_interval  Synchronization Interval of Bucket Properties ( Default: 300, Unit:  sec  )    NFS-related configurations     nfs.mountd.port  Mountd\u2019s port number  ( Default: 22050 )    nfs.mountd.acceptors  Mountd\u2019s the number of acceptors  ( Default: 128 )    nfs.nfsd.port  NFSd\u2019s port number  ( Default: 2049 )    nfs.nfsd.acceptors  NFSd\u2019s the number of acceptors  ( Default: 128 )    Large object processing configuration     large_object.max_chunked_objs  Maximum number of chunked objects  ( Default: 1000 )    large_object.chunked_obj_len  Length of a chunked object. This value must be  =  large_object.reading_chunked_obj_len   ( Default: 5242880, Unit:  byte  )    large_object.threshold_of_chunk_len  Threshold when object is chunked  ( Default: 5767168, Unit:  byte  )    large_object.reading_chunked_obj_len  Read length of a chunked object. This value must be  =  large_object.chunked_obj_len   ( Default: 5242880, Unit:  byte  )    Cache configuration     cache.http_cache  Enable HTTP-Cache mode, working like Varnish/Squid. Otherwise as Object Cache ( Default: false )    cache.cache_workers  Number of cache workers  ( Default: 16 )    cache.cache_ram_capacity  Memory Cache Capacity, divide across workers. This has to satisfy  (8 * 1024 * 1024) * cache.cache_workers  = cache.cache_ram_capacity   ( Default: 268435456, Unit:  byte  )    cache.cache_disc_capacity  Disk Cache Capacity, divide across workers. This has to satisfy  (8 * 1024 * 1024) * cache.cache_workers  = cache.cache_disc_capacity   ( Default: 524288000, Unit:  byte  )    cache.cache_disc_threshold_len  Threshold when object is stored in disk cache  ( Default: 1048576, Unit:  byte  )    cache.cache_disc_dir_data  Directory for disk cache data  ( Default: ./cache/data )    cache.cache_disc_dir_journal  Directory for disk cache journal  ( Default: ./cache/journal )    HTTP-Cache related     cache.cache_expire  Cache expiry time  ( Default: 300, Unit:  sec )    cache.cache_max_content_len  Maximum length of cached object  ( Default: 1048576, Unit:  byte  )    cache.cachable_content_type  Object types to be cached    cache.cachable_path_pattern  Path pattern(s) to be cached (regular expression)    Watchdog / REX     watchdog.rex.is_enabled  Enables or disables the rex-watchdog which monitors the memory usage of  Erlangs RPC component . ( Default: true )    watchdog.rex.interval  An interval of executing the watchdog processing ( Default: 10, Unit:  sec  )    Watchdog / CPU     watchdog.cpu.is_enabled  Enables or disables the CPU-watchdog which monitors both  CPU load average  and  CPU utilization ( Default: false )    watchdog.cpu.raised_error_times  Times of raising error to a client ( Default: 5 )    watchdog.cpu.interval  An interval of executing the watchdog processing ( Default: 10, Unit:  sec  )    watchdog.cpu.threshold_cpu_load_avg  Threshold of CPU load average ( Default: 5.0 )    watchdog.cpu.threshold_cpu_util  Threshold of CPU utilization ( Default: 100 )    Watchdog / IO (Erlang VM Internal Traffic)     watchdog.io.is_enabled  Enables or disables the IO-watchdog which monitors the  Erlang VM Internal Traffic   ( Default: false )    watchdog.io.interval  Watchdog interval  ( Default: 1, Unit:  sec  )    watchdog.io.threshold_input_per_sec  Threshold input per second  ( Default: 134217728, Unit:  byte  )    watchdog.io.threshold_output_per_sec  Threshold output per second  ( Default: 134217728, Unit:  byte  )    Timeout     timeout.level_1  Timeout when put object to LeoStorage  (~65536 bytes)    ( Default: 5000, Unit:  msec  )    timeout.level_2  Timeout when put object to LeoStorage  (~131071 bytes)    ( Default: 7000, Unit:  msec  )    timeout.level_3  Timeout when put object to LeoStorage  (~524287 bytes)    ( Default: 10000, Unit:  msec  )    timeout.level_4  Timeout when put object to LeoStorage  (~1048576 bytes)    ( Default: 20000, Unit:  msec  )    timeout.level_5  Timeout when put object to LeoStorage  (1048576~ bytes)    ( Default: 30000, Unit:  msec  )    timeout.get  Timeout when get object from LeoStorage  ( Default: 30000, Unit:  msec  )    timeout.ls  Timeout when list object from LeoStorage  ( Default: 30000, Unit:  msec  )    Log     log.log_level  Log level: 0:debug 1:info 2:warn 3:error ( Default: 1 )    log.is_enable_access_log  Enables or disables the access-log feature ( Default: false )    log.erlang  Destination of log file(s) of Erlang's log ( Default: ./log/erlang )    log.app  Destination of log file(s) of LeoStorage ( Default: ./log/app )    log.member_dir  Destination of log file(s) of members of storage-cluster ( Default: ./log/ring )    log.ring_dir  Destination of log file(s) of RING ( Default: ./log/ring )    Other Directories Settings     queue_dir  Directory of queue for monitoring \"RING\" ( Default: ./work/queue )    snmp_agent  Directory of SNMP agent configuration ( Default: ./snmp/snmpa_gateway_0/LEO-GATEWAY )", 
            "title": "LeoGateway Configurations"
        }, 
        {
            "location": "/admin/settings/leo_gateway/#erlang-vms-related-configurations", 
            "text": "Item  Description      nodename  The format of the node name is  NAME @ IP-ADDRESS , which must be unique always in a LeoFS system ( Default: storage_0@127.0.0.1 )    distributed_cookie  Sets the magic cookie of the node to  Cookie .  - See also:  Distributed Erlang ( Default: 401321b4 )    erlang.kernel_poll  Kernel poll reduces LeoFS' CPU usage when it has hundreds (or more) network connections. ( Default: true )    erlang.asyc_threads  The total number of Erlang aynch threads ( Default: 32 )    erlang.max_ports  The max_ports sets the default value of maximum number of ports. - See also:  Erlang erlang:open_port/2 ( Default: 64000 )    erlang.crash_dump  The output destination of an Erlang crash dump ( Default: ./log/erl_crash.dump )    erlang.max_ets_tables  The maxinum number of  Erlagn ETS  tables ( Default: 256000 )    erlang.smp  -smp  enable and  -smp  start the Erlang runtime system with  SMP  support enabled. ( Default: enable )    erlang.schedulers.compaction_of_load  Enables or disables scheduler compaction of load. If it's enabled, the Erlang VM will attempt to fully load as many scheduler threads as mush as possible. ( Default: true )    erlang.schedulers.utilization_balancing  Enables or disables scheduler utilization balancing of load. By default scheduler utilization balancing is disabled and instead scheduler compaction of load is enabled, which strives for a load distribution that causes as many scheduler threads as possible to be fully loaded (that is, not run out of work). ( Default: false )    erlang.distribution_buffer_size  Sender-side network distribution buffer size  (unit: KB) ( Default: 32768 )    erlang.fullsweep_after  Option fullsweep_after makes it possible to specify the maximum number of generational collections before forcing a fullsweep, even if there is room on the old heap. Setting the number to zero disables the general collection algorithm, that is, all live data is copied at every garbage collection. ( Default: 0 )    erlang.secio  Enables or disables eager check I/O scheduling. The flag effects when schedulers will check for I/O operations possible to execute, and when such I/O operations will execute. ( Default: true )    process_limit  The maxinum number of Erlang processes. Sets the maximum number of simultaneously existing processes for this system if a Number is passed as value. Valid range for Number is [1024-134217727] ( Default: 1048576 )", 
            "title": "Erlang VM's Related Configurations"
        }, 
        {
            "location": "/admin/settings/leo_gateway/#notes-and-tips-of-the-configuration", 
            "text": "", 
            "title": "Notes and Tips of the Configuration"
        }, 
        {
            "location": "/admin/settings/leo_gateway/#cache-consistency-between-leogateway-and-leostorage", 
            "text": "LeoGateway's cache feature does not depend on the consistency level of a cluster. There is a possibility of object inconsistency.  LeoGateway requests a storage node to compare a cached object's hash value with its stored object's hash value. LeoGateway selects a LeoStorage's node from RING,  a distributed hash table  by a target object name, then LeoGateway requests a LeoStorage node of the redundant node. If the requested object is inconsistent in the replicas and LeoGateway cached it, a client may get inconsistent objects.  If you need strong consistency on a LeoFS system, you can disable the cache setting.  1\n2 cache.cache_ram_capacity   =   0  cache.cache_disc_capacity   =   0", 
            "title": "Cache Consistency between LeoGateway and LeoStorage"
        }, 
        {
            "location": "/admin/settings/leo_gateway/#configuration-related-to-disk-cache", 
            "text": "A total number of directories to store cache files is equal to  cache.cache_workers . A maximum size of a cacheable object per a directory has been determined by  cache.cache_disc_capacity / cache.cache_workers . If the size of a requested object more than the maximum size, LeoGateway avoids storing the object into the disk cache.  And also, when size of a requested object more than  cache.cache_max_content_len , LeoGateway similarly refuses to store the object into the disk cache.", 
            "title": "Configuration related to Disk Cache"
        }, 
        {
            "location": "/admin/settings/leo_gateway/#related-links", 
            "text": "Concept and Architecture / LeoGateway's Architecture  For Administrators / Interface / S3-API  For Administrators / Interface / REST-API  For Administrators / Interface / NFS v3  For Administrators / System Operations / S3-API related Operations  For Administrators / Settings / Environment Configuration       Amazon S3 API    Wikipedia: Network File System", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/protocols/s3/", 
            "text": "Interface: S3-API\n\n\nConfiguration of LeoGateway\n\n\nUpdate LeoGateway's protocol configuration to \ns3\n in your \nLeoGateway configuration\n\n\n1\n2\n3\n4\n5\n## --------------------------------------------------------------------\n\n\n## GATEWAY Protocol\n\n\n## --------------------------------------------------------------------\n\n\n## Gateway Protocol to use: [s3 | rest | embed | nfs]\n\n\nprotocol\n \n=\n \ns3\n\n\n\n\n\n\n\nS3 API Implementation Status\n\n\nObject Operation\n\n\n\n\n\n\n\n\nNo\n\n\nAPI\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n1\n\n\nDELETE Object\n\n\nYes\n\n\n\n\n\n\n2\n\n\nDelete Multiple Objects\n\n\nYes\n\n\n\n\n\n\n3\n\n\nGET Object\n\n\nYes\n\n\n\n\n\n\n4\n\n\nGET Object ACL\n\n\nPlans to support with v1.5\n\n\n\n\n\n\n5\n\n\nGET Object torrent\n\n\nNo\n\n\n\n\n\n\n6\n\n\nHEAD Object\n\n\nYes\n\n\n\n\n\n\n7\n\n\nPOST Object\n\n\nYes\n\n\n\n\n\n\n8\n\n\nPUT Object\n\n\nYes\n\n\n\n\n\n\n9\n\n\nPUT Object ACL\n\n\nPlans to support with v1.5\n\n\n\n\n\n\n10\n\n\nPUT Object - Copy\n\n\nYes\n\n\n\n\n\n\n11\n\n\nInitiate Multipart Upload\n\n\nYes\n\n\n\n\n\n\n12\n\n\nUpload Part\n\n\nYes\n\n\n\n\n\n\n13\n\n\nUpload Part - Copy\n\n\nYes\n\n\n\n\n\n\n14\n\n\nComplete Multipart Upload\n\n\nYes\n\n\n\n\n\n\n15\n\n\nAbort Multipart Upload\n\n\nYes\n\n\n\n\n\n\n16\n\n\nList Parts\n\n\nNo\n\n\n\n\n\n\n\n\nBucket Operation\n\n\n\n\n\n\n\n\nNo\n\n\nAPI\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n1\n\n\nDELETE Bucket\n\n\nYes\n\n\n\n\n\n\n2\n\n\nDELETE Bucket life cycle\n\n\nNo\n\n\n\n\n\n\n3\n\n\nDELETE Bucket policy\n\n\nNo\n\n\n\n\n\n\n4\n\n\nDELETE Bucket website\n\n\nNo\n\n\n\n\n\n\n5\n\n\nGET Bucket (List Objects)\n\n\nYes\n\n\n\n\n\n\n6\n\n\nGET Bucket ACL\n\n\nYes\n\n\n\n\n\n\n7\n\n\nGET Bucket life cycle\n\n\nNo\n\n\n\n\n\n\n8\n\n\nGET Bucket policy\n\n\nNo\n\n\n\n\n\n\n9\n\n\nGET Bucket location\n\n\nNo\n\n\n\n\n\n\n10\n\n\nGET Bucket logging\n\n\nNo\n\n\n\n\n\n\n11\n\n\nGET Bucket notification\n\n\nNo\n\n\n\n\n\n\n12\n\n\nGET Bucket Object versions\n\n\nPlan to support with v2.0\n\n\n\n\n\n\n13\n\n\nGET Bucket requestPayment\n\n\nNo\n\n\n\n\n\n\n14\n\n\nGET Bucket versioning\n\n\nPlan to support with v2.0\n\n\n\n\n\n\n15\n\n\nGET Bucket website\n\n\nNo\n\n\n\n\n\n\n16\n\n\nHEAD Bucket\n\n\nYes\n\n\n\n\n\n\n17\n\n\nList Multipart Uploads\n\n\nNo\n\n\n\n\n\n\n18\n\n\nPUT Bucket\n\n\nYes\n\n\n\n\n\n\n19\n\n\nPUT Bucket ACL\n\n\nYes\n\n\n\n\n\n\n20\n\n\nPUT Bucket life cycle\n\n\nNo\n\n\n\n\n\n\n21\n\n\nPUT Bucket policy\n\n\nNo\n\n\n\n\n\n\n22\n\n\nPUT Bucket logging\n\n\nNo\n\n\n\n\n\n\n23\n\n\nPUT Bucket notification\n\n\nNo\n\n\n\n\n\n\n24\n\n\nPUT Bucket requestPayment\n\n\nNo\n\n\n\n\n\n\n24\n\n\nPUT Bucket versioning\n\n\nPlan to support with v2.0\n\n\n\n\n\n\n25\n\n\nPUT Bucket website\n\n\nNo\n\n\n\n\n\n\n\n\nAmazon S3 Interface\n\n\nAmazon S3 Official Documentation\n\n\n\n\nAWS Documentation / Amazon Simple Storage Service (S3) / Developer Guide / What Is Amazon S3?\n\n\n\n\nHow to determine the name of Bucket\n\n\n\n\nVirtual Hosting of Buckets\n\n\nValues used for determining the name:\n\n\nS3 uses a HTTP host header or a path segment in the HTTP request line\n\n\nHow S3 determines what to use depends on a domain name\n\n\n\n\n\n\n\n\n1. \nhttps://s3.amazonaws.com/bucket/path_to_file\n\n\nIn this case, a bucket's name is the first segment of a path.\n\n\n\n\n\n\n\n\nName\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nRequest line\n\n\nGET /bucket/path_to_file HTTP/1.1\n\n\n\n\n\n\nHost header\n\n\nHost: s3.amazonaws.com\n\n\n\n\n\n\n\n\n2. \nhttps://www.example.com.s3.amazonaws.com/path_to_file\n\n\nIn this case, the name of the bucket is the part of the domain name before \n.s3.amazonaws.com\n.\n\n\n\n\n\n\n\n\nName\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nRequest line\n\n\nGET /path_to_file HTTP/1.1\n\n\n\n\n\n\nHost header\n\n\nHost: \nwww.example.com.s3.amazonaws.com\n\n\n\n\n\n\n\n\n3. \nhttps://www.example.com/path_to_file\n\n\nIn this case, a bucket's name is equal to the \nFQDN\n.\n\n\n\n\n\n\n\n\nName\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nRequest line\n\n\nGET /path_to_file HTTP/1.1\n\n\n\n\n\n\nHost header\n\n\nHost header\n\n\n\n\n\n\n\n\nThe argument of LeoFS\u2019 \nwhereis\n OR \npurge\n commands should be \"\nwww.example.com/path_to_file\n\".\n\n\nNaming Rule of a Bucket\n\n\nWe recommend as a best practice that you always use DNS-compliant bucket names regardless of the region in which you create the bucket.\n\n\n\n\nBucket names can be as long as between 3 and 255 characters.\n\n\nBucket names can contain lowercase letters, numbers, periods (.), dashes (-) and underscores (_).\n\n\nBucket names must not be formatted as an IP address (e.g., 192.168.5.4).\n\n\nBucket name cannot start and end with periods (.), dashes (-) and underscores (_).\n\n\n\n\nS3 Clients\n\n\n\n\nAWS SDKs\n\n\nDragon Disk, A cloud stroage client\n\n\nS3cmd, Command Line S3 Client and Backup for Linux and Mac\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Settings / LeoGateway Settings", 
            "title": "S3-API"
        }, 
        {
            "location": "/admin/protocols/s3/#interface-s3-api", 
            "text": "", 
            "title": "Interface: S3-API"
        }, 
        {
            "location": "/admin/protocols/s3/#configuration-of-leogateway", 
            "text": "Update LeoGateway's protocol configuration to  s3  in your  LeoGateway configuration  1\n2\n3\n4\n5 ## --------------------------------------------------------------------  ## GATEWAY Protocol  ## --------------------------------------------------------------------  ## Gateway Protocol to use: [s3 | rest | embed | nfs]  protocol   =   s3", 
            "title": "Configuration of LeoGateway"
        }, 
        {
            "location": "/admin/protocols/s3/#s3-api-implementation-status", 
            "text": "", 
            "title": "S3 API Implementation Status"
        }, 
        {
            "location": "/admin/protocols/s3/#object-operation", 
            "text": "No  API  Status      1  DELETE Object  Yes    2  Delete Multiple Objects  Yes    3  GET Object  Yes    4  GET Object ACL  Plans to support with v1.5    5  GET Object torrent  No    6  HEAD Object  Yes    7  POST Object  Yes    8  PUT Object  Yes    9  PUT Object ACL  Plans to support with v1.5    10  PUT Object - Copy  Yes    11  Initiate Multipart Upload  Yes    12  Upload Part  Yes    13  Upload Part - Copy  Yes    14  Complete Multipart Upload  Yes    15  Abort Multipart Upload  Yes    16  List Parts  No", 
            "title": "Object Operation"
        }, 
        {
            "location": "/admin/protocols/s3/#bucket-operation", 
            "text": "No  API  Status      1  DELETE Bucket  Yes    2  DELETE Bucket life cycle  No    3  DELETE Bucket policy  No    4  DELETE Bucket website  No    5  GET Bucket (List Objects)  Yes    6  GET Bucket ACL  Yes    7  GET Bucket life cycle  No    8  GET Bucket policy  No    9  GET Bucket location  No    10  GET Bucket logging  No    11  GET Bucket notification  No    12  GET Bucket Object versions  Plan to support with v2.0    13  GET Bucket requestPayment  No    14  GET Bucket versioning  Plan to support with v2.0    15  GET Bucket website  No    16  HEAD Bucket  Yes    17  List Multipart Uploads  No    18  PUT Bucket  Yes    19  PUT Bucket ACL  Yes    20  PUT Bucket life cycle  No    21  PUT Bucket policy  No    22  PUT Bucket logging  No    23  PUT Bucket notification  No    24  PUT Bucket requestPayment  No    24  PUT Bucket versioning  Plan to support with v2.0    25  PUT Bucket website  No", 
            "title": "Bucket Operation"
        }, 
        {
            "location": "/admin/protocols/s3/#amazon-s3-interface", 
            "text": "", 
            "title": "Amazon S3 Interface"
        }, 
        {
            "location": "/admin/protocols/s3/#amazon-s3-official-documentation", 
            "text": "AWS Documentation / Amazon Simple Storage Service (S3) / Developer Guide / What Is Amazon S3?", 
            "title": "Amazon S3 Official Documentation"
        }, 
        {
            "location": "/admin/protocols/s3/#how-to-determine-the-name-of-bucket", 
            "text": "Virtual Hosting of Buckets  Values used for determining the name:  S3 uses a HTTP host header or a path segment in the HTTP request line  How S3 determines what to use depends on a domain name", 
            "title": "How to determine the name of Bucket"
        }, 
        {
            "location": "/admin/protocols/s3/#1-httpss3amazonawscombucketpath_to_file", 
            "text": "In this case, a bucket's name is the first segment of a path.     Name  Value      Request line  GET /bucket/path_to_file HTTP/1.1    Host header  Host: s3.amazonaws.com", 
            "title": "1. https://s3.amazonaws.com/bucket/path_to_file"
        }, 
        {
            "location": "/admin/protocols/s3/#2-httpswwwexamplecoms3amazonawscompath_to_file", 
            "text": "In this case, the name of the bucket is the part of the domain name before  .s3.amazonaws.com .     Name  Value      Request line  GET /path_to_file HTTP/1.1    Host header  Host:  www.example.com.s3.amazonaws.com", 
            "title": "2. https://www.example.com.s3.amazonaws.com/path_to_file"
        }, 
        {
            "location": "/admin/protocols/s3/#3-httpswwwexamplecompath_to_file", 
            "text": "In this case, a bucket's name is equal to the  FQDN .     Name  Value      Request line  GET /path_to_file HTTP/1.1    Host header  Host header     The argument of LeoFS\u2019  whereis  OR  purge  commands should be \" www.example.com/path_to_file \".", 
            "title": "3. https://www.example.com/path_to_file"
        }, 
        {
            "location": "/admin/protocols/s3/#naming-rule-of-a-bucket", 
            "text": "We recommend as a best practice that you always use DNS-compliant bucket names regardless of the region in which you create the bucket.   Bucket names can be as long as between 3 and 255 characters.  Bucket names can contain lowercase letters, numbers, periods (.), dashes (-) and underscores (_).  Bucket names must not be formatted as an IP address (e.g., 192.168.5.4).  Bucket name cannot start and end with periods (.), dashes (-) and underscores (_).", 
            "title": "Naming Rule of a Bucket"
        }, 
        {
            "location": "/admin/protocols/s3/#s3-clients", 
            "text": "AWS SDKs  Dragon Disk, A cloud stroage client  S3cmd, Command Line S3 Client and Backup for Linux and Mac", 
            "title": "S3 Clients"
        }, 
        {
            "location": "/admin/protocols/s3/#related-links", 
            "text": "For Administrators / Settings / LeoGateway Settings", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/protocols/rest/", 
            "text": "Interface: REST-API\n\n\nConfiguration\n\n\nUpdate LeoGateway's protocol configuration to \nrest\n in your \nLeoGateway configuration\n\n\n1\n2\n3\n4\n5\n## --------------------------------------------------------------------\n\n\n## GATEWAY Protocol\n\n\n## --------------------------------------------------------------------\n\n\n## Gateway Protocol to use: [s3 | rest | embed | nfs]\n\n\nprotocol\n \n=\n \nrest\n\n\n\n\n\n\n\nInterface\n\n\nDescription of LeoFS\u2019 behavior for each HTTP verb\n\n\n\n\n\n\n\n\nHTTP Verb\n\n\nLeoFS\u2019 Behavior\n\n\n\n\n\n\n\n\n\n\nPUT/POST\n\n\nInsert an object into the storage cluster\n\n\n\n\n\n\nGET\n\n\nRetrieve an object from the storage cluster\n\n\n\n\n\n\nDELETE\n\n\nRemove an object from the storage cluster\n\n\n\n\n\n\n\n\nURL format\n\n\n\n\nURL format: http[s]://\nHOST\n:8080/\nFILEPATH\n\n\nLeoFS only uses the \nFILEPATH\n which part of the URL to identify objects.\n\n\nYou can check that an object exists in a LeoFS' cluster by using \nleofs-adm whereis\n command.\n\n\n\n\n\n\n\n\n1\n$ leofs-adm whereis \nFILEPATH\n\n\n\n\n\n\n\nExamples\n\n\nPOST/PUT\n\n\n1\n2\n3\n4\n5\n$ curl -X POST -H \nContent-Type:image/jpg\n \n\\\n\n          --data-binary @test_1.jpg https://hostname:8080/_test/_image/file.png\n\n$ curl -X PUT -H \nContent-Type:image/jpg\n \n\\\n\n          --data-binary @test_2.jpg https://hostname:8080/_test/_image/file.png\n\n\n\n\n\n\nGET\n\n\n1\n$ curl -X GET https://hostname:8080/_test/_image/test_2.jpg\n\n\n\n\n\n\nDELETE\n\n\n1\n$ curl -X DELETE https://hostname:8080/_test/_image/file.png\n\n\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Settings / LeoGateway Settings", 
            "title": "REST-API"
        }, 
        {
            "location": "/admin/protocols/rest/#interface-rest-api", 
            "text": "", 
            "title": "Interface: REST-API"
        }, 
        {
            "location": "/admin/protocols/rest/#configuration", 
            "text": "Update LeoGateway's protocol configuration to  rest  in your  LeoGateway configuration  1\n2\n3\n4\n5 ## --------------------------------------------------------------------  ## GATEWAY Protocol  ## --------------------------------------------------------------------  ## Gateway Protocol to use: [s3 | rest | embed | nfs]  protocol   =   rest", 
            "title": "Configuration"
        }, 
        {
            "location": "/admin/protocols/rest/#interface", 
            "text": "", 
            "title": "Interface"
        }, 
        {
            "location": "/admin/protocols/rest/#description-of-leofs-behavior-for-each-http-verb", 
            "text": "HTTP Verb  LeoFS\u2019 Behavior      PUT/POST  Insert an object into the storage cluster    GET  Retrieve an object from the storage cluster    DELETE  Remove an object from the storage cluster", 
            "title": "Description of LeoFS\u2019 behavior for each HTTP verb"
        }, 
        {
            "location": "/admin/protocols/rest/#url-format", 
            "text": "URL format: http[s]:// HOST :8080/ FILEPATH  LeoFS only uses the  FILEPATH  which part of the URL to identify objects.  You can check that an object exists in a LeoFS' cluster by using  leofs-adm whereis  command.     1 $ leofs-adm whereis  FILEPATH", 
            "title": "URL format"
        }, 
        {
            "location": "/admin/protocols/rest/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/admin/protocols/rest/#postput", 
            "text": "1\n2\n3\n4\n5 $ curl -X POST -H  Content-Type:image/jpg   \\ \n          --data-binary @test_1.jpg https://hostname:8080/_test/_image/file.png\n\n$ curl -X PUT -H  Content-Type:image/jpg   \\ \n          --data-binary @test_2.jpg https://hostname:8080/_test/_image/file.png", 
            "title": "POST/PUT"
        }, 
        {
            "location": "/admin/protocols/rest/#get", 
            "text": "1 $ curl -X GET https://hostname:8080/_test/_image/test_2.jpg", 
            "title": "GET"
        }, 
        {
            "location": "/admin/protocols/rest/#delete", 
            "text": "1 $ curl -X DELETE https://hostname:8080/_test/_image/file.png", 
            "title": "DELETE"
        }, 
        {
            "location": "/admin/protocols/rest/#related-links", 
            "text": "For Administrators / Settings / LeoGateway Settings", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/protocols/nfs_v3/", 
            "text": "Interface: NFS v3\n\n\nPre-requirement\n\n\nWe have checked the \nNFSv3\n server feature with CentOS 6.5, 7.x and Ubuntu Server 14.04, 16.04 LTS, but we did not strictly test other platforms, FreeBSD and SmartOS yet.\n\n\nConfiguration\n\n\nUpdate LeoGateway's protocol configuration to \nnfs\n, and configure NFS related configurations in your \nLeoGateway configuration\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n## --------------------------------------------------------------------\n\n\n## GATEWAY Protocol\n\n\n## --------------------------------------------------------------------\n\n\n## Gateway Protocol to use: [s3 | rest | embed | nfs]\n\n\nprotocol\n \n=\n \nnfs\n\n\n.\n\n\n.\n\n\n.\n\n\n## --------------------------------------------------------------------\n\n\n## GATEWAY - NFS-related configurations\n\n\n## --------------------------------------------------------------------\n\n\n## Mountd\ns port number\n\n\nnfs.mountd.port\n \n=\n \n22050\n\n\n\n## Mountd\ns the number of acceptors\n\n\nnfs.mountd.acceptors\n \n=\n \n128\n\n\n\n## NFSd\ns port number\n\n\nnfs.nfsd.port\n \n=\n \n2049\n\n\n\n## NFSd\ns the number of acceptors\n\n\nnfs.nfsd.acceptors\n \n=\n \n128\n\n\n\n\n\n\n\nInstallation\n\n\nCentOS 6.x / 7.x\n\n\n1\n$ sudo yum install nfs-utils\n\n\n\n\n\n\nUbnutu 14.04 / 16.04\n\n\n1\n$ sudo apt-get install nfs-common\n\n\n\n\n\n\nStart LeoFS as a NFS Server with other dependent programs\n\n\n\n\nStart a LeoFS storage system\n\n\nRef: \nQuick Installation and Setup\n\n\nRef: \nBuilding a LeoFS' cluster with Ansible\n\n\n\n\n\n\nStart \nrpcbind\n\n\n\n\n1\n$ sudo service rpcbind start\n\n\n\n\n\n\n\n\nCreate a bucket and a token for LeoFS' NFSv3 server with \nleofs-adm gen-nfs-mnt-key \nBUCKET\n \nACCESS-KEY-ID\n \nCLIENT-IP_ADDRESS\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n$ ./leofs-adm add-bucket \ntest\n \n05236\n\nOK\n\n$ ./leofs-adm get-buckets\ncluster id   \n|\n bucket   \n|\n owner       \n|\n permissions      \n|\n created at\n-------------+----------+-------------+------------------+---------------------------\nleofs_1      \n|\n \ntest\n     \n|\n _test_leofs \n|\n Me\n(\nfull_control\n)\n \n|\n \n2014\n-07-31 \n10\n:20:42 +0900\n\n$ ./leofs-adm gen-nfs-mnt-key \ntest\n \n05236\n \n127\n.0.0.1\nbb5034f0c740148a346ed663ca0cf5157efb439f\n\n\n\n\n\n\n\n\nCreate a mount point and Mount\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n$ sudo mkdir /mnt/leofs\n\n\n## for Linux - \nsudo mount -t nfs -o nolock \nhost\n:/\nbucket\n/\ntoken\n \ndir\n\n$ sudo mount -t nfs -o nolock \n127\n.0.0.1:/test/05236/bb5034f0c740148a346ed663ca0cf5157efb439f /mnt/leofs\n\n\n## for FreeBSD - \nsudo mount -t nfs -o nolockd \nhost\n:/\nbucket\n/\ntoken\n \ndir\n\n$ sudo mount -t nfs -o nolockd \n127\n.0.0.1:/test/05236/bb5034f0c740148a346ed663ca0cf5157efb439f /mnt/leofs\n\n\n\n\n\n\n\n\nNow you can operate the bucket test in LeoFS as a filesystem via \n/mnt/leofs\n.\n\n\n\n\nConfirm that NFS works\n\n\n\n\nCreate a file\n\n\n\n\n1\n2\n3\n4\n5\n6\n$ touch /mnt/leofs/newfile\n$ ls -al /mnt/leofs\n\ndrwxrwxrwx. \n0\n root root \n4096\n Jul \n31\n \n10\n:09 \n2014\n .\ndrwxr-xr-x. \n6\n root root \n4096\n Jul \n11\n \n12\n:38 \n2014\n ..\n-rw-rw-rw-  \n0\n root root    \n0\n Jul \n31\n \n10\n:25 \n2014\n newfile\n\n\n\n\n\n\n\n\nModify a file\n\n\n\n\n1\n2\n3\n4\n$ \necho\n \nhello world\n \n /mnt/leofs/newfile\n$ cat /mnt/leofs/newfile\n\nhello world\n\n\n\n\n\n\n\n\nCopy a file\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n$ cp /mnt/leofs/newfile /mnt/leofs/newfile.copy\n$ ls -al /mnt/leofs\n\ndrwxrwxrwx  \n0\n root root \n4096\n Jul \n31\n \n10\n:09 \n2014\n .\ndrwxr-xr-x. \n6\n root root \n4096\n Jul \n11\n \n12\n:38 \n2014\n ..\n-rw-rw-rw-  \n0\n root root   \n12\n Jul \n31\n \n10\n:29 \n2014\n newfile\n-rw-rw-rw-  \n0\n root root   \n12\n Jul \n31\n \n10\n:31 \n2014\n newfile.copy\n\n\n\n\n\n\n\n\nCheck the file whether to store it into LeoFS with the \nleofs-adm whereis\n command\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n$ ./leofs-adm whereis test/newfile\n-------+--------------------------+--------------------------------------+------------+--------------+----------------+----------------+----------------------------\n del?  \n|\n           node           \n|\n             ring address             \n|\n    size    \n|\n   checksum   \n|\n  \n# of chunks   |     clock      |             when\n\n-------+--------------------------+--------------------------------------+------------+--------------+----------------+----------------+----------------------------\n       \n|\n storage_0@127.0.0.1      \n|\n 22f3d93762d31abc5f5704f78edf1691     \n|\n        12B \n|\n   6f5902ac23 \n|\n              \n0\n \n|\n 4ffe2d105f1f4  \n|\n \n2014\n-07-31 \n10\n:29:01 +0900\n\n$ ./leofs-adm whereis test/newfile.copy\n-------+--------------------------+--------------------------------------+------------+--------------+----------------+----------------+----------------------------\n del?  \n|\n           node           \n|\n             ring address             \n|\n    size    \n|\n   checksum   \n|\n  \n# of chunks   |     clock      |             when\n\n-------+--------------------------+--------------------------------------+------------+--------------+----------------+----------------+----------------------------\n       \n|\n storage_0@127.0.0.1      \n|\n d02e1e52d93242d2dcdb98224421a1fb     \n|\n        12B \n|\n   6f5902ac23 \n|\n              \n0\n \n|\n 4ffe2d20343a3  \n|\n \n2014\n-07-31 \n10\n:31:17 +0900\n\n\n\n\n\n\n\n\nDiff files\n\n\n\n\n1\n$ diff /mnt/leofs/newfile /mnt/leofs/newfile.copy\n\n\n\n\n\n\n\n\nRemove a file\n\n\n\n\n1\n2\n3\n4\n5\n6\n$ rm /mnt/leofs/newfile\n$ ls -al /mnt/leofs\n\ndrwxrwxrwx  \n0\n root root \n4096\n Jul \n31\n \n10\n:09 \n2014\n .\ndrwxr-xr-x. \n6\n root root \n4096\n Jul \n11\n \n12\n:38 \n2014\n ..\n-rw-rw-rw-  \n0\n root root   \n12\n Jul \n31\n \n10\n:31 \n2014\n newfile.copy\n\n\n\n\n\n\n\n\nCheck the file whether to remove it into LeoFS with the \nleofs-adm whereis\n command\n\n\n\n\n1\n2\n3\n4\n5\n$ ./leofs-adm whereis test/newfile\n-------+--------------------------+--------------------------------------+------------+--------------+----------------+----------------+----------------------------\n del?  \n|\n           node           \n|\n             ring address             \n|\n    size    \n|\n   checksum   \n|\n  \n# of chunks   |     clock      |             when\n\n-------+--------------------------+--------------------------------------+------------+--------------+----------------+----------------+----------------------------\n  *    \n|\n storage_0@127.0.0.1      \n|\n 22f3d93762d31abc5f5704f78edf1691     \n|\n         0B \n|\n   d41d8cd98f \n|\n              \n0\n \n|\n 4ffe2e5d9cffe  \n|\n \n2014\n-07-31 \n10\n:34:50 +0900\n\n\n\n\n\n\n\n\nCreate a directory\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n$ mkdir -p /mnt/leofs/1/2/3\n$ ls -alR /mnt/leofs/1\n\n/mnt/leofs/1:\ndrwxrwxrwx \n0\n root root \n4096\n Jul \n31\n \n19\n:37 \n2014\n .\ndrwxrwxrwx \n0\n root root \n4096\n Jul \n31\n \n10\n:09 \n2014\n ..\ndrwxrwxrwx \n0\n root root \n4096\n Jul \n31\n \n10\n:37 \n2014\n \n2\n\n\n/mnt/leofs/1/2:\ndrwxrwxrwx \n0\n root root \n4096\n Jul \n31\n \n19\n:37 \n2014\n .\ndrwxrwxrwx \n0\n root root \n4096\n Jul \n31\n \n19\n:37 \n2014\n ..\ndrwxrwxrwx \n0\n root root \n4096\n Jul \n31\n \n10\n:37 \n2014\n \n3\n\n\n/mnt/leofs/1/2/3:\ndrwxrwxrwx \n0\n root root \n4096\n Jul \n31\n \n19\n:37 \n2014\n .\ndrwxrwxrwx \n0\n root root \n4096\n Jul \n31\n \n19\n:37 \n2014\n ..\n\n\n\n\n\n\n\n\nRemove files recursively\n\n\n\n\n1\n2\n3\n4\n5\n6\n$ rm -rf /mnt/leofs/1/\n$ ls -al /mnt/leofs\n\ndrwxrwxrwx  \n0\n root root \n4096\n Jul \n31\n \n10\n:09 \n2014\n .\ndrwxr-xr-x. \n6\n root root \n4096\n Jul \n11\n \n12\n:38 \n2014\n ..\n-rw-rw-rw-  \n0\n root root   \n12\n Jul \n31\n \n10\n:31 \n2014\n leofs.copy\n\n\n\n\n\n\nOther basic file OR directory operations also should work except controlling owners/permissions/symbolic links/special files.\n\n\nLimits\n\n\nSince LeoFS NFS implementation is still the beta version, there are some limitations. The details are described at \nLeoFS Limits\n.\n\n\nRelated Links\n\n\n\n\nFor Administrators / Settings / LeoGateway Settings", 
            "title": "NFS v3"
        }, 
        {
            "location": "/admin/protocols/nfs_v3/#interface-nfs-v3", 
            "text": "", 
            "title": "Interface: NFS v3"
        }, 
        {
            "location": "/admin/protocols/nfs_v3/#pre-requirement", 
            "text": "We have checked the  NFSv3  server feature with CentOS 6.5, 7.x and Ubuntu Server 14.04, 16.04 LTS, but we did not strictly test other platforms, FreeBSD and SmartOS yet.", 
            "title": "Pre-requirement"
        }, 
        {
            "location": "/admin/protocols/nfs_v3/#configuration", 
            "text": "Update LeoGateway's protocol configuration to  nfs , and configure NFS related configurations in your  LeoGateway configuration   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22 ## --------------------------------------------------------------------  ## GATEWAY Protocol  ## --------------------------------------------------------------------  ## Gateway Protocol to use: [s3 | rest | embed | nfs]  protocol   =   nfs  .  .  .  ## --------------------------------------------------------------------  ## GATEWAY - NFS-related configurations  ## --------------------------------------------------------------------  ## Mountd s port number  nfs.mountd.port   =   22050  ## Mountd s the number of acceptors  nfs.mountd.acceptors   =   128  ## NFSd s port number  nfs.nfsd.port   =   2049  ## NFSd s the number of acceptors  nfs.nfsd.acceptors   =   128", 
            "title": "Configuration"
        }, 
        {
            "location": "/admin/protocols/nfs_v3/#installation", 
            "text": "", 
            "title": "Installation"
        }, 
        {
            "location": "/admin/protocols/nfs_v3/#centos-6x-7x", 
            "text": "1 $ sudo yum install nfs-utils", 
            "title": "CentOS 6.x / 7.x"
        }, 
        {
            "location": "/admin/protocols/nfs_v3/#ubnutu-1404-1604", 
            "text": "1 $ sudo apt-get install nfs-common", 
            "title": "Ubnutu 14.04 / 16.04"
        }, 
        {
            "location": "/admin/protocols/nfs_v3/#start-leofs-as-a-nfs-server-with-other-dependent-programs", 
            "text": "Start a LeoFS storage system  Ref:  Quick Installation and Setup  Ref:  Building a LeoFS' cluster with Ansible    Start  rpcbind   1 $ sudo service rpcbind start    Create a bucket and a token for LeoFS' NFSv3 server with  leofs-adm gen-nfs-mnt-key  BUCKET   ACCESS-KEY-ID   CLIENT-IP_ADDRESS    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 $ ./leofs-adm add-bucket  test   05236 \nOK\n\n$ ./leofs-adm get-buckets\ncluster id    |  bucket    |  owner        |  permissions       |  created at\n-------------+----------+-------------+------------------+---------------------------\nleofs_1       |   test       |  _test_leofs  |  Me ( full_control )   |   2014 -07-31  10 :20:42 +0900\n\n$ ./leofs-adm gen-nfs-mnt-key  test   05236   127 .0.0.1\nbb5034f0c740148a346ed663ca0cf5157efb439f    Create a mount point and Mount   1\n2\n3\n4\n5\n6\n7 $ sudo mkdir /mnt/leofs ## for Linux -  sudo mount -t nfs -o nolock  host :/ bucket / token   dir \n$ sudo mount -t nfs -o nolock  127 .0.0.1:/test/05236/bb5034f0c740148a346ed663ca0cf5157efb439f /mnt/leofs ## for FreeBSD -  sudo mount -t nfs -o nolockd  host :/ bucket / token   dir \n$ sudo mount -t nfs -o nolockd  127 .0.0.1:/test/05236/bb5034f0c740148a346ed663ca0cf5157efb439f /mnt/leofs    Now you can operate the bucket test in LeoFS as a filesystem via  /mnt/leofs .", 
            "title": "Start LeoFS as a NFS Server with other dependent programs"
        }, 
        {
            "location": "/admin/protocols/nfs_v3/#confirm-that-nfs-works", 
            "text": "Create a file   1\n2\n3\n4\n5\n6 $ touch /mnt/leofs/newfile\n$ ls -al /mnt/leofs\n\ndrwxrwxrwx.  0  root root  4096  Jul  31   10 :09  2014  .\ndrwxr-xr-x.  6  root root  4096  Jul  11   12 :38  2014  ..\n-rw-rw-rw-   0  root root     0  Jul  31   10 :25  2014  newfile    Modify a file   1\n2\n3\n4 $  echo   hello world    /mnt/leofs/newfile\n$ cat /mnt/leofs/newfile\n\nhello world    Copy a file   1\n2\n3\n4\n5\n6\n7 $ cp /mnt/leofs/newfile /mnt/leofs/newfile.copy\n$ ls -al /mnt/leofs\n\ndrwxrwxrwx   0  root root  4096  Jul  31   10 :09  2014  .\ndrwxr-xr-x.  6  root root  4096  Jul  11   12 :38  2014  ..\n-rw-rw-rw-   0  root root    12  Jul  31   10 :29  2014  newfile\n-rw-rw-rw-   0  root root    12  Jul  31   10 :31  2014  newfile.copy    Check the file whether to store it into LeoFS with the  leofs-adm whereis  command    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 $ ./leofs-adm whereis test/newfile\n-------+--------------------------+--------------------------------------+------------+--------------+----------------+----------------+----------------------------\n del?   |            node            |              ring address              |     size     |    checksum    |    # of chunks   |     clock      |             when \n-------+--------------------------+--------------------------------------+------------+--------------+----------------+----------------+----------------------------\n        |  storage_0@127.0.0.1       |  22f3d93762d31abc5f5704f78edf1691      |         12B  |    6f5902ac23  |                0   |  4ffe2d105f1f4   |   2014 -07-31  10 :29:01 +0900\n\n$ ./leofs-adm whereis test/newfile.copy\n-------+--------------------------+--------------------------------------+------------+--------------+----------------+----------------+----------------------------\n del?   |            node            |              ring address              |     size     |    checksum    |    # of chunks   |     clock      |             when \n-------+--------------------------+--------------------------------------+------------+--------------+----------------+----------------+----------------------------\n        |  storage_0@127.0.0.1       |  d02e1e52d93242d2dcdb98224421a1fb      |         12B  |    6f5902ac23  |                0   |  4ffe2d20343a3   |   2014 -07-31  10 :31:17 +0900    Diff files   1 $ diff /mnt/leofs/newfile /mnt/leofs/newfile.copy    Remove a file   1\n2\n3\n4\n5\n6 $ rm /mnt/leofs/newfile\n$ ls -al /mnt/leofs\n\ndrwxrwxrwx   0  root root  4096  Jul  31   10 :09  2014  .\ndrwxr-xr-x.  6  root root  4096  Jul  11   12 :38  2014  ..\n-rw-rw-rw-   0  root root    12  Jul  31   10 :31  2014  newfile.copy    Check the file whether to remove it into LeoFS with the  leofs-adm whereis  command   1\n2\n3\n4\n5 $ ./leofs-adm whereis test/newfile\n-------+--------------------------+--------------------------------------+------------+--------------+----------------+----------------+----------------------------\n del?   |            node            |              ring address              |     size     |    checksum    |    # of chunks   |     clock      |             when \n-------+--------------------------+--------------------------------------+------------+--------------+----------------+----------------+----------------------------\n  *     |  storage_0@127.0.0.1       |  22f3d93762d31abc5f5704f78edf1691      |          0B  |    d41d8cd98f  |                0   |  4ffe2e5d9cffe   |   2014 -07-31  10 :34:50 +0900    Create a directory    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 $ mkdir -p /mnt/leofs/1/2/3\n$ ls -alR /mnt/leofs/1\n\n/mnt/leofs/1:\ndrwxrwxrwx  0  root root  4096  Jul  31   19 :37  2014  .\ndrwxrwxrwx  0  root root  4096  Jul  31   10 :09  2014  ..\ndrwxrwxrwx  0  root root  4096  Jul  31   10 :37  2014   2 \n\n/mnt/leofs/1/2:\ndrwxrwxrwx  0  root root  4096  Jul  31   19 :37  2014  .\ndrwxrwxrwx  0  root root  4096  Jul  31   19 :37  2014  ..\ndrwxrwxrwx  0  root root  4096  Jul  31   10 :37  2014   3 \n\n/mnt/leofs/1/2/3:\ndrwxrwxrwx  0  root root  4096  Jul  31   19 :37  2014  .\ndrwxrwxrwx  0  root root  4096  Jul  31   19 :37  2014  ..    Remove files recursively   1\n2\n3\n4\n5\n6 $ rm -rf /mnt/leofs/1/\n$ ls -al /mnt/leofs\n\ndrwxrwxrwx   0  root root  4096  Jul  31   10 :09  2014  .\ndrwxr-xr-x.  6  root root  4096  Jul  11   12 :38  2014  ..\n-rw-rw-rw-   0  root root    12  Jul  31   10 :31  2014  leofs.copy   Other basic file OR directory operations also should work except controlling owners/permissions/symbolic links/special files.", 
            "title": "Confirm that NFS works"
        }, 
        {
            "location": "/admin/protocols/nfs_v3/#limits", 
            "text": "Since LeoFS NFS implementation is still the beta version, there are some limitations. The details are described at  LeoFS Limits .", 
            "title": "Limits"
        }, 
        {
            "location": "/admin/protocols/nfs_v3/#related-links", 
            "text": "For Administrators / Settings / LeoGateway Settings", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/system_operations/flow/", 
            "text": "Launch and Operation Flow\n\n\nOperation Flow\n\n\n\n\nBuild a LeoManager cluster\n\n\nConfirm the state of the LeoManager cluster\n\n\nBuild a LeoStorage cluster\n\n\nConfirm the state of the LeoStorage cluster\n\n\nLaunch LeoGateway nodes\n\n\nConfirm the state of the LeoFS system\n\n\nAfter launching the system, execute processing depending on the status of the system\n\n\nAttach a LeoStorage node\n\n\nDetach a LeoStorage node\n\n\nSuspend a LeoStorage node\n\n\nResume a LeoStorage node\n\n\n\n\n\n\n\n\n\n\n\n\nThe diagram only\n\n\n\n\nLaunch Order of LeoFS' Components\n\n\n\n\n\n\n\n\n\n\nCommand\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nLeoManager\n\n\n\n\n\n\n\n\n$ leofs_manager start\n\n\nStart LeoManager\u2019s master\n\n\n\n\n\n\n$ leofs_manager start\n\n\nStart LeoManager\u2019s slave\n\n\n\n\n\n\nLeoManager\n\n\n\n\n\n\n\n\n$ leofs_storage start\n\n\nStart a LeoStorage node\n\n\n\n\n\n\n(\nRepeatedly launch LeoStorage nodes\n)\n\n\n\n\n\n\n\n\n$ leofs-adm start\n\n\nStart Leostorage cluster\n\n\n\n\n\n\n$ leofs-adm status\n\n\nConfirm the current state of the LeoFS system (1)\n\n\n\n\n\n\nLeoGateway\n\n\n\n\n\n\n\n\n$ bin/leofs_gateway start\n\n\nStart a LeoGateway node\n\n\n\n\n\n\n(\nRepeatedly launch LeoGateway nodes\n)\n\n\n\n\n\n\n\n\n$ leofs-adm status\n\n\nConfirm the current state of the LeoFS system (2)\n\n\n\n\n\n\n\n\n\n\nNote: Restart a LeoManager when both of them are down\n\n\nWhen both of the LeoManagers are down and you try to restart a LeoManager that is NOT the one terminated at last, you can not restart the LeoManager because a sprit-brain could happen. If you make sure there is no case the data inconsistency could happen due to a sprit-brain then do \nleo_manager start force_load\n that allows you to restart a LeoManager in such cases. Please refer \nWhat is the significance of a Mnesia Master Node in a cluster\n for more information.\n\n\n\n\nRelated Links\n\n\n\n\nGetting Started / Quick Installation and Setup\n\n\nGetting Started / Building a LeoFS' cluster with Ansible\n\n\nFor Administrators / System Operations / Cluster Operations", 
            "title": "Operation Flow"
        }, 
        {
            "location": "/admin/system_operations/flow/#launch-and-operation-flow", 
            "text": "", 
            "title": "Launch and Operation Flow"
        }, 
        {
            "location": "/admin/system_operations/flow/#operation-flow", 
            "text": "Build a LeoManager cluster  Confirm the state of the LeoManager cluster  Build a LeoStorage cluster  Confirm the state of the LeoStorage cluster  Launch LeoGateway nodes  Confirm the state of the LeoFS system  After launching the system, execute processing depending on the status of the system  Attach a LeoStorage node  Detach a LeoStorage node  Suspend a LeoStorage node  Resume a LeoStorage node       The diagram only", 
            "title": "Operation Flow"
        }, 
        {
            "location": "/admin/system_operations/flow/#launch-order-of-leofs-components", 
            "text": "Command  Description      LeoManager     $ leofs_manager start  Start LeoManager\u2019s master    $ leofs_manager start  Start LeoManager\u2019s slave    LeoManager     $ leofs_storage start  Start a LeoStorage node    ( Repeatedly launch LeoStorage nodes )     $ leofs-adm start  Start Leostorage cluster    $ leofs-adm status  Confirm the current state of the LeoFS system (1)    LeoGateway     $ bin/leofs_gateway start  Start a LeoGateway node    ( Repeatedly launch LeoGateway nodes )     $ leofs-adm status  Confirm the current state of the LeoFS system (2)      Note: Restart a LeoManager when both of them are down  When both of the LeoManagers are down and you try to restart a LeoManager that is NOT the one terminated at last, you can not restart the LeoManager because a sprit-brain could happen. If you make sure there is no case the data inconsistency could happen due to a sprit-brain then do  leo_manager start force_load  that allows you to restart a LeoManager in such cases. Please refer  What is the significance of a Mnesia Master Node in a cluster  for more information.", 
            "title": "Launch Order of LeoFS' Components"
        }, 
        {
            "location": "/admin/system_operations/flow/#related-links", 
            "text": "Getting Started / Quick Installation and Setup  Getting Started / Building a LeoFS' cluster with Ansible  For Administrators / System Operations / Cluster Operations", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/system_operations/s3/", 
            "text": "S3-API Related Operations\n\n\nPrior Knowledge\n\n\nWhen choosing Amazon S3 API\n1\n as LeoGateway's protocol, you need LeoFS' S3-API related operations including \nEndpoint\n, \nUser\n, and \nBucket\n operations.\n\n\nYou can access a LeoFS system whose LeoGateway's protocol is S3-API by following the flow.\n\n\n\n\nCreate an endpoint\n to be able to access a LeoFS system's resources.\n\n\nCreate a user\n to separately manage the user's buckets and objects.\n\n\nAfter creating a bucket, If you need to \nchange ACL of a bucket\n, you can set it as an administrator of a LeoFS system. The current version, v1.3 of LeoFS does not support\n\n\n\n\nOperations\n\n\nEndpoint\n\n\nYou can continue to use a LeoFS system's DNS name to access its resources after you've set up an endpoint, which has a policy that controls the use of the endpoint to access the resources.\n\n\nCreate an endpoint, \nadd-endpoint\n\n\nCreates an endpoint so users can access a LeoFS system's resources.\n\n\n1\n2\n3\n4\n5\n$ leofs-adm add-endpoint \nendpoint\n\n\n\n## Example\n\n$ leofs-adm add-endpoint leo-project.net\nOK\n\n\n\n\n\n\nRemove an endpoint, \ndelete-endpoint\n\n\nRemoves an endpoint if a LeoFS system's system does not need it.\n\n\n1\n2\n3\n4\n5\n$ leofs-adm delete-endpoint \nendpoint\n\n\n\n## Example\n\n$ leofs-adm delete-endpoint leo-project.net\nOK\n\n\n\n\n\n\nRetrieve a list of endpoints, \nget-endpoints\n\n\nRetrieves a list of the endpoints if you need to know the existing endpoints.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n$ leofs-adm get-endpoints\n\n\n## Example\n\n$ leofs-adm get-endpoints\nendpoint         \n|\n created at\n-----------------+---------------------------\nleo-project.net  \n|\n \n2017\n-04-07 \n10\n:07:31 +0900\nlocalhost        \n|\n \n2017\n-04-07 \n10\n:06:18 +0900\ns3.amazonaws.com \n|\n \n2017\n-04-07 \n10\n:06:18 +0900\n\n\n\n\n\n\nUser\n\n\nTo access a LeoFS system, its system's users need to create an account and retrieve \nAccessKeyID\n and \nSecretAccessKey\n.\n\n\nCreate a user, \ncreate-user\n\n\nCreates a user's account so its user can access a LeoFS system.\n\n\n1\n2\n3\n4\n5\n6\n$ leofs-adm create-user \nuser-id\n \npassword\n\n\n\n## Example\n\n$ leofs-adm create-user leofs-user-1 PASSWORD\naccess-key-id: b5f0413d45855fcc055e\nsecret-access-key: b74f9ae91dd0de81f71e19f8d455a7c081f34c57\n\n\n\n\n\n\nRemove a user, \ndelete-user\n\n\nRemoves a user account if a LeoFS system does not need it.\n\n\n1\n2\n3\n4\n5\n$ leofs-adm delete-user \nuser-id\n\n\n\n## Example\n\n$ leofs-adm delete-user leofs-user-1\nOK\n\n\n\n\n\n\nRetrieve a list of users, \nget-users\n\n\nRetrieves a list of the users if you need to know the existing users.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n$ leofs-adm get-users\n\n\n## Example\n\n$ leofs-adm get-users\nuser_id      \n|\n role_id \n|\n access_key_id          \n|\n created_at\n-------------+---------+------------------------+---------------------------\n_test_leofs  \n|\n \n9\n       \n|\n \n05236\n                  \n|\n \n2017\n-04-10 \n09\n:52:39 +0900\nleofs-user-2 \n|\n \n1\n       \n|\n b5f0413d45855fcc055e   \n|\n \n2017\n-04-10 \n09\n:54:35 +0900\n\n\n\n\n\n\nChange role of a user, \nupdate-user-role\n\n\nUpdates a user's role if you need to change the role of a LeoFS system. You can choose a role from \nAdministrator\n or \nGeneral user\n.\n\n\n\n\n\n\n\n\nRole Id\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n1\n\n\nAdministrator\n\n\n\n\n\n\n9\n\n\nGeneral User\n\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n$ leofs-adm update-user-role \nuser-id\n \nrole-id\n\n\n\n## Example\n\n$ leofs-adm update-user-role leofs-user-2 \n9\n\nOK\n\n\n## Confirm the latest list of the users\n\n$ leofs-adm get-users\nuser_id      \n|\n role_id \n|\n access_key_id          \n|\n created_at\n-------------+---------+------------------------+---------------------------\n_test_leofs  \n|\n \n9\n       \n|\n \n05236\n                  \n|\n \n2017\n-04-10 \n09\n:52:39 +0900\nleofs-user   \n|\n \n1\n       \n|\n 3c1d889cdd2088fb9482   \n|\n \n2017\n-04-10 \n09\n:53:33 +0900\nleofs-user-2 \n|\n \n9\n       \n|\n b5f0413d45855fcc055e   \n|\n \n2017\n-04-10 \n09\n:54:35 +0900\n\n\n\n\n\n\nBucket\n\n\nTo manage objects under a bucket, you need to create a bucket with \nleofs-adm\n or any client for Amazon S3 such as s3cmd\n2\n and DragonDisk\n3\n.\n\n\nCreate a bucket, \nadd-bucket\n\n\nCreates a bucket so a LeoFS system' users can manage resources under its bucket.\n\n\n1\n2\n3\n4\n5\n$ leofs-adm add-bucket \nbucket-name\n \naccess-key-id\n\n\n\n## Example\n\n$ leofs-adm add-bucket backup \n05236\n\nOK\n\n\n\n\n\n\nRemove a bucket, \ndelete-bucket\n\n\nRemoves a bucket if its user does not need it.\n\n\n1\n2\n3\n4\n5\n$ leofs-adm delete-bucket \nbucket-name\n \naccess-key-id\n\n\n\n## Example\n\n$ leofs-adm delete-bucket backup \n05236\n\nOK\n\n\n\n\n\n\nRetrieve a list of buckets, \nget-buckets\n\n\nRetrieves a list of the buckets if you need to know the existing buckets.\n\n\n1\n2\n3\n4\n5\n6\n7\n$ leofs-adm get-buckets\n\n\n## Example\n\n$ leofs-adm get-buckets\ncluster id   \n|\n bucket   \n|\n owner       \n|\n permissions      \n|\n redundancy method            \n|\n created at\n-------------+----------+-------------+------------------+------------------------------+---------------------------\nleofs_1      \n|\n \ntest\n     \n|\n _test_leofs \n|\n Me\n(\nfull_control\n)\n \n|\n copy, \n{\nn:1, w:1, r:1, d:1\n}\n   \n|\n \n2017\n-04-10 \n10\n:57:29 +0900\n\n\n\n\n\n\nRetrieve a bucket, \nget-bucket\n\n\nRetrieves a bucket if you need to know its bucket's information.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n$ leofs-adm get-bucket \naccess-key-id\n\n\n\n## Example\n\n$ leofs-adm get-bucket \n05236\n\nbucket   \n|\n permissions                            \n|\n created at\n---------+----------------------------------------+---------------------------\nbackup   \n|\n Me\n(\nfull_control\n)\n, Everyone\n(\nread\n)\n       \n|\n \n2017\n-04-10 \n10\n:39:01 +0900\ndocs     \n|\n Me\n(\nfull_control\n)\n, Everyone\n(\nread\n)\n       \n|\n \n2017\n-04-10 \n10\n:39:25 +0900\nlogs     \n|\n Me\n(\nfull_control\n)\n, Everyone\n(\nread,write\n)\n \n|\n \n2017\n-04-10 \n10\n:39:38 +0900\nmovie    \n|\n Me\n(\nfull_control\n)\n                       \n|\n \n2017\n-04-10 \n10\n:39:45 +0900\n\n\n\n\n\n\nChange an owner of a bucket, \nchown-bucket\n\n\nUpdates an owner of a bucket if you need to change its owner to someone else.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n$ leofs-adm chown-bucket \nbucket\n \naccess-key-id\n\n\n\n## Example\n\n$ leofs-adm chown-bucket \ntest\n b5f0413d45855fcc055e\nOK\n\n\n## Confirm the latest list of the buckets\n\n$ leofs-adm get-buckets\ncluster id   \n|\n bucket   \n|\n owner        \n|\n permissions      \n|\n redundancy method            \n|\n created at\n-------------+----------+--------------+------------------+------------------------------+---------------------------\nleofs_1      \n|\n \ntest\n     \n|\n leofs-user-2 \n|\n Me\n(\nfull_control\n)\n \n|\n copy, \n{\nn:1, w:1, r:1, d:1\n}\n   \n|\n \n2017\n-04-10 \n10\n:57:29 +0900\n\n\n\n\n\n\nUpdate ACL of a bucket, \nupdate-acl\n\n\nUpdates ACL of a bucket if you need to change it.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n$ leofs-adm update-acl \nbucket\n \naccess-key-id\n \ncanned-ACL\n\n\n\n## Example\n\n$ leofs-adm update-acl \ntest\n \n05236\n private\nok\n$ leofs-adm update-acl \ntest\n \n05236\n public-read\nok\n$ leofs-adm update-acl \ntest\n \n05236\n public-read-write\nok\n\n\n\n\n\n\nCanned ACL\n\n\nWhen using S3-API, LeoFS supports a set of predefined grants, known as canned ACLs. Each canned ACL has a predefined a set of grantees and permissions. The following table lists the set of canned ACLs and the associated predefined grants.\n\n\n\n\n\n\n\n\nCanned ACL\n\n\nApplies To\n\n\nPermissions Added To ACL\n\n\n\n\n\n\n\n\n\n\nprivate\n\n\nBucket and object\n\n\n[default]\n Owner gets FULL_CONTROL. No one else has access rights.\n\n\n\n\n\n\npublic-read\n\n\nBucket and object\n\n\nOwner gets FULL_CONTROL. The AllUsers group gets READ access.\n\n\n\n\n\n\npublic-read-write\n\n\nBucket and object\n\n\nOwner gets FULL_CONTROL. The AllUsers group gets READ and WRITE access. Granting this on a bucket is generally not recommended.\n\n\n\n\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Interface / S3-API\n\n\n\n\n\n\n\n\n\n\n\n\nAmazon S3 API\n\n\n\n\n\n\nS3cmd: A command Line S3 Client and Backup for Linux and Mac\n\n\n\n\n\n\nDragonDisk: A cloud storage client", 
            "title": "S3-API related Operations"
        }, 
        {
            "location": "/admin/system_operations/s3/#s3-api-related-operations", 
            "text": "", 
            "title": "S3-API Related Operations"
        }, 
        {
            "location": "/admin/system_operations/s3/#prior-knowledge", 
            "text": "When choosing Amazon S3 API 1  as LeoGateway's protocol, you need LeoFS' S3-API related operations including  Endpoint ,  User , and  Bucket  operations.  You can access a LeoFS system whose LeoGateway's protocol is S3-API by following the flow.   Create an endpoint  to be able to access a LeoFS system's resources.  Create a user  to separately manage the user's buckets and objects.  After creating a bucket, If you need to  change ACL of a bucket , you can set it as an administrator of a LeoFS system. The current version, v1.3 of LeoFS does not support", 
            "title": "Prior Knowledge"
        }, 
        {
            "location": "/admin/system_operations/s3/#operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/admin/system_operations/s3/#endpoint", 
            "text": "You can continue to use a LeoFS system's DNS name to access its resources after you've set up an endpoint, which has a policy that controls the use of the endpoint to access the resources.", 
            "title": "Endpoint"
        }, 
        {
            "location": "/admin/system_operations/s3/#create-an-endpoint-add-endpoint", 
            "text": "Creates an endpoint so users can access a LeoFS system's resources.  1\n2\n3\n4\n5 $ leofs-adm add-endpoint  endpoint  ## Example \n$ leofs-adm add-endpoint leo-project.net\nOK", 
            "title": "Create an endpoint, add-endpoint"
        }, 
        {
            "location": "/admin/system_operations/s3/#remove-an-endpoint-delete-endpoint", 
            "text": "Removes an endpoint if a LeoFS system's system does not need it.  1\n2\n3\n4\n5 $ leofs-adm delete-endpoint  endpoint  ## Example \n$ leofs-adm delete-endpoint leo-project.net\nOK", 
            "title": "Remove an endpoint, delete-endpoint"
        }, 
        {
            "location": "/admin/system_operations/s3/#retrieve-a-list-of-endpoints-get-endpoints", 
            "text": "Retrieves a list of the endpoints if you need to know the existing endpoints.  1\n2\n3\n4\n5\n6\n7\n8\n9 $ leofs-adm get-endpoints ## Example \n$ leofs-adm get-endpoints\nendpoint          |  created at\n-----------------+---------------------------\nleo-project.net   |   2017 -04-07  10 :07:31 +0900\nlocalhost         |   2017 -04-07  10 :06:18 +0900\ns3.amazonaws.com  |   2017 -04-07  10 :06:18 +0900", 
            "title": "Retrieve a list of endpoints, get-endpoints"
        }, 
        {
            "location": "/admin/system_operations/s3/#user", 
            "text": "To access a LeoFS system, its system's users need to create an account and retrieve  AccessKeyID  and  SecretAccessKey .", 
            "title": "User"
        }, 
        {
            "location": "/admin/system_operations/s3/#create-a-user-create-user", 
            "text": "Creates a user's account so its user can access a LeoFS system.  1\n2\n3\n4\n5\n6 $ leofs-adm create-user  user-id   password  ## Example \n$ leofs-adm create-user leofs-user-1 PASSWORD\naccess-key-id: b5f0413d45855fcc055e\nsecret-access-key: b74f9ae91dd0de81f71e19f8d455a7c081f34c57", 
            "title": "Create a user, create-user"
        }, 
        {
            "location": "/admin/system_operations/s3/#remove-a-user-delete-user", 
            "text": "Removes a user account if a LeoFS system does not need it.  1\n2\n3\n4\n5 $ leofs-adm delete-user  user-id  ## Example \n$ leofs-adm delete-user leofs-user-1\nOK", 
            "title": "Remove a user, delete-user"
        }, 
        {
            "location": "/admin/system_operations/s3/#retrieve-a-list-of-users-get-users", 
            "text": "Retrieves a list of the users if you need to know the existing users.  1\n2\n3\n4\n5\n6\n7\n8 $ leofs-adm get-users ## Example \n$ leofs-adm get-users\nuser_id       |  role_id  |  access_key_id           |  created_at\n-------------+---------+------------------------+---------------------------\n_test_leofs   |   9         |   05236                    |   2017 -04-10  09 :52:39 +0900\nleofs-user-2  |   1         |  b5f0413d45855fcc055e    |   2017 -04-10  09 :54:35 +0900", 
            "title": "Retrieve a list of users, get-users"
        }, 
        {
            "location": "/admin/system_operations/s3/#change-role-of-a-user-update-user-role", 
            "text": "Updates a user's role if you need to change the role of a LeoFS system. You can choose a role from  Administrator  or  General user .     Role Id  Description      1  Administrator    9  General User      1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 $ leofs-adm update-user-role  user-id   role-id  ## Example \n$ leofs-adm update-user-role leofs-user-2  9 \nOK ## Confirm the latest list of the users \n$ leofs-adm get-users\nuser_id       |  role_id  |  access_key_id           |  created_at\n-------------+---------+------------------------+---------------------------\n_test_leofs   |   9         |   05236                    |   2017 -04-10  09 :52:39 +0900\nleofs-user    |   1         |  3c1d889cdd2088fb9482    |   2017 -04-10  09 :53:33 +0900\nleofs-user-2  |   9         |  b5f0413d45855fcc055e    |   2017 -04-10  09 :54:35 +0900", 
            "title": "Change role of a user, update-user-role"
        }, 
        {
            "location": "/admin/system_operations/s3/#bucket", 
            "text": "To manage objects under a bucket, you need to create a bucket with  leofs-adm  or any client for Amazon S3 such as s3cmd 2  and DragonDisk 3 .", 
            "title": "Bucket"
        }, 
        {
            "location": "/admin/system_operations/s3/#create-a-bucket-add-bucket", 
            "text": "Creates a bucket so a LeoFS system' users can manage resources under its bucket.  1\n2\n3\n4\n5 $ leofs-adm add-bucket  bucket-name   access-key-id  ## Example \n$ leofs-adm add-bucket backup  05236 \nOK", 
            "title": "Create a bucket, add-bucket"
        }, 
        {
            "location": "/admin/system_operations/s3/#remove-a-bucket-delete-bucket", 
            "text": "Removes a bucket if its user does not need it.  1\n2\n3\n4\n5 $ leofs-adm delete-bucket  bucket-name   access-key-id  ## Example \n$ leofs-adm delete-bucket backup  05236 \nOK", 
            "title": "Remove a bucket, delete-bucket"
        }, 
        {
            "location": "/admin/system_operations/s3/#retrieve-a-list-of-buckets-get-buckets", 
            "text": "Retrieves a list of the buckets if you need to know the existing buckets.  1\n2\n3\n4\n5\n6\n7 $ leofs-adm get-buckets ## Example \n$ leofs-adm get-buckets\ncluster id    |  bucket    |  owner        |  permissions       |  redundancy method             |  created at\n-------------+----------+-------------+------------------+------------------------------+---------------------------\nleofs_1       |   test       |  _test_leofs  |  Me ( full_control )   |  copy,  { n:1, w:1, r:1, d:1 }     |   2017 -04-10  10 :57:29 +0900", 
            "title": "Retrieve a list of buckets, get-buckets"
        }, 
        {
            "location": "/admin/system_operations/s3/#retrieve-a-bucket-get-bucket", 
            "text": "Retrieves a bucket if you need to know its bucket's information.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 $ leofs-adm get-bucket  access-key-id  ## Example \n$ leofs-adm get-bucket  05236 \nbucket    |  permissions                             |  created at\n---------+----------------------------------------+---------------------------\nbackup    |  Me ( full_control ) , Everyone ( read )         |   2017 -04-10  10 :39:01 +0900\ndocs      |  Me ( full_control ) , Everyone ( read )         |   2017 -04-10  10 :39:25 +0900\nlogs      |  Me ( full_control ) , Everyone ( read,write )   |   2017 -04-10  10 :39:38 +0900\nmovie     |  Me ( full_control )                         |   2017 -04-10  10 :39:45 +0900", 
            "title": "Retrieve a bucket, get-bucket"
        }, 
        {
            "location": "/admin/system_operations/s3/#change-an-owner-of-a-bucket-chown-bucket", 
            "text": "Updates an owner of a bucket if you need to change its owner to someone else.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 $ leofs-adm chown-bucket  bucket   access-key-id  ## Example \n$ leofs-adm chown-bucket  test  b5f0413d45855fcc055e\nOK ## Confirm the latest list of the buckets \n$ leofs-adm get-buckets\ncluster id    |  bucket    |  owner         |  permissions       |  redundancy method             |  created at\n-------------+----------+--------------+------------------+------------------------------+---------------------------\nleofs_1       |   test       |  leofs-user-2  |  Me ( full_control )   |  copy,  { n:1, w:1, r:1, d:1 }     |   2017 -04-10  10 :57:29 +0900", 
            "title": "Change an owner of a bucket, chown-bucket"
        }, 
        {
            "location": "/admin/system_operations/s3/#update-acl-of-a-bucket-update-acl", 
            "text": "Updates ACL of a bucket if you need to change it.  1\n2\n3\n4\n5\n6\n7\n8\n9 $ leofs-adm update-acl  bucket   access-key-id   canned-ACL  ## Example \n$ leofs-adm update-acl  test   05236  private\nok\n$ leofs-adm update-acl  test   05236  public-read\nok\n$ leofs-adm update-acl  test   05236  public-read-write\nok", 
            "title": "Update ACL of a bucket, update-acl"
        }, 
        {
            "location": "/admin/system_operations/s3/#canned-acl", 
            "text": "When using S3-API, LeoFS supports a set of predefined grants, known as canned ACLs. Each canned ACL has a predefined a set of grantees and permissions. The following table lists the set of canned ACLs and the associated predefined grants.     Canned ACL  Applies To  Permissions Added To ACL      private  Bucket and object  [default]  Owner gets FULL_CONTROL. No one else has access rights.    public-read  Bucket and object  Owner gets FULL_CONTROL. The AllUsers group gets READ access.    public-read-write  Bucket and object  Owner gets FULL_CONTROL. The AllUsers group gets READ and WRITE access. Granting this on a bucket is generally not recommended.", 
            "title": "Canned ACL"
        }, 
        {
            "location": "/admin/system_operations/s3/#related-links", 
            "text": "For Administrators / Interface / S3-API       Amazon S3 API    S3cmd: A command Line S3 Client and Backup for Linux and Mac    DragonDisk: A cloud storage client", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/system_operations/cluster/", 
            "text": "Cluster Operations\n\n\nPrior Knowledge\n\n\nLeoFS provides the cluster operation features which are implemented on \nleofs-adm\n, LeoFS CLI for administration. LeoFS supports \nnode addition\n and \nnode deletion\n, and already covers as unique features of LeoFS, \nnode suspension\n, \nnode restart\n, and \nnode takeover\n. You can use those functions after starting a LeoFS system.\n\n\nOperations\n\n\nAdd a Node\n\n\nLeoFS temporally adds a node into the member table of LeoManager's database after launching a new LeoStorage node. If you decide to join it in the cluster, you need to execute \nleofs-adm rebalance\n command.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n## Example:\n\n\n## 1. Launch a new LeoStorage node\n\n\n\n## 2. Check the current state of the cluster\n\n$ leofs-adm status\n \n[\nSystem Confiuration\n]\n\n-----------------------------------+----------\n Item                              \n|\n Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version \n|\n \n1\n.3.3\n                        cluster Id \n|\n leofs_1\n                             DC Id \n|\n dc_1\n                    Total replicas \n|\n \n2\n\n          number of successes of R \n|\n \n1\n\n          number of successes of W \n|\n \n1\n\n          number of successes of D \n|\n \n1\n\n number of rack-awareness replicas \n|\n \n0\n\n                         ring size \n|\n \n2\n^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs \n|\n \n2\n\n           number of replicas a DC \n|\n \n1\n\n-----------------------------------+----------\n Manager RING \nhash\n\n-----------------------------------+----------\n                 current ring-hash \n|\n d5d667a6\n                previous ring-hash \n|\n d5d667a6\n-----------------------------------+----------\n\n \n[\nState of Node\n(\ns\n)]\n\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n \ntype\n  \n|\n           node           \n|\n    state     \n|\n  current ring  \n|\n   prev ring    \n|\n          updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S    \n|\n storage_0@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:20:19 +0900\n  S    \n|\n storage_1@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:20:19 +0900\n  S    \n|\n storage_2@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:20:19 +0900\n  S    \n|\n storage_3@127.0.0.1      \n|\n attached     \n|\n                \n|\n                \n|\n \n2017\n-04-18 \n18\n:20:37 +0900\n  G    \n|\n gateway_0@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:20:21 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n\n## 3. Execute `rebalance`\n\n$ leofs-adm rebalance\nGenerating rebalance-list...\nGenerated rebalance-list\nDistributing rebalance-list to the storage nodes\nOK  \n25\n% - storage_0@127.0.0.1\nOK  \n50\n% - storage_1@127.0.0.1\nOK  \n75\n% - storage_2@127.0.0.1\nOK \n100\n% - storage_3@127.0.0.1\nOK\n\n\n## 4. Check the latest state of cluster after rebalancing the cluster\n\n$ leofs-adm status\n \n[\nSystem Confiuration\n]\n\n-----------------------------------+----------\n Item                              \n|\n Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version \n|\n \n1\n.3.3\n                        cluster Id \n|\n leofs_1\n                             DC Id \n|\n dc_1\n                    Total replicas \n|\n \n2\n\n          number of successes of R \n|\n \n1\n\n          number of successes of W \n|\n \n1\n\n          number of successes of D \n|\n \n1\n\n number of rack-awareness replicas \n|\n \n0\n\n                         ring size \n|\n \n2\n^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs \n|\n \n2\n\n           number of replicas a DC \n|\n \n1\n\n-----------------------------------+----------\n Manager RING \nhash\n\n-----------------------------------+----------\n                 current ring-hash \n|\n ce4bece1\n                previous ring-hash \n|\n 3923d007\n-----------------------------------+----------\n\n \n[\nState of Node\n(\ns\n)]\n\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n \ntype\n  \n|\n           node           \n|\n    state     \n|\n  current ring  \n|\n   prev ring    \n|\n          updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S    \n|\n storage_0@127.0.0.1      \n|\n running      \n|\n ce4bece1       \n|\n 3923d007       \n|\n \n2017\n-04-18 \n18\n:20:19 +0900\n  S    \n|\n storage_1@127.0.0.1      \n|\n running      \n|\n ce4bece1       \n|\n 3923d007       \n|\n \n2017\n-04-18 \n18\n:20:19 +0900\n  S    \n|\n storage_2@127.0.0.1      \n|\n running      \n|\n ce4bece1       \n|\n 3923d007       \n|\n \n2017\n-04-18 \n18\n:20:19 +0900\n  S    \n|\n storage_3@127.0.0.1      \n|\n running      \n|\n ce4bece1       \n|\n 3923d007       \n|\n \n2017\n-04-18 \n18\n:21:25 +0900\n  G    \n|\n gateway_0@127.0.0.1      \n|\n running      \n|\n ce4bece1       \n|\n 3923d007       \n|\n \n2017\n-04-18 \n18\n:20:21 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n\n\n\n\n\nRemove a Node\n\n\nIf you need to shrink a target LeoFS' cluster size, you can realize that by following the operation flow.\n\n\n\n\nDecide to remove a LeoStorage node, whose state must be \nrunning\n or \nstop\n\n\nThen execute \nleofs-adm detach\n command\n\n\nFinally, execute \nleofs-adm rebalance\n command to start rebalancing data in the cluster\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n## Example:\n\n\n## 1. Check the current state of the cluster\n\n$ leofs-adm status\n \n[\nSystem Confiuration\n]\n\n-----------------------------------+----------\n Item                              \n|\n Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version \n|\n \n1\n.3.3\n                        cluster Id \n|\n leofs_1\n                             DC Id \n|\n dc_1\n                    Total replicas \n|\n \n2\n\n          number of successes of R \n|\n \n1\n\n          number of successes of W \n|\n \n1\n\n          number of successes of D \n|\n \n1\n\n number of rack-awareness replicas \n|\n \n0\n\n                         ring size \n|\n \n2\n^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs \n|\n \n2\n\n           number of replicas a DC \n|\n \n1\n\n-----------------------------------+----------\n Manager RING \nhash\n\n-----------------------------------+----------\n                 current ring-hash \n|\n 3923d007\n                previous ring-hash \n|\n 3923d007\n-----------------------------------+----------\n\n \n[\nState of Node\n(\ns\n)]\n\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n \ntype\n  \n|\n           node           \n|\n    state     \n|\n  current ring  \n|\n   prev ring    \n|\n          updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S    \n|\n storage_0@127.0.0.1      \n|\n running      \n|\n 3923d007       \n|\n 3923d007       \n|\n \n2017\n-04-18 \n18\n:31:37 +0900\n  S    \n|\n storage_1@127.0.0.1      \n|\n running      \n|\n 3923d007       \n|\n 3923d007       \n|\n \n2017\n-04-18 \n18\n:31:37 +0900\n  S    \n|\n storage_2@127.0.0.1      \n|\n running      \n|\n 3923d007       \n|\n 3923d007       \n|\n \n2017\n-04-18 \n18\n:31:37 +0900\n  S    \n|\n storage_3@127.0.0.1      \n|\n running      \n|\n 3923d007       \n|\n 3923d007       \n|\n \n2017\n-04-18 \n18\n:31:37 +0900\n  G    \n|\n gateway_0@127.0.0.1      \n|\n running      \n|\n 3923d007       \n|\n 3923d007       \n|\n \n2017\n-04-18 \n18\n:31:55 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n\n## 2. Remove a LeoStorage node\n\n$ leofs-adm detach storage_3@127.0.0.1\nOK\n\n\n## 3. Execute `rebalance`\n\n$ leofs-adm rebalance\nGenerating rebalance-list...\nGenerated rebalance-list\nDistributing rebalance-list to the storage nodes\nOK  \n33\n% - storage_0@127.0.0.1\nOK  \n67\n% - storage_1@127.0.0.1\nOK \n100\n% - storage_2@127.0.0.1\nOK\n\n\n## 3. Check the latest state of cluster after rebalancing the cluster\n\n$ leofs-adm status\n \n[\nSystem Confiuration\n]\n\n-----------------------------------+----------\n Item                              \n|\n Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version \n|\n \n1\n.3.3\n                        cluster Id \n|\n leofs_1\n                             DC Id \n|\n dc_1\n                    Total replicas \n|\n \n2\n\n          number of successes of R \n|\n \n1\n\n          number of successes of W \n|\n \n1\n\n          number of successes of D \n|\n \n1\n\n number of rack-awareness replicas \n|\n \n0\n\n                         ring size \n|\n \n2\n^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs \n|\n \n2\n\n           number of replicas a DC \n|\n \n1\n\n-----------------------------------+----------\n Manager RING \nhash\n\n-----------------------------------+----------\n                 current ring-hash \n|\n d5d667a6\n                previous ring-hash \n|\n d5d667a6\n-----------------------------------+----------\n\n \n[\nState of Node\n(\ns\n)]\n\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n \ntype\n  \n|\n           node           \n|\n    state     \n|\n  current ring  \n|\n   prev ring    \n|\n          updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S    \n|\n storage_0@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:31:37 +0900\n  S    \n|\n storage_1@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:31:37 +0900\n  S    \n|\n storage_2@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:31:37 +0900\n  G    \n|\n gateway_0@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:31:55 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n\n\n\n\n\nTake Over a Node\n\n\nIf a new LeoStorage node takes over a detached node, you can realize that by following the operation flow.\n\n\n\n\nExecute \nleofs-adm detach\n command to remove a target node in the cluster\n\n\nThen launch a new node to take over the detached node\n\n\nFinally, execute \nleofs-adm reebalance\n command to start rebalancing data in the cluster\n\n\n\n\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n 11\n 12\n 13\n 14\n 15\n 16\n 17\n 18\n 19\n 20\n 21\n 22\n 23\n 24\n 25\n 26\n 27\n 28\n 29\n 30\n 31\n 32\n 33\n 34\n 35\n 36\n 37\n 38\n 39\n 40\n 41\n 42\n 43\n 44\n 45\n 46\n 47\n 48\n 49\n 50\n 51\n 52\n 53\n 54\n 55\n 56\n 57\n 58\n 59\n 60\n 61\n 62\n 63\n 64\n 65\n 66\n 67\n 68\n 69\n 70\n 71\n 72\n 73\n 74\n 75\n 76\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n## Example:\n\n\n## 1. Check the current state of the cluster (1)\n\n$ leofs-adm status\n \n[\nSystem Confiuration\n]\n\n-----------------------------------+----------\n Item                              \n|\n Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version \n|\n \n1\n.3.3\n                        cluster Id \n|\n leofs_1\n                             DC Id \n|\n dc_1\n                    Total replicas \n|\n \n2\n\n          number of successes of R \n|\n \n1\n\n          number of successes of W \n|\n \n1\n\n          number of successes of D \n|\n \n1\n\n number of rack-awareness replicas \n|\n \n0\n\n                         ring size \n|\n \n2\n^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs \n|\n \n2\n\n           number of replicas a DC \n|\n \n1\n\n-----------------------------------+----------\n Manager RING \nhash\n\n-----------------------------------+----------\n                 current ring-hash \n|\n d5d667a6\n                previous ring-hash \n|\n d5d667a6\n-----------------------------------+----------\n\n \n[\nState of Node\n(\ns\n)]\n\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n \ntype\n  \n|\n           node           \n|\n    state     \n|\n  current ring  \n|\n   prev ring    \n|\n          updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S    \n|\n storage_0@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:55:35 +0900\n  S    \n|\n storage_1@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:55:35 +0900\n  S    \n|\n storage_2@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:55:35 +0900\n  G    \n|\n gateway_0@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:55:37 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n\n## 2. Remove a LeoStorage node\n\n$ leofs-adm detach storage_0@127.0.0.1\nOK\n\n\n## 3. Launch a new LeoStorage node\n\n\n\n## 4. Check the current state of the cluster(2)\n\n\n$ leofs-adm status\n \n[\nSystem Confiuration\n]\n\n-----------------------------------+----------\n Item                              \n|\n Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version \n|\n \n1\n.3.3\n                        cluster Id \n|\n leofs_1\n                             DC Id \n|\n dc_1\n                    Total replicas \n|\n \n2\n\n          number of successes of R \n|\n \n1\n\n          number of successes of W \n|\n \n1\n\n          number of successes of D \n|\n \n1\n\n number of rack-awareness replicas \n|\n \n0\n\n                         ring size \n|\n \n2\n^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs \n|\n \n2\n\n           number of replicas a DC \n|\n \n1\n\n-----------------------------------+----------\n Manager RING \nhash\n\n-----------------------------------+----------\n                 current ring-hash \n|\n d5d667a6\n                previous ring-hash \n|\n d5d667a6\n-----------------------------------+----------\n\n \n[\nState of Node\n(\ns\n)]\n\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n \ntype\n  \n|\n           node           \n|\n    state     \n|\n  current ring  \n|\n   prev ring    \n|\n          updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S    \n|\n storage_0@127.0.0.1      \n|\n detached     \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:56:32 +0900\n  S    \n|\n storage_1@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:55:35 +0900\n  S    \n|\n storage_2@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:55:35 +0900\n  S    \n|\n storage_3@127.0.0.1      \n|\n attached     \n|\n                \n|\n                \n|\n \n2017\n-04-18 \n18\n:56:47 +0900\n  G    \n|\n gateway_0@127.0.0.1      \n|\n running      \n|\n d5d667a6       \n|\n d5d667a6       \n|\n \n2017\n-04-18 \n18\n:55:37 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n\n## 5. Execute `rebalance`\n\n$ leofs-adm rebalance\nGenerating rebalance-list...\nGenerated rebalance-list\nDistributing rebalance-list to the storage nodes\nOK  \n33\n% - storage_2@127.0.0.1\nOK  \n67\n% - storage_3@127.0.0.1\nOK \n100\n% - storage_1@127.0.0.1\nOK\n\n\n## 6. Check the latest state of the cluster\n\n$ leofs-adm status\n \n[\nSystem Confiuration\n]\n\n-----------------------------------+----------\n Item                              \n|\n Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version \n|\n \n1\n.3.3\n                        cluster Id \n|\n leofs_1\n                             DC Id \n|\n dc_1\n                    Total replicas \n|\n \n2\n\n          number of successes of R \n|\n \n1\n\n          number of successes of W \n|\n \n1\n\n          number of successes of D \n|\n \n1\n\n number of rack-awareness replicas \n|\n \n0\n\n                         ring size \n|\n \n2\n^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs \n|\n \n2\n\n           number of replicas a DC \n|\n \n1\n\n-----------------------------------+----------\n Manager RING \nhash\n\n-----------------------------------+----------\n                 current ring-hash \n|\n c613a468\n                previous ring-hash \n|\n c613a468\n-----------------------------------+----------\n\n \n[\nState of Node\n(\ns\n)]\n\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n \ntype\n  \n|\n           node           \n|\n    state     \n|\n  current ring  \n|\n   prev ring    \n|\n          updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S    \n|\n storage_1@127.0.0.1      \n|\n running      \n|\n c613a468       \n|\n c613a468       \n|\n \n2017\n-04-18 \n18\n:55:35 +0900\n  S    \n|\n storage_2@127.0.0.1      \n|\n running      \n|\n c613a468       \n|\n c613a468       \n|\n \n2017\n-04-18 \n18\n:55:35 +0900\n  S    \n|\n storage_3@127.0.0.1      \n|\n running      \n|\n c613a468       \n|\n c613a468       \n|\n \n2017\n-04-18 \n18\n:58:16 +0900\n  G    \n|\n gateway_0@127.0.0.1      \n|\n running      \n|\n c613a468       \n|\n c613a468       \n|\n \n2017\n-04-18 \n18\n:55:37 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n\n\n\n\n\nSuspend a Node\n\n\nWhen maintenance of a node is necessary, you can suspend a target node temporally. A suspended node does not receive requests from LeoGateway nodes and LeoStorage nodes. LeoFS eventually distributes the state of the cluster to every node.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n## Example:\n\n\n## 1. Execute `suspend`\n\n$ leofs-adm \nsuspend\n storage_1@127.0.0.1\nOK\n\n\n\n## 2. Check the latest state of the cluster\n\n$ leofs-adm status\n \n[\nSystem Confiuration\n]\n\n-----------------------------------+----------\n Item                              \n|\n Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version \n|\n \n1\n.3.3\n                        cluster Id \n|\n leofs_1\n                             DC Id \n|\n dc_1\n                    Total replicas \n|\n \n2\n\n          number of successes of R \n|\n \n1\n\n          number of successes of W \n|\n \n1\n\n          number of successes of D \n|\n \n1\n\n number of rack-awareness replicas \n|\n \n0\n\n                         ring size \n|\n \n2\n^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs \n|\n \n2\n\n           number of replicas a DC \n|\n \n1\n\n-----------------------------------+----------\n Manager RING \nhash\n\n-----------------------------------+----------\n                 current ring-hash \n|\n c613a468\n                previous ring-hash \n|\n c613a468\n-----------------------------------+----------\n\n \n[\nState of Node\n(\ns\n)]\n\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n \ntype\n  \n|\n           node           \n|\n    state     \n|\n  current ring  \n|\n   prev ring    \n|\n          updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S    \n|\n storage_1@127.0.0.1      \n|\n \nsuspend\n      \n|\n c613a468       \n|\n c613a468       \n|\n \n2017\n-04-18 \n18\n:55:35 +0900\n  S    \n|\n storage_2@127.0.0.1      \n|\n running      \n|\n c613a468       \n|\n c613a468       \n|\n \n2017\n-04-18 \n18\n:55:35 +0900\n  S    \n|\n storage_3@127.0.0.1      \n|\n running      \n|\n c613a468       \n|\n c613a468       \n|\n \n2017\n-04-18 \n18\n:58:16 +0900\n  G    \n|\n gateway_0@127.0.0.1      \n|\n running      \n|\n c613a468       \n|\n c613a468       \n|\n \n2017\n-04-18 \n18\n:55:37 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n\n\n\n\n\nResume a Node\n\n\nAfter suspending a node, if its node restarts and rejoins the cluster, execute \nleofs-adm resume\n command.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n## Example:\n\n\n## 1. Execute `resume`\n\n$ leofs-adm resume storage_1@127.0.0.1\nOK\n\n\n## 2. Check the latest state of the cluster\n\n$ leofs-adm status\n \n[\nSystem Confiuration\n]\n\n-----------------------------------+----------\n Item                              \n|\n Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version \n|\n \n1\n.3.3\n                        cluster Id \n|\n leofs_1\n                             DC Id \n|\n dc_1\n                    Total replicas \n|\n \n2\n\n          number of successes of R \n|\n \n1\n\n          number of successes of W \n|\n \n1\n\n          number of successes of D \n|\n \n1\n\n number of rack-awareness replicas \n|\n \n0\n\n                         ring size \n|\n \n2\n^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs \n|\n \n2\n\n           number of replicas a DC \n|\n \n1\n\n-----------------------------------+----------\n Manager RING \nhash\n\n-----------------------------------+----------\n                 current ring-hash \n|\n c613a468\n                previous ring-hash \n|\n c613a468\n-----------------------------------+----------\n\n \n[\nState of Node\n(\ns\n)]\n\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n \ntype\n  \n|\n           node           \n|\n    state     \n|\n  current ring  \n|\n   prev ring    \n|\n          updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S    \n|\n storage_1@127.0.0.1      \n|\n running      \n|\n c613a468       \n|\n c613a468       \n|\n \n2017\n-04-18 \n19\n:01:48 +0900\n  S    \n|\n storage_2@127.0.0.1      \n|\n running      \n|\n c613a468       \n|\n c613a468       \n|\n \n2017\n-04-18 \n18\n:55:35 +0900\n  S    \n|\n storage_3@127.0.0.1      \n|\n running      \n|\n c613a468       \n|\n c613a468       \n|\n \n2017\n-04-18 \n18\n:58:16 +0900\n  G    \n|\n gateway_0@127.0.0.1      \n|\n running      \n|\n c613a468       \n|\n c613a468       \n|\n \n2017\n-04-18 \n18\n:55:37 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n\n\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Settings / Cluster Settings", 
            "title": "Cluster Operations"
        }, 
        {
            "location": "/admin/system_operations/cluster/#cluster-operations", 
            "text": "", 
            "title": "Cluster Operations"
        }, 
        {
            "location": "/admin/system_operations/cluster/#prior-knowledge", 
            "text": "LeoFS provides the cluster operation features which are implemented on  leofs-adm , LeoFS CLI for administration. LeoFS supports  node addition  and  node deletion , and already covers as unique features of LeoFS,  node suspension ,  node restart , and  node takeover . You can use those functions after starting a LeoFS system.", 
            "title": "Prior Knowledge"
        }, 
        {
            "location": "/admin/system_operations/cluster/#operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/admin/system_operations/cluster/#add-a-node", 
            "text": "LeoFS temporally adds a node into the member table of LeoManager's database after launching a new LeoStorage node. If you decide to join it in the cluster, you need to execute  leofs-adm rebalance  command.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93 ## Example:  ## 1. Launch a new LeoStorage node  ## 2. Check the current state of the cluster \n$ leofs-adm status\n  [ System Confiuration ] \n-----------------------------------+----------\n Item                               |  Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version  |   1 .3.3\n                        cluster Id  |  leofs_1\n                             DC Id  |  dc_1\n                    Total replicas  |   2 \n          number of successes of R  |   1 \n          number of successes of W  |   1 \n          number of successes of D  |   1 \n number of rack-awareness replicas  |   0 \n                         ring size  |   2 ^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs  |   2 \n           number of replicas a DC  |   1 \n-----------------------------------+----------\n Manager RING  hash \n-----------------------------------+----------\n                 current ring-hash  |  d5d667a6\n                previous ring-hash  |  d5d667a6\n-----------------------------------+----------\n\n  [ State of Node ( s )] \n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  type    |            node            |     state      |   current ring   |    prev ring     |           updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S     |  storage_0@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :20:19 +0900\n  S     |  storage_1@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :20:19 +0900\n  S     |  storage_2@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :20:19 +0900\n  S     |  storage_3@127.0.0.1       |  attached      |                  |                  |   2017 -04-18  18 :20:37 +0900\n  G     |  gateway_0@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :20:21 +0900\n-------+--------------------------+--------------+----------------+----------------+---------------------------- ## 3. Execute `rebalance` \n$ leofs-adm rebalance\nGenerating rebalance-list...\nGenerated rebalance-list\nDistributing rebalance-list to the storage nodes\nOK   25 % - storage_0@127.0.0.1\nOK   50 % - storage_1@127.0.0.1\nOK   75 % - storage_2@127.0.0.1\nOK  100 % - storage_3@127.0.0.1\nOK ## 4. Check the latest state of cluster after rebalancing the cluster \n$ leofs-adm status\n  [ System Confiuration ] \n-----------------------------------+----------\n Item                               |  Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version  |   1 .3.3\n                        cluster Id  |  leofs_1\n                             DC Id  |  dc_1\n                    Total replicas  |   2 \n          number of successes of R  |   1 \n          number of successes of W  |   1 \n          number of successes of D  |   1 \n number of rack-awareness replicas  |   0 \n                         ring size  |   2 ^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs  |   2 \n           number of replicas a DC  |   1 \n-----------------------------------+----------\n Manager RING  hash \n-----------------------------------+----------\n                 current ring-hash  |  ce4bece1\n                previous ring-hash  |  3923d007\n-----------------------------------+----------\n\n  [ State of Node ( s )] \n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  type    |            node            |     state      |   current ring   |    prev ring     |           updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S     |  storage_0@127.0.0.1       |  running       |  ce4bece1        |  3923d007        |   2017 -04-18  18 :20:19 +0900\n  S     |  storage_1@127.0.0.1       |  running       |  ce4bece1        |  3923d007        |   2017 -04-18  18 :20:19 +0900\n  S     |  storage_2@127.0.0.1       |  running       |  ce4bece1        |  3923d007        |   2017 -04-18  18 :20:19 +0900\n  S     |  storage_3@127.0.0.1       |  running       |  ce4bece1        |  3923d007        |   2017 -04-18  18 :21:25 +0900\n  G     |  gateway_0@127.0.0.1       |  running       |  ce4bece1        |  3923d007        |   2017 -04-18  18 :20:21 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------", 
            "title": "Add a Node"
        }, 
        {
            "location": "/admin/system_operations/cluster/#remove-a-node", 
            "text": "If you need to shrink a target LeoFS' cluster size, you can realize that by following the operation flow.   Decide to remove a LeoStorage node, whose state must be  running  or  stop  Then execute  leofs-adm detach  command  Finally, execute  leofs-adm rebalance  command to start rebalancing data in the cluster    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93 ## Example:  ## 1. Check the current state of the cluster \n$ leofs-adm status\n  [ System Confiuration ] \n-----------------------------------+----------\n Item                               |  Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version  |   1 .3.3\n                        cluster Id  |  leofs_1\n                             DC Id  |  dc_1\n                    Total replicas  |   2 \n          number of successes of R  |   1 \n          number of successes of W  |   1 \n          number of successes of D  |   1 \n number of rack-awareness replicas  |   0 \n                         ring size  |   2 ^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs  |   2 \n           number of replicas a DC  |   1 \n-----------------------------------+----------\n Manager RING  hash \n-----------------------------------+----------\n                 current ring-hash  |  3923d007\n                previous ring-hash  |  3923d007\n-----------------------------------+----------\n\n  [ State of Node ( s )] \n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  type    |            node            |     state      |   current ring   |    prev ring     |           updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S     |  storage_0@127.0.0.1       |  running       |  3923d007        |  3923d007        |   2017 -04-18  18 :31:37 +0900\n  S     |  storage_1@127.0.0.1       |  running       |  3923d007        |  3923d007        |   2017 -04-18  18 :31:37 +0900\n  S     |  storage_2@127.0.0.1       |  running       |  3923d007        |  3923d007        |   2017 -04-18  18 :31:37 +0900\n  S     |  storage_3@127.0.0.1       |  running       |  3923d007        |  3923d007        |   2017 -04-18  18 :31:37 +0900\n  G     |  gateway_0@127.0.0.1       |  running       |  3923d007        |  3923d007        |   2017 -04-18  18 :31:55 +0900\n-------+--------------------------+--------------+----------------+----------------+---------------------------- ## 2. Remove a LeoStorage node \n$ leofs-adm detach storage_3@127.0.0.1\nOK ## 3. Execute `rebalance` \n$ leofs-adm rebalance\nGenerating rebalance-list...\nGenerated rebalance-list\nDistributing rebalance-list to the storage nodes\nOK   33 % - storage_0@127.0.0.1\nOK   67 % - storage_1@127.0.0.1\nOK  100 % - storage_2@127.0.0.1\nOK ## 3. Check the latest state of cluster after rebalancing the cluster \n$ leofs-adm status\n  [ System Confiuration ] \n-----------------------------------+----------\n Item                               |  Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version  |   1 .3.3\n                        cluster Id  |  leofs_1\n                             DC Id  |  dc_1\n                    Total replicas  |   2 \n          number of successes of R  |   1 \n          number of successes of W  |   1 \n          number of successes of D  |   1 \n number of rack-awareness replicas  |   0 \n                         ring size  |   2 ^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs  |   2 \n           number of replicas a DC  |   1 \n-----------------------------------+----------\n Manager RING  hash \n-----------------------------------+----------\n                 current ring-hash  |  d5d667a6\n                previous ring-hash  |  d5d667a6\n-----------------------------------+----------\n\n  [ State of Node ( s )] \n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  type    |            node            |     state      |   current ring   |    prev ring     |           updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S     |  storage_0@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :31:37 +0900\n  S     |  storage_1@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :31:37 +0900\n  S     |  storage_2@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :31:37 +0900\n  G     |  gateway_0@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :31:55 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------", 
            "title": "Remove a Node"
        }, 
        {
            "location": "/admin/system_operations/cluster/#take-over-a-node", 
            "text": "If a new LeoStorage node takes over a detached node, you can realize that by following the operation flow.   Execute  leofs-adm detach  command to remove a target node in the cluster  Then launch a new node to take over the detached node  Finally, execute  leofs-adm reebalance  command to start rebalancing data in the cluster     1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n 11\n 12\n 13\n 14\n 15\n 16\n 17\n 18\n 19\n 20\n 21\n 22\n 23\n 24\n 25\n 26\n 27\n 28\n 29\n 30\n 31\n 32\n 33\n 34\n 35\n 36\n 37\n 38\n 39\n 40\n 41\n 42\n 43\n 44\n 45\n 46\n 47\n 48\n 49\n 50\n 51\n 52\n 53\n 54\n 55\n 56\n 57\n 58\n 59\n 60\n 61\n 62\n 63\n 64\n 65\n 66\n 67\n 68\n 69\n 70\n 71\n 72\n 73\n 74\n 75\n 76\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135 ## Example:  ## 1. Check the current state of the cluster (1) \n$ leofs-adm status\n  [ System Confiuration ] \n-----------------------------------+----------\n Item                               |  Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version  |   1 .3.3\n                        cluster Id  |  leofs_1\n                             DC Id  |  dc_1\n                    Total replicas  |   2 \n          number of successes of R  |   1 \n          number of successes of W  |   1 \n          number of successes of D  |   1 \n number of rack-awareness replicas  |   0 \n                         ring size  |   2 ^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs  |   2 \n           number of replicas a DC  |   1 \n-----------------------------------+----------\n Manager RING  hash \n-----------------------------------+----------\n                 current ring-hash  |  d5d667a6\n                previous ring-hash  |  d5d667a6\n-----------------------------------+----------\n\n  [ State of Node ( s )] \n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  type    |            node            |     state      |   current ring   |    prev ring     |           updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S     |  storage_0@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :55:35 +0900\n  S     |  storage_1@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :55:35 +0900\n  S     |  storage_2@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :55:35 +0900\n  G     |  gateway_0@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :55:37 +0900\n-------+--------------------------+--------------+----------------+----------------+---------------------------- ## 2. Remove a LeoStorage node \n$ leofs-adm detach storage_0@127.0.0.1\nOK ## 3. Launch a new LeoStorage node  ## 4. Check the current state of the cluster(2) \n\n$ leofs-adm status\n  [ System Confiuration ] \n-----------------------------------+----------\n Item                               |  Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version  |   1 .3.3\n                        cluster Id  |  leofs_1\n                             DC Id  |  dc_1\n                    Total replicas  |   2 \n          number of successes of R  |   1 \n          number of successes of W  |   1 \n          number of successes of D  |   1 \n number of rack-awareness replicas  |   0 \n                         ring size  |   2 ^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs  |   2 \n           number of replicas a DC  |   1 \n-----------------------------------+----------\n Manager RING  hash \n-----------------------------------+----------\n                 current ring-hash  |  d5d667a6\n                previous ring-hash  |  d5d667a6\n-----------------------------------+----------\n\n  [ State of Node ( s )] \n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  type    |            node            |     state      |   current ring   |    prev ring     |           updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S     |  storage_0@127.0.0.1       |  detached      |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :56:32 +0900\n  S     |  storage_1@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :55:35 +0900\n  S     |  storage_2@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :55:35 +0900\n  S     |  storage_3@127.0.0.1       |  attached      |                  |                  |   2017 -04-18  18 :56:47 +0900\n  G     |  gateway_0@127.0.0.1       |  running       |  d5d667a6        |  d5d667a6        |   2017 -04-18  18 :55:37 +0900\n-------+--------------------------+--------------+----------------+----------------+---------------------------- ## 5. Execute `rebalance` \n$ leofs-adm rebalance\nGenerating rebalance-list...\nGenerated rebalance-list\nDistributing rebalance-list to the storage nodes\nOK   33 % - storage_2@127.0.0.1\nOK   67 % - storage_3@127.0.0.1\nOK  100 % - storage_1@127.0.0.1\nOK ## 6. Check the latest state of the cluster \n$ leofs-adm status\n  [ System Confiuration ] \n-----------------------------------+----------\n Item                               |  Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version  |   1 .3.3\n                        cluster Id  |  leofs_1\n                             DC Id  |  dc_1\n                    Total replicas  |   2 \n          number of successes of R  |   1 \n          number of successes of W  |   1 \n          number of successes of D  |   1 \n number of rack-awareness replicas  |   0 \n                         ring size  |   2 ^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs  |   2 \n           number of replicas a DC  |   1 \n-----------------------------------+----------\n Manager RING  hash \n-----------------------------------+----------\n                 current ring-hash  |  c613a468\n                previous ring-hash  |  c613a468\n-----------------------------------+----------\n\n  [ State of Node ( s )] \n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  type    |            node            |     state      |   current ring   |    prev ring     |           updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S     |  storage_1@127.0.0.1       |  running       |  c613a468        |  c613a468        |   2017 -04-18  18 :55:35 +0900\n  S     |  storage_2@127.0.0.1       |  running       |  c613a468        |  c613a468        |   2017 -04-18  18 :55:35 +0900\n  S     |  storage_3@127.0.0.1       |  running       |  c613a468        |  c613a468        |   2017 -04-18  18 :58:16 +0900\n  G     |  gateway_0@127.0.0.1       |  running       |  c613a468        |  c613a468        |   2017 -04-18  18 :55:37 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------", 
            "title": "Take Over a Node"
        }, 
        {
            "location": "/admin/system_operations/cluster/#suspend-a-node", 
            "text": "When maintenance of a node is necessary, you can suspend a target node temporally. A suspended node does not receive requests from LeoGateway nodes and LeoStorage nodes. LeoFS eventually distributes the state of the cluster to every node.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44 ## Example:  ## 1. Execute `suspend` \n$ leofs-adm  suspend  storage_1@127.0.0.1\nOK ## 2. Check the latest state of the cluster \n$ leofs-adm status\n  [ System Confiuration ] \n-----------------------------------+----------\n Item                               |  Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version  |   1 .3.3\n                        cluster Id  |  leofs_1\n                             DC Id  |  dc_1\n                    Total replicas  |   2 \n          number of successes of R  |   1 \n          number of successes of W  |   1 \n          number of successes of D  |   1 \n number of rack-awareness replicas  |   0 \n                         ring size  |   2 ^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs  |   2 \n           number of replicas a DC  |   1 \n-----------------------------------+----------\n Manager RING  hash \n-----------------------------------+----------\n                 current ring-hash  |  c613a468\n                previous ring-hash  |  c613a468\n-----------------------------------+----------\n\n  [ State of Node ( s )] \n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  type    |            node            |     state      |   current ring   |    prev ring     |           updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S     |  storage_1@127.0.0.1       |   suspend        |  c613a468        |  c613a468        |   2017 -04-18  18 :55:35 +0900\n  S     |  storage_2@127.0.0.1       |  running       |  c613a468        |  c613a468        |   2017 -04-18  18 :55:35 +0900\n  S     |  storage_3@127.0.0.1       |  running       |  c613a468        |  c613a468        |   2017 -04-18  18 :58:16 +0900\n  G     |  gateway_0@127.0.0.1       |  running       |  c613a468        |  c613a468        |   2017 -04-18  18 :55:37 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------", 
            "title": "Suspend a Node"
        }, 
        {
            "location": "/admin/system_operations/cluster/#resume-a-node", 
            "text": "After suspending a node, if its node restarts and rejoins the cluster, execute  leofs-adm resume  command.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43 ## Example:  ## 1. Execute `resume` \n$ leofs-adm resume storage_1@127.0.0.1\nOK ## 2. Check the latest state of the cluster \n$ leofs-adm status\n  [ System Confiuration ] \n-----------------------------------+----------\n Item                               |  Value\n-----------------------------------+----------\n Basic/Consistency level\n-----------------------------------+----------\n                    system version  |   1 .3.3\n                        cluster Id  |  leofs_1\n                             DC Id  |  dc_1\n                    Total replicas  |   2 \n          number of successes of R  |   1 \n          number of successes of W  |   1 \n          number of successes of D  |   1 \n number of rack-awareness replicas  |   0 \n                         ring size  |   2 ^128\n-----------------------------------+----------\n Multi DC replication settings\n-----------------------------------+----------\n        max number of joinable DCs  |   2 \n           number of replicas a DC  |   1 \n-----------------------------------+----------\n Manager RING  hash \n-----------------------------------+----------\n                 current ring-hash  |  c613a468\n                previous ring-hash  |  c613a468\n-----------------------------------+----------\n\n  [ State of Node ( s )] \n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  type    |            node            |     state      |   current ring   |    prev ring     |           updated at\n-------+--------------------------+--------------+----------------+----------------+----------------------------\n  S     |  storage_1@127.0.0.1       |  running       |  c613a468        |  c613a468        |   2017 -04-18  19 :01:48 +0900\n  S     |  storage_2@127.0.0.1       |  running       |  c613a468        |  c613a468        |   2017 -04-18  18 :55:35 +0900\n  S     |  storage_3@127.0.0.1       |  running       |  c613a468        |  c613a468        |   2017 -04-18  18 :58:16 +0900\n  G     |  gateway_0@127.0.0.1       |  running       |  c613a468        |  c613a468        |   2017 -04-18  18 :55:37 +0900\n-------+--------------------------+--------------+----------------+----------------+----------------------------", 
            "title": "Resume a Node"
        }, 
        {
            "location": "/admin/system_operations/cluster/#related-links", 
            "text": "For Administrators / Settings / Cluster Settings", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/system_operations/data/", 
            "text": "Data Operations\n\n\nThis section provides information to operate data stored in LeoFS as well as the prior knowledge that gives you the better understanding about what will happen behind the scene when you invoke a command for a data operation.\n\n\nStorage Architecture\n\n\nA brief introduction how LeoFS organize data into actual OS files.\n\n\nAppend-Only for Content\n\n\nOnce a PUT/DELETE remote procedure call (\nRPC\n) arrives on LeoStorage appends new blocks including the object information such as the key, data itself and also various associated metadata to the end of a file.\n\n\nThis \nAppend-Only-File\n we call \nAVS, Aria Vector Storage\n which is referenced when retrieving the data through \nGET RPCs\n. With its inherent nature of the \nAppend-Only-File\n, a data-compaction process is needed to clean up the orphaned space in an AVS.\n\n\nEmbedded KVS for Metadata\n\n\nAfter having succeeded in appending new blocks to an AVS, then leo_object_storage updates a corresponding \nKVS\n file with the key and its associated metadata \n(same information stored in the AVS except for the data itself)\n and its offset where the new blocks stored in AVS as a value. This KVS file is referenced when retrieving the data/metadata through GET/HEAD RPCs.\n\n\nMessage Queue for Async Operations\n\n\nSome data can be stored into a \nQueue\n for processing later in the case\n\n\n\n\nA PUT/DELETE operation failed\n\n\nA Multi DC Replication (\nMDCR\n) failed\n\n\nrebalance/recover-(file|node|cluster)\n invoked through leofs-adm\n\n\n\n\nFile Structure\n\n\nMultiple AVS/KVS pairs can be placed on one node to enable LeoFS handling as much use cases and hardware requirements as possible. See \nConcept and Architecture / LeoStorage's Architecture - Data Structure\n.\n\n\n\n\nContainer : AVS/KVS pair = 1 : N\n\n\nMultiple AVS/KVS pairs can be stored under one OS directory (We call it \nContainer\n).\n\n\n'N' can be specified through \nleo_storage.conf\n.\n\n\nHow to choose optimal 'N'\n\n\nAs a data-compaction is executed per AVS/KVS pair, at least the size of a AVS/KVS pair is needed to run data-compaction so that the larger 'N', the less disk space LeoFS uses for data-compaction.\n\n\nHowever the larger N, the more disk seeks LeoFS suffers.\n\n\nTha said, the optimal N is determined by setting the largest value that doesn't affect the online throughput you would expect.\n\n\n\n\n\n\n\n\n\n\nNode : Container = 1 : N\n\n\nEach Container can be stored under a different OS directory.\n\n\nN can be specified through leo_storage.conf.\n\n\nSetting \nN \n 1\n can be useful when there are multiple JBOD disks on the node. The one JBOD disk array can be map to the one container.\n\n\n\n\n\n\n\n\nData Compaction\n\n\nThis section provides information about what/how data-compaction can affect the online performance.\n\n\n\n\nParallelism\n\n\nA data-compaction can be executed across multiple AVS/KVS pairs in parallel.\n\n\nThe number of data-compaction processes can be specified through an argument of leofs-adm.\n\n\nIncreasing the number can be useful when the load coming from online is relatively low and want a data-compaction process to boost as much as possible.\n\n\nBe careful that too much number can make a data-compaction process slow down.\n\n\n\n\n\n\nConcurrent with any operation coming from online.\n\n\nGET/HEAD never be blocked by a data-compaction.\n\n\nPUT/DELETE can be blocked while a data-compaction is processing the tail part of an AVS.\n\n\nGiven that the above limitation, We would recommend suspending a node you are supposed to run a data-compaction if a LeoFS system's cluster handles write intensive workload.\n\n\n\n\n\n\n\n\nHow To Operate Data Compaction\n\n\nAs described in the previous section, A compaction process is needed to remove logically deleted objects and its corresponding metadata.\n\n\nState Transition\n\n\n\n\nCommands\n\n\nCommands related to Compaction as well as Disk Usage.\n\n\n\n\n\n\n\n\nShell\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCompaction Commands\n\n\n\n\n\n\n\n\nleofs-adm compact-start \nstorage-node\n (all/\nnum-of-targets\n) [\nnum-of-compaction-proc\n]\n\n\nStart Compaction (Transfer its state to \nrunning\n).\nnum-of-targets\n: How many AVS/KVS pairs are compacted.\nnum-of-compaction-pro\n: How many processes are run in parallel.\n\n\n\n\n\n\nleofs-adm compact-suspend \nstorage-node\n\n\nSuspend Compaction \n(Transfer its state to 'suspend' from running)\n.\n\n\n\n\n\n\nleofs-adm compact-resume \nstorage-node\n\n\nResume Compaction \n(Transfer its state to 'running' from suspend)\n.\n\n\n\n\n\n\nleofs-adm compact-status \nstorage-node\n\n\nSee the Current Compaction Status.\n\n\n\n\n\n\nleofs-adm diagnose-start \nstorage-node\n\n\nStart Diagnose (Not actually doing Compaction but scanning all AVS/KVS pairs and reporting what objects/metadatas exist as a file).\n\n\n\n\n\n\nDisk Usage\n\n\n\n\n\n\n\n\nleofs-adm du \nstorage-node\n\n\nSee the Current Disk Usage.\n\n\n\n\n\n\nleofs-adm du detail \nstorage-node\n\n\nSee the Current Disk Usage in detail.\n\n\n\n\n\n\n\n\ncompact-start\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n## Example:\n\n\n## All AVS/KVS pairs on storage_0@127.0.0.1 will be compacted with 3 concurrent processes\n\n\n## (default concurrency is 3)\n\n$ leofs-adm compact-start storage_0@127.0.0.1 all\nOK\n\n\n## 5 AVS/KVS pairs on storage_0@127.0.0.1 will be compacted with 2 concurrent processes\n\n$ leofs-adm compact-start storage_0@127.0.0.1 \n5\n \n2\n\nOK\n\n\n\n\n\n\ncompact-suspend\n\n\n1\n2\n3\n## Example:\n\n$ leofs-adm compact-suspend storage_0@127.0.0.1\nOK\n\n\n\n\n\n\ncompact-resume\n\n\n1\n2\n3\n## Example:\n\n$ leofs-adm compact-resume storage_0@127.0.0.1\nOK\n\n\n\n\n\n\ncompact-status\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n## Example:\n\n$ leofs-adm compact-status storage_0@127.0.0.1\n        current status: running\n last compaction start: \n2013\n-03-04 \n12\n:39:47 +0900\n         total targets: \n64\n\n  \n# of pending targets: 5\n\n  \n# of ongoing targets: 3\n\n  \n# of out of targets : 56\n\n\n\n\n\n\n\ndiagnose-start\n\n\nSee also \ndiagnosis-log format\n to understand the output log format.\n\n\n1\n2\n3\n## Example:\n\n$ leofs-adm diagnose-start storage_0@127.0.0.1\nOK\n\n\n\n\n\n\ndu\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n## Example:\n\n$ leofs-adm du storage_0@127.0.0.1\n active number of objects: \n19968\n\n  total number of objects: \n39936\n\n   active size of objects: \n198256974\n.0\n    total size of objects: \n254725020\n.0\n     ratio of active size: \n77\n.83%\n    last compaction start: \n2013\n-03-04 \n12\n:39:47 +0900\n      last compaction end: \n2013\n-03-04 \n12\n:39:55 +0900\n\n\n\n\n\n\ndu detail\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n## Example:\n\n$ leofs-adm du detail storage_0@127.0.0.1\n\n[\ndu\n(\nstorage stats\n)]\n\n                file path: /home/leofs/dev/leofs/package/leofs/storage/avs/object/0.avs\n active number of objects: \n320\n\n  total number of objects: \n640\n\n   active size of objects: \n3206378\n.0\n    total size of objects: \n4082036\n.0\n     ratio of active size: \n78\n.55%\n    last compaction start: \n2013\n-03-04 \n12\n:39:47 +0900\n      last compaction end: \n2013\n-03-04 \n12\n:39:55 +0900\n.\n.\n.\n                file path: /home/leofs/dev/leofs/package/leofs/storage/avs/object/63.avs\n active number of objects: \n293\n\n  total number of objects: \n586\n\n   active size of objects: \n2968909\n.0\n    total size of objects: \n3737690\n.0\n     ratio of active size: \n79\n.43%\n    last compaction start: ____-__-__ __:__:__\n      last compaction end: ____-__-__ __:__:__\n\n\n\n\n\n\nDiagnosis\n\n\nThis section explains the file format generated by \nleofs-adm diagnose-start\n in detail.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n## Example:\n------+------------------------------------------+------------------------------------------------------------+-----------+------------+------------------+--------------------------+----\nOffset| RING\ns address-id                        | Filename                                                   | Child num | File Size  | Unixtime         | Localtime                |del?\n------+------------------------------------------+------------------------------------------------------------+-----------+------------+------------------+--------------------------+----\n194     296754181484029444656944009564610621293   photo/leo_redundant_manager/Makefile                             0       2034        1413348050768344   2014-10-15 13:40:50 +0900   0\n2400    185993533055981727582172380494809056426   photo/leo_redundant_manager/ebin/leo_redundant_manager.beam      0       24396       1413348050869454   2014-10-15 13:40:50 +0900   0\n38446   53208912738248114804281793572563205919    photo/leo_rpc/.git/refs/remotes/origin/HEAD                      0       33          1413348057441546   2014-10-15 13:40:57 +0900   0\n38658   57520977797167422772945547576980778561    photo/leo_rpc/ebin/leo_rpc_client_utils.beam                     0       2576        1413348057512261   2014-10-15 13:40:57 +0900   0\n69506   187294034498591995039607573685274229706   photo/leo_backend_db/src/leo_backend_db_server.erl               0       13911       1413348068031188   2014-10-15 13:41:08 +0900   0\n83603   316467020376888598364250682951088839795   photo/leo_backend_db/test/leo_backend_db_api_prop.erl            0       3507        1413348068052219   2014-10-15 13:41:08 +0900   1\n\n\n\n\n\n\nThe file is formatted as Tab Separated Values \n(TSV)\n except headers \n(head three lines of a file)\n. The detail of each column are described below:\n\n\n\n\n\n\n\n\nColumn Number\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n1\n\n\nbyte-wise Offset where the object is located in an AVS.\n\n\n\n\n\n\n2\n\n\nAddress ID on RING (Distribute Hash Routing Table).\n\n\n\n\n\n\n3\n\n\nFile Name.\n\n\n\n\n\n\n4\n\n\nThe Number of Children in a File.\n\n\n\n\n\n\n5\n\n\nFile Size in bytes.\n\n\n\n\n\n\n6\n\n\nTimestamp in Unix Time.\n\n\n\n\n\n\n7\n\n\nTimestamp in Local Time.\n\n\n\n\n\n\n8\n\n\nFlag (0/1) representing whether the object is removed.\n\n\n\n\n\n\n\n\nRecover Objects\n\n\nThis section provides information about the recovery commands that can be used in order to recover inconsistencies in a LeoFS cluster according to failures.\n\n\nCommands\n\n\n\n\n\n\n\n\nShell\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nleofs-adm recover-file \\\nfile-path>\n\n\nRecover an inconsistent object specified by the file-path.\n\n\n\n\n\n\nleofs-adm recover-node \\\nstorage-node>\n\n\nRecover all inconsistent objects in the specified storage-node.\n\n\n\n\n\n\nleofs-adm recover-cluster \\\ncluster-id>\n\n\nRecover all inconsistent objects in the specified cluster-id.\n\n\n\n\n\n\n\n\nrecover-file\n\n\n1\n2\n3\n## Example:\n\n$ leofs-adm recover-file leo/fast/storage.key\nOK\n\n\n\n\n\n\nrecover-node\n\n\n1\n2\n3\n## Example:\n\n$ leofs-adm recover-node storage_0@127.0.0.1\nOK\n\n\n\n\n\n\n\n\nrecover-cluster\n\n\n\n\n1\n2\n3\n## Example:\n\n$ leofs-adm recover-cluster cluster-1\nOK\n\n\n\n\n\n\nUse Cases\n\n\nWhen/How to use recover commands.\n\n\n\n\nAVS/KVS Broken\n\n\nInvoke \nrecover-node\n with a node having broken AVS/KVS files.\n\n\n\n\n\n\nQueue Broken\n\n\nInvoke \nrecover-node\n with every node except which having broken Queue files.\n\n\nThe procedure might be improved in future when \nissue#618\n solved.\n\n\n\n\n\n\nDisk Broken\n\n\nInvoke \nsuspend\n with a node having broken Disk arrays and subsequently run \nleo_storage stop\n.\n\n\nExchange broken Disk arrays.\n\n\nRun \nleo_storage start\n and subsequently Invoke \nresume\n with the node.\n\n\nInvoke \nrecover-node\n with the node.\n\n\n\n\n\n\nNode Broken\n\n\nInvoke \ndetach\n with a broken node.\n\n\nPrepare a new node that will take over all objects assigned to a detached node.\n\n\nInvoke \nrebalance\n.\n\n\n\n\n\n\nSource/Destination Cluster Down\n\n\nInvoke \nrecover-cluster\n with a downed cluster.\n\n\n\n\n\n\nSource/Destination Cluster Down and delete operations on the other side got lost (compacted).\n\n\nSet up the cluster from scratch\n\n\ninvoke \nrecover-cluster\n with the new cluster\n\n\nSee also \nissue#636\n for more information.\n\n\n\n\n\n\n\n\nRelated Links\n\n\n\n\nConcept and Architecture / LeoStorage's Architecture", 
            "title": "Data Operations"
        }, 
        {
            "location": "/admin/system_operations/data/#data-operations", 
            "text": "This section provides information to operate data stored in LeoFS as well as the prior knowledge that gives you the better understanding about what will happen behind the scene when you invoke a command for a data operation.", 
            "title": "Data Operations"
        }, 
        {
            "location": "/admin/system_operations/data/#storage-architecture", 
            "text": "A brief introduction how LeoFS organize data into actual OS files.", 
            "title": "Storage Architecture"
        }, 
        {
            "location": "/admin/system_operations/data/#append-only-for-content", 
            "text": "Once a PUT/DELETE remote procedure call ( RPC ) arrives on LeoStorage appends new blocks including the object information such as the key, data itself and also various associated metadata to the end of a file.  This  Append-Only-File  we call  AVS, Aria Vector Storage  which is referenced when retrieving the data through  GET RPCs . With its inherent nature of the  Append-Only-File , a data-compaction process is needed to clean up the orphaned space in an AVS.", 
            "title": "Append-Only for Content"
        }, 
        {
            "location": "/admin/system_operations/data/#embedded-kvs-for-metadata", 
            "text": "After having succeeded in appending new blocks to an AVS, then leo_object_storage updates a corresponding  KVS  file with the key and its associated metadata  (same information stored in the AVS except for the data itself)  and its offset where the new blocks stored in AVS as a value. This KVS file is referenced when retrieving the data/metadata through GET/HEAD RPCs.", 
            "title": "Embedded KVS for Metadata"
        }, 
        {
            "location": "/admin/system_operations/data/#message-queue-for-async-operations", 
            "text": "Some data can be stored into a  Queue  for processing later in the case   A PUT/DELETE operation failed  A Multi DC Replication ( MDCR ) failed  rebalance/recover-(file|node|cluster)  invoked through leofs-adm", 
            "title": "Message Queue for Async Operations"
        }, 
        {
            "location": "/admin/system_operations/data/#file-structure", 
            "text": "Multiple AVS/KVS pairs can be placed on one node to enable LeoFS handling as much use cases and hardware requirements as possible. See  Concept and Architecture / LeoStorage's Architecture - Data Structure .   Container : AVS/KVS pair = 1 : N  Multiple AVS/KVS pairs can be stored under one OS directory (We call it  Container ).  'N' can be specified through  leo_storage.conf .  How to choose optimal 'N'  As a data-compaction is executed per AVS/KVS pair, at least the size of a AVS/KVS pair is needed to run data-compaction so that the larger 'N', the less disk space LeoFS uses for data-compaction.  However the larger N, the more disk seeks LeoFS suffers.  Tha said, the optimal N is determined by setting the largest value that doesn't affect the online throughput you would expect.      Node : Container = 1 : N  Each Container can be stored under a different OS directory.  N can be specified through leo_storage.conf.  Setting  N   1  can be useful when there are multiple JBOD disks on the node. The one JBOD disk array can be map to the one container.", 
            "title": "File Structure"
        }, 
        {
            "location": "/admin/system_operations/data/#data-compaction", 
            "text": "This section provides information about what/how data-compaction can affect the online performance.   Parallelism  A data-compaction can be executed across multiple AVS/KVS pairs in parallel.  The number of data-compaction processes can be specified through an argument of leofs-adm.  Increasing the number can be useful when the load coming from online is relatively low and want a data-compaction process to boost as much as possible.  Be careful that too much number can make a data-compaction process slow down.    Concurrent with any operation coming from online.  GET/HEAD never be blocked by a data-compaction.  PUT/DELETE can be blocked while a data-compaction is processing the tail part of an AVS.  Given that the above limitation, We would recommend suspending a node you are supposed to run a data-compaction if a LeoFS system's cluster handles write intensive workload.", 
            "title": "Data Compaction"
        }, 
        {
            "location": "/admin/system_operations/data/#how-to-operate-data-compaction", 
            "text": "As described in the previous section, A compaction process is needed to remove logically deleted objects and its corresponding metadata.", 
            "title": "How To Operate Data Compaction"
        }, 
        {
            "location": "/admin/system_operations/data/#state-transition", 
            "text": "", 
            "title": "State Transition"
        }, 
        {
            "location": "/admin/system_operations/data/#commands", 
            "text": "Commands related to Compaction as well as Disk Usage.     Shell  Description      Compaction Commands     leofs-adm compact-start  storage-node  (all/ num-of-targets ) [ num-of-compaction-proc ]  Start Compaction (Transfer its state to  running ). num-of-targets : How many AVS/KVS pairs are compacted. num-of-compaction-pro : How many processes are run in parallel.    leofs-adm compact-suspend  storage-node  Suspend Compaction  (Transfer its state to 'suspend' from running) .    leofs-adm compact-resume  storage-node  Resume Compaction  (Transfer its state to 'running' from suspend) .    leofs-adm compact-status  storage-node  See the Current Compaction Status.    leofs-adm diagnose-start  storage-node  Start Diagnose (Not actually doing Compaction but scanning all AVS/KVS pairs and reporting what objects/metadatas exist as a file).    Disk Usage     leofs-adm du  storage-node  See the Current Disk Usage.    leofs-adm du detail  storage-node  See the Current Disk Usage in detail.", 
            "title": "Commands"
        }, 
        {
            "location": "/admin/system_operations/data/#compact-start", 
            "text": "1\n2\n3\n4\n5\n6\n7\n8\n9 ## Example:  ## All AVS/KVS pairs on storage_0@127.0.0.1 will be compacted with 3 concurrent processes  ## (default concurrency is 3) \n$ leofs-adm compact-start storage_0@127.0.0.1 all\nOK ## 5 AVS/KVS pairs on storage_0@127.0.0.1 will be compacted with 2 concurrent processes \n$ leofs-adm compact-start storage_0@127.0.0.1  5   2 \nOK", 
            "title": "compact-start"
        }, 
        {
            "location": "/admin/system_operations/data/#compact-suspend", 
            "text": "1\n2\n3 ## Example: \n$ leofs-adm compact-suspend storage_0@127.0.0.1\nOK", 
            "title": "compact-suspend"
        }, 
        {
            "location": "/admin/system_operations/data/#compact-resume", 
            "text": "1\n2\n3 ## Example: \n$ leofs-adm compact-resume storage_0@127.0.0.1\nOK", 
            "title": "compact-resume"
        }, 
        {
            "location": "/admin/system_operations/data/#compact-status", 
            "text": "1\n2\n3\n4\n5\n6\n7\n8 ## Example: \n$ leofs-adm compact-status storage_0@127.0.0.1\n        current status: running\n last compaction start:  2013 -03-04  12 :39:47 +0900\n         total targets:  64 \n   # of pending targets: 5 \n   # of ongoing targets: 3 \n   # of out of targets : 56", 
            "title": "compact-status"
        }, 
        {
            "location": "/admin/system_operations/data/#diagnose-start", 
            "text": "See also  diagnosis-log format  to understand the output log format.  1\n2\n3 ## Example: \n$ leofs-adm diagnose-start storage_0@127.0.0.1\nOK", 
            "title": "diagnose-start"
        }, 
        {
            "location": "/admin/system_operations/data/#du", 
            "text": "1\n2\n3\n4\n5\n6\n7\n8\n9 ## Example: \n$ leofs-adm du storage_0@127.0.0.1\n active number of objects:  19968 \n  total number of objects:  39936 \n   active size of objects:  198256974 .0\n    total size of objects:  254725020 .0\n     ratio of active size:  77 .83%\n    last compaction start:  2013 -03-04  12 :39:47 +0900\n      last compaction end:  2013 -03-04  12 :39:55 +0900", 
            "title": "du"
        }, 
        {
            "location": "/admin/system_operations/data/#du-detail", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22 ## Example: \n$ leofs-adm du detail storage_0@127.0.0.1 [ du ( storage stats )] \n                file path: /home/leofs/dev/leofs/package/leofs/storage/avs/object/0.avs\n active number of objects:  320 \n  total number of objects:  640 \n   active size of objects:  3206378 .0\n    total size of objects:  4082036 .0\n     ratio of active size:  78 .55%\n    last compaction start:  2013 -03-04  12 :39:47 +0900\n      last compaction end:  2013 -03-04  12 :39:55 +0900\n.\n.\n.\n                file path: /home/leofs/dev/leofs/package/leofs/storage/avs/object/63.avs\n active number of objects:  293 \n  total number of objects:  586 \n   active size of objects:  2968909 .0\n    total size of objects:  3737690 .0\n     ratio of active size:  79 .43%\n    last compaction start: ____-__-__ __:__:__\n      last compaction end: ____-__-__ __:__:__", 
            "title": "du detail"
        }, 
        {
            "location": "/admin/system_operations/data/#diagnosis", 
            "text": "This section explains the file format generated by  leofs-adm diagnose-start  in detail.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 ## Example:\n------+------------------------------------------+------------------------------------------------------------+-----------+------------+------------------+--------------------------+----\nOffset| RING s address-id                        | Filename                                                   | Child num | File Size  | Unixtime         | Localtime                |del?\n------+------------------------------------------+------------------------------------------------------------+-----------+------------+------------------+--------------------------+----\n194     296754181484029444656944009564610621293   photo/leo_redundant_manager/Makefile                             0       2034        1413348050768344   2014-10-15 13:40:50 +0900   0\n2400    185993533055981727582172380494809056426   photo/leo_redundant_manager/ebin/leo_redundant_manager.beam      0       24396       1413348050869454   2014-10-15 13:40:50 +0900   0\n38446   53208912738248114804281793572563205919    photo/leo_rpc/.git/refs/remotes/origin/HEAD                      0       33          1413348057441546   2014-10-15 13:40:57 +0900   0\n38658   57520977797167422772945547576980778561    photo/leo_rpc/ebin/leo_rpc_client_utils.beam                     0       2576        1413348057512261   2014-10-15 13:40:57 +0900   0\n69506   187294034498591995039607573685274229706   photo/leo_backend_db/src/leo_backend_db_server.erl               0       13911       1413348068031188   2014-10-15 13:41:08 +0900   0\n83603   316467020376888598364250682951088839795   photo/leo_backend_db/test/leo_backend_db_api_prop.erl            0       3507        1413348068052219   2014-10-15 13:41:08 +0900   1   The file is formatted as Tab Separated Values  (TSV)  except headers  (head three lines of a file) . The detail of each column are described below:     Column Number  Description      1  byte-wise Offset where the object is located in an AVS.    2  Address ID on RING (Distribute Hash Routing Table).    3  File Name.    4  The Number of Children in a File.    5  File Size in bytes.    6  Timestamp in Unix Time.    7  Timestamp in Local Time.    8  Flag (0/1) representing whether the object is removed.", 
            "title": "Diagnosis"
        }, 
        {
            "location": "/admin/system_operations/data/#recover-objects", 
            "text": "This section provides information about the recovery commands that can be used in order to recover inconsistencies in a LeoFS cluster according to failures.", 
            "title": "Recover Objects"
        }, 
        {
            "location": "/admin/system_operations/data/#commands_1", 
            "text": "Shell  Description      leofs-adm recover-file \\ file-path>  Recover an inconsistent object specified by the file-path.    leofs-adm recover-node \\ storage-node>  Recover all inconsistent objects in the specified storage-node.    leofs-adm recover-cluster \\ cluster-id>  Recover all inconsistent objects in the specified cluster-id.", 
            "title": "Commands"
        }, 
        {
            "location": "/admin/system_operations/data/#recover-file", 
            "text": "1\n2\n3 ## Example: \n$ leofs-adm recover-file leo/fast/storage.key\nOK", 
            "title": "recover-file"
        }, 
        {
            "location": "/admin/system_operations/data/#recover-node", 
            "text": "1\n2\n3 ## Example: \n$ leofs-adm recover-node storage_0@127.0.0.1\nOK    recover-cluster   1\n2\n3 ## Example: \n$ leofs-adm recover-cluster cluster-1\nOK", 
            "title": "recover-node"
        }, 
        {
            "location": "/admin/system_operations/data/#use-cases", 
            "text": "When/How to use recover commands.   AVS/KVS Broken  Invoke  recover-node  with a node having broken AVS/KVS files.    Queue Broken  Invoke  recover-node  with every node except which having broken Queue files.  The procedure might be improved in future when  issue#618  solved.    Disk Broken  Invoke  suspend  with a node having broken Disk arrays and subsequently run  leo_storage stop .  Exchange broken Disk arrays.  Run  leo_storage start  and subsequently Invoke  resume  with the node.  Invoke  recover-node  with the node.    Node Broken  Invoke  detach  with a broken node.  Prepare a new node that will take over all objects assigned to a detached node.  Invoke  rebalance .    Source/Destination Cluster Down  Invoke  recover-cluster  with a downed cluster.    Source/Destination Cluster Down and delete operations on the other side got lost (compacted).  Set up the cluster from scratch  invoke  recover-cluster  with the new cluster  See also  issue#636  for more information.", 
            "title": "Use Cases"
        }, 
        {
            "location": "/admin/system_operations/data/#related-links", 
            "text": "Concept and Architecture / LeoStorage's Architecture", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/system_operations/multi_dc_replication/", 
            "text": "Multi Data Center Replication\n\n\nConfiguration\n\n\nLeoFS provides the multi data center replication related configuration items, which contain \nleo_manager_0.conf\n. Modify those configuration items to work its feature correctly.\n\n\nLeoManager Master's Configuration For MDCR\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nmdc_replication.num_of_replicas_a_dc\n\n\nA remote cluster of a LeoFS system which receives this cluster's objects, and then replicates them, which adhere to a replication method of each object\n\n\n\n\n\n\nmdc_replication.consistency.write\n\n\nA number of replicas needed for a successful WRITE-operation\n\n\n\n\n\n\nmdc_replication.consistency.read\n\n\nA number of replicas needed for a successful READ-operation\n\n\n\n\n\n\nmdc_replication.consistency.delete\n\n\nA number of replicas needed for a successful DELETE-operation\n\n\n\n\n\n\n\n\nHow To Operate Multi Data Center Replication\n\n\nCommands\n\n\nThere are three commands, \njoin-cluster\n, \nremove-cluster\n, and \ncluster-status\n. You can control the multi data center repliation by LeoFS' CUI, \nleofs-adm\n.\n\n\n\n\n\n\n\n\nCommand\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\njoin-cluster \nRMM\n \nRMS\n\n\nBegin to communicate between the local cluster and the remote cluster\n\n\n\n\n\n\nremove-cluster \nRMM\n \nRMS\n\n\nTerminate to communicate between the local cluster and the remote cluster\n\n\n\n\n\n\ncluster-status\n\n\nSee the current state of cluster(s)\n\n\n\n\n\n\n\n\n\n\nRMM\n: A Remote cluster's LeoMnager-Master\n\n\nRMS\n: A Remote cluster's LeoManager-Slave\n\n\n\n\njoin-cluster\n\n\n1\n2\n$ leofs-adm join-cluster manager_10@127.0.0.1:13095 manager_11@127.0.0.1:13096\nOK\n\n\n\n\n\n\nremove-cluster\n\n\n1\n2\n$ leofs-adm remove-cluster manager_10@127.0.0.1:13095 manager_11@127.0.0.1:13096\nOK\n\n\n\n\n\n\ncluster-status\n\n\n1\n2\n3\n4\n$ leofs-adm cluster-status\ncluster id |   dc id    |    status    | # of storages  |          updated at\n-----------+------------+--------------+----------------+-----------------------------\nleofs_2    | dc_2       | running      |              3 | 2017-04-26 10:23:59 +0900\n\n\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Settings / LeoManager Settings", 
            "title": "Multi Data Center Replication"
        }, 
        {
            "location": "/admin/system_operations/multi_dc_replication/#multi-data-center-replication", 
            "text": "", 
            "title": "Multi Data Center Replication"
        }, 
        {
            "location": "/admin/system_operations/multi_dc_replication/#configuration", 
            "text": "LeoFS provides the multi data center replication related configuration items, which contain  leo_manager_0.conf . Modify those configuration items to work its feature correctly.", 
            "title": "Configuration"
        }, 
        {
            "location": "/admin/system_operations/multi_dc_replication/#leomanager-masters-configuration-for-mdcr", 
            "text": "Item  Description      mdc_replication.num_of_replicas_a_dc  A remote cluster of a LeoFS system which receives this cluster's objects, and then replicates them, which adhere to a replication method of each object    mdc_replication.consistency.write  A number of replicas needed for a successful WRITE-operation    mdc_replication.consistency.read  A number of replicas needed for a successful READ-operation    mdc_replication.consistency.delete  A number of replicas needed for a successful DELETE-operation", 
            "title": "LeoManager Master's Configuration For MDCR"
        }, 
        {
            "location": "/admin/system_operations/multi_dc_replication/#how-to-operate-multi-data-center-replication", 
            "text": "", 
            "title": "How To Operate Multi Data Center Replication"
        }, 
        {
            "location": "/admin/system_operations/multi_dc_replication/#commands", 
            "text": "There are three commands,  join-cluster ,  remove-cluster , and  cluster-status . You can control the multi data center repliation by LeoFS' CUI,  leofs-adm .     Command  Description      join-cluster  RMM   RMS  Begin to communicate between the local cluster and the remote cluster    remove-cluster  RMM   RMS  Terminate to communicate between the local cluster and the remote cluster    cluster-status  See the current state of cluster(s)      RMM : A Remote cluster's LeoMnager-Master  RMS : A Remote cluster's LeoManager-Slave", 
            "title": "Commands"
        }, 
        {
            "location": "/admin/system_operations/multi_dc_replication/#join-cluster", 
            "text": "1\n2 $ leofs-adm join-cluster manager_10@127.0.0.1:13095 manager_11@127.0.0.1:13096\nOK", 
            "title": "join-cluster"
        }, 
        {
            "location": "/admin/system_operations/multi_dc_replication/#remove-cluster", 
            "text": "1\n2 $ leofs-adm remove-cluster manager_10@127.0.0.1:13095 manager_11@127.0.0.1:13096\nOK", 
            "title": "remove-cluster"
        }, 
        {
            "location": "/admin/system_operations/multi_dc_replication/#cluster-status", 
            "text": "1\n2\n3\n4 $ leofs-adm cluster-status\ncluster id |   dc id    |    status    | # of storages  |          updated at\n-----------+------------+--------------+----------------+-----------------------------\nleofs_2    | dc_2       | running      |              3 | 2017-04-26 10:23:59 +0900", 
            "title": "cluster-status"
        }, 
        {
            "location": "/admin/system_operations/multi_dc_replication/#related-links", 
            "text": "For Administrators / Settings / LeoManager Settings", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/system_admin/log_management/", 
            "text": "Log Management\n\n\nLeoGateway Access-log\n\n\nOverview\n\n\nLeoGateway provides \naccess-log\n output feature so that you can investigate requests from users.\n\n\nHow To Output Access-log\n\n\nModify \naccess-log\n configuration item which include \nleo_gateway.conf\n. After starting LeoGateway node(s), those nodes output \naccess-log\n into the local disk, which is under each LeoGateway\u2019s log directory. See more detail \nFor Administrators / Settings / LeoGateway Settings\n.\n\n\nAccess-log Configuration\n\n\n1\n2\n## Is enable access-log [true, false]\n\n\nlog.is_enable_access_log\n \n=\n \ntrue\n\n\n\n\n\n\n\nLeoGateway's Log Files\n\n\n\n\n\n\n\n\nLog File\n\n\nFile Name\n\n\n\n\n\n\n\n\n\n\naccess-log\n\n\n/log/app/access.*\n\n\n\n\n\n\nerror-log\n\n\n/log/app/error.*\n\n\n\n\n\n\ninfo-log\n\n\n/log/app/info.*\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nlog/\n\u251c\u2500\u2500 [ 272 Apr 28 14:13]  app\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [  98 Apr 28 14:13]  access -\n /leofs/package/leo_gateway/log/app/access.20170428.14.1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [7.4K Apr 28 14:13]  access.20170428.14.1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [  97 Apr 28 14:13]  error -\n /leofs/package/leo_gateway/log/app/error.20170428.14.1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [   0 Apr 28 14:13]  error.20170428.14.1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [  96 Apr 28 14:13]  info -\n /leofs/package/leo_gateway/log/app/info.20170428.14.1\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 [5.4K Apr 28 14:13]  info.20170428.14.1\n\n\n\n\n\n\nExample\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n--------+-------+--------------------+----------+-------+---------------------------------------+-----------------------+----------\nMethod  | Bucket| Path               |Child Num |  Size | Timestamp                             | Unixtime              | Response\n--------+-------+--------------------+----------|-------+---------------------------------------+-----------------------+----------\n[HEAD]   photo   photo/1              0          0       2013-10-18 13:28:56.148269 +0900        1381206536148320        500\n[HEAD]   photo   photo/1              0          0       2013-10-18 13:28:56.465670 +0900        1381206536465735        404\n[HEAD]   photo   photo/city/tokyo.png 0          0       2013-10-18 13:28:56.489234 +0900        1381206536489289        200\n[GET]    photo   photo/1              0          1024    2013-10-18 13:28:56.518631 +0900        1381206536518693        500\n[GET]    photo   photo/city/paris.png 0          2048    2013-10-18 13:28:56.550376 +0900        1381206536550444        404\n[PUT]    logs    logs/leofs           1          5242880 2013-10-18 13:28:56.518631 +0900        1381206536518693        500\n[PUT]    logs    logs/leofs           2          5242880 2013-10-18 13:28:56.518631 +0900        1381206536518693        500\n[PUT]    logs    logs/leofs           3          5120    2013-10-18 13:28:56.518631 +0900        1381206536518693        500\n\n\n\n\n\n\nFormat\n\n\nAn access-log's format is TSV, \nTab Separated Values\n.\n\n\n\n\n\n\n\n\nColumn Number\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n1\n\n\nMethod: [HEAD\n\n\n\n\n\n\n2\n\n\nBucket\n\n\n\n\n\n\n3\n\n\nFilename (including path)\n\n\n\n\n\n\n4\n\n\nChild number of a file\n\n\n\n\n\n\n5\n\n\nFile Size (byte)\n\n\n\n\n\n\n6\n\n\nTimestamp with timezone\n\n\n\n\n\n\n7\n\n\nUnixtime (including micro-second)\n\n\n\n\n\n\n8\n\n\nResponse (HTTP Status Code)\n\n\n\n\n\n\n\n\nLeoStorage Data Diagnosis-log\n\n\nOverview\n\n\nLeoStorage provides \ndiagnosis-log\n output feature so that you can investigate a LeoStorage's data. If you would like to turn on its feature, you need to modify \nleo_storage.conf\n which contain LeoStorage's package.\n\n\nHow To Output Diagnosis-log\n\n\nModify \ndiagnosis-log\n configuration item which include \nleo_storage.conf\n. After executing the \ndiagnose-start\n command, a specified node outputs \ndiagnosis-log\n into the local disk, which is under each LeoStorage\u2019s AVS directory. See more detail \nFor Administrators / Settings / LeoStorage Settings\n.\n\n\nExecute the \ndiagnose-start\n command with a specified LeoStroage node, then its LeoStorage output diagnosis-log(s) into the local disk, which is under its LeoStorage\u2019s log directory. You can configure its log directory. See more detail \nFor Administrators / System Operations / Data Operations - Diagnosis\n and \nFor Administrators / Settings / LeoStroage Settings\n.\n\n\n1\n$ leofs-adm diagnose-start \nstorage-node\n\n\n\n\n\n\n\nDiagnosis-log Configuration\n\n\n1\n2\n## Output data-diagnosis log\n\n\nlog.is_enable_diagnosis_log\n \n=\n \ntrue\n\n\n\n\n\n\n\nLeoStorage's Log Files\n\n\n\n\n\n\n\n\nLog File\n\n\nFile Name\n\n\n\n\n\n\n\n\n\n\naccess-log\n\n\n/log/app/access.*\n\n\n\n\n\n\ndiagnosis-log\n\n\n/avs/log/leo_object_storage_\n.\n\n\n\n\n\n\nerror-log\n\n\n/log/app/error.*\n\n\n\n\n\n\ninfo-log\n\n\n/log/app/info.*\n\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n## diagnosis-log files\n/leofs/package/leo_storage/avs/log/\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_0 -\n /leofs/package/leo_storage/avs/log/leo_object_storage_0.20170428.14.2\n\u251c\u2500\u2500 [ 683 Apr 28 14:40]  leo_object_storage_0.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_0.20170428.14.2\n\u251c\u2500\u2500 [ 366 Apr 28 14:40]  leo_object_storage_0.report.63660577222\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_1 -\n /leofs/package/leo_storage/avs/log/leo_object_storage_1.20170428.14.2\n\u251c\u2500\u2500 [ 779 Apr 28 14:40]  leo_object_storage_1.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_1.20170428.14.2\n\u251c\u2500\u2500 [ 366 Apr 28 14:40]  leo_object_storage_1.report.63660577222\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_2 -\n /leofs/package/leo_storage/avs/log/leo_object_storage_2.20170428.14.2\n\u251c\u2500\u2500 [ 786 Apr 28 14:40]  leo_object_storage_2.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_2.20170428.14.2\n\u251c\u2500\u2500 [ 368 Apr 28 14:40]  leo_object_storage_2.report.63660577223\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_3 -\n /leofs/package/leo_storage/avs/log/leo_object_storage_3.20170428.14.2\n\u251c\u2500\u2500 [1.3K Apr 28 14:40]  leo_object_storage_3.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_3.20170428.14.2\n\u251c\u2500\u2500 [ 366 Apr 28 14:40]  leo_object_storage_3.report.63660577223\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_4 -\n /leofs/package/leo_storage/avs/log/leo_object_storage_4.20170428.14.2\n\u251c\u2500\u2500 [1.3K Apr 28 14:40]  leo_object_storage_4.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_4.20170428.14.2\n\u251c\u2500\u2500 [ 368 Apr 28 14:40]  leo_object_storage_4.report.63660577224\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_5 -\n /leofs/package/leo_storage/avs/log/leo_object_storage_5.20170428.14.2\n\u251c\u2500\u2500 [ 943 Apr 28 14:40]  leo_object_storage_5.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_5.20170428.14.2\n\u251c\u2500\u2500 [ 366 Apr 28 14:40]  leo_object_storage_5.report.63660577225\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_6 -\n /leofs/package/leo_storage/avs/log/leo_object_storage_6.20170428.14.2\n\u251c\u2500\u2500 [1.5K Apr 28 14:40]  leo_object_storage_6.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_6.20170428.14.2\n\u251c\u2500\u2500 [ 368 Apr 28 14:40]  leo_object_storage_6.report.63660577225\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_7 -\n /leofs/package/leo_storage/avs/log/leo_object_storage_7.20170428.14.2\n\u251c\u2500\u2500 [1.1K Apr 28 14:40]  leo_object_storage_7.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_7.20170428.14.2\n\u2514\u2500\u2500 [ 368 Apr 28 14:40]  leo_object_storage_7.report.63660577226\n\n0 directories, 32 files\n\n\n\n\n\n\nExample\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n## Example:\n------+------------------------------------------+------------------------------------------------------------+-----------+------------+------------------+--------------------------+----\nOffset| RING\ns address-id                        | Filename                                                   | Child num | File Size  | Unixtime         | Localtime                |del?\n------+------------------------------------------+------------------------------------------------------------+-----------+------------+------------------+--------------------------+----\n194     296754181484029444656944009564610621293   photo/leo_redundant_manager/Makefile                             0       2034        1413348050768344   2014-10-15 13:40:50 +0900   0\n2400    185993533055981727582172380494809056426   photo/leo_redundant_manager/ebin/leo_redundant_manager.beam      0       24396       1413348050869454   2014-10-15 13:40:50 +0900   0\n38446   53208912738248114804281793572563205919    photo/leo_rpc/.git/refs/remotes/origin/HEAD                      0       33          1413348057441546   2014-10-15 13:40:57 +0900   0\n38658   57520977797167422772945547576980778561    photo/leo_rpc/ebin/leo_rpc_client_utils.beam                     0       2576        1413348057512261   2014-10-15 13:40:57 +0900   0\n69506   187294034498591995039607573685274229706   photo/leo_backend_db/src/leo_backend_db_server.erl               0       13911       1413348068031188   2014-10-15 13:41:08 +0900   0\n83603   316467020376888598364250682951088839795   photo/leo_backend_db/test/leo_backend_db_api_prop.erl            0       3507        1413348068052219   2014-10-15 13:41:08 +0900   1\n\n\n\n\n\n\nFormat\n\n\nA diagnose-log's format is TSV, \nTab Separated Values\n.\n\n\n\n\n\n\n\n\nColumn Number\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n1\n\n\nOffset of the AVS-file\n\n\n\n\n\n\n2\n\n\nRING\u2019s address id (routing-table)\n\n\n\n\n\n\n3\n\n\nFilename\n\n\n\n\n\n\n4\n\n\nChild number of a file\n\n\n\n\n\n\n5\n\n\nFile Size (byte)\n\n\n\n\n\n\n6\n\n\nTimestamp - unixtime\n\n\n\n\n\n\n7\n\n\nTimestamp - localtime\n\n\n\n\n\n\n8\n\n\nRemoved file?\n\n\n\n\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Settings / LeoGateway Settings\n\n\nFor Administrators / Settings / LeoStroage Settings\n\n\nFor Administrators / System Operations / Data Operations - Diagnosis", 
            "title": "Log Management"
        }, 
        {
            "location": "/admin/system_admin/log_management/#log-management", 
            "text": "", 
            "title": "Log Management"
        }, 
        {
            "location": "/admin/system_admin/log_management/#leogateway-access-log", 
            "text": "", 
            "title": "LeoGateway Access-log"
        }, 
        {
            "location": "/admin/system_admin/log_management/#overview", 
            "text": "LeoGateway provides  access-log  output feature so that you can investigate requests from users.", 
            "title": "Overview"
        }, 
        {
            "location": "/admin/system_admin/log_management/#how-to-output-access-log", 
            "text": "Modify  access-log  configuration item which include  leo_gateway.conf . After starting LeoGateway node(s), those nodes output  access-log  into the local disk, which is under each LeoGateway\u2019s log directory. See more detail  For Administrators / Settings / LeoGateway Settings .", 
            "title": "How To Output Access-log"
        }, 
        {
            "location": "/admin/system_admin/log_management/#access-log-configuration", 
            "text": "1\n2 ## Is enable access-log [true, false]  log.is_enable_access_log   =   true", 
            "title": "Access-log Configuration"
        }, 
        {
            "location": "/admin/system_admin/log_management/#leogateways-log-files", 
            "text": "Log File  File Name      access-log  /log/app/access.*    error-log  /log/app/error.*    info-log  /log/app/info.*     1\n2\n3\n4\n5\n6\n7\n8 log/\n\u251c\u2500\u2500 [ 272 Apr 28 14:13]  app\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [  98 Apr 28 14:13]  access -  /leofs/package/leo_gateway/log/app/access.20170428.14.1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [7.4K Apr 28 14:13]  access.20170428.14.1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [  97 Apr 28 14:13]  error -  /leofs/package/leo_gateway/log/app/error.20170428.14.1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [   0 Apr 28 14:13]  error.20170428.14.1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [  96 Apr 28 14:13]  info -  /leofs/package/leo_gateway/log/app/info.20170428.14.1\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 [5.4K Apr 28 14:13]  info.20170428.14.1", 
            "title": "LeoGateway's Log Files"
        }, 
        {
            "location": "/admin/system_admin/log_management/#example", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 --------+-------+--------------------+----------+-------+---------------------------------------+-----------------------+----------\nMethod  | Bucket| Path               |Child Num |  Size | Timestamp                             | Unixtime              | Response\n--------+-------+--------------------+----------|-------+---------------------------------------+-----------------------+----------\n[HEAD]   photo   photo/1              0          0       2013-10-18 13:28:56.148269 +0900        1381206536148320        500\n[HEAD]   photo   photo/1              0          0       2013-10-18 13:28:56.465670 +0900        1381206536465735        404\n[HEAD]   photo   photo/city/tokyo.png 0          0       2013-10-18 13:28:56.489234 +0900        1381206536489289        200\n[GET]    photo   photo/1              0          1024    2013-10-18 13:28:56.518631 +0900        1381206536518693        500\n[GET]    photo   photo/city/paris.png 0          2048    2013-10-18 13:28:56.550376 +0900        1381206536550444        404\n[PUT]    logs    logs/leofs           1          5242880 2013-10-18 13:28:56.518631 +0900        1381206536518693        500\n[PUT]    logs    logs/leofs           2          5242880 2013-10-18 13:28:56.518631 +0900        1381206536518693        500\n[PUT]    logs    logs/leofs           3          5120    2013-10-18 13:28:56.518631 +0900        1381206536518693        500", 
            "title": "Example"
        }, 
        {
            "location": "/admin/system_admin/log_management/#format", 
            "text": "An access-log's format is TSV,  Tab Separated Values .     Column Number  Description      1  Method: [HEAD    2  Bucket    3  Filename (including path)    4  Child number of a file    5  File Size (byte)    6  Timestamp with timezone    7  Unixtime (including micro-second)    8  Response (HTTP Status Code)", 
            "title": "Format"
        }, 
        {
            "location": "/admin/system_admin/log_management/#leostorage-data-diagnosis-log", 
            "text": "", 
            "title": "LeoStorage Data Diagnosis-log"
        }, 
        {
            "location": "/admin/system_admin/log_management/#overview_1", 
            "text": "LeoStorage provides  diagnosis-log  output feature so that you can investigate a LeoStorage's data. If you would like to turn on its feature, you need to modify  leo_storage.conf  which contain LeoStorage's package.", 
            "title": "Overview"
        }, 
        {
            "location": "/admin/system_admin/log_management/#how-to-output-diagnosis-log", 
            "text": "Modify  diagnosis-log  configuration item which include  leo_storage.conf . After executing the  diagnose-start  command, a specified node outputs  diagnosis-log  into the local disk, which is under each LeoStorage\u2019s AVS directory. See more detail  For Administrators / Settings / LeoStorage Settings .  Execute the  diagnose-start  command with a specified LeoStroage node, then its LeoStorage output diagnosis-log(s) into the local disk, which is under its LeoStorage\u2019s log directory. You can configure its log directory. See more detail  For Administrators / System Operations / Data Operations - Diagnosis  and  For Administrators / Settings / LeoStroage Settings .  1 $ leofs-adm diagnose-start  storage-node", 
            "title": "How To Output Diagnosis-log"
        }, 
        {
            "location": "/admin/system_admin/log_management/#diagnosis-log-configuration", 
            "text": "1\n2 ## Output data-diagnosis log  log.is_enable_diagnosis_log   =   true", 
            "title": "Diagnosis-log Configuration"
        }, 
        {
            "location": "/admin/system_admin/log_management/#leostorages-log-files", 
            "text": "Log File  File Name      access-log  /log/app/access.*    diagnosis-log  /avs/log/leo_object_storage_ .    error-log  /log/app/error.*    info-log  /log/app/info.*      1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36 ## diagnosis-log files\n/leofs/package/leo_storage/avs/log/\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_0 -  /leofs/package/leo_storage/avs/log/leo_object_storage_0.20170428.14.2\n\u251c\u2500\u2500 [ 683 Apr 28 14:40]  leo_object_storage_0.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_0.20170428.14.2\n\u251c\u2500\u2500 [ 366 Apr 28 14:40]  leo_object_storage_0.report.63660577222\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_1 -  /leofs/package/leo_storage/avs/log/leo_object_storage_1.20170428.14.2\n\u251c\u2500\u2500 [ 779 Apr 28 14:40]  leo_object_storage_1.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_1.20170428.14.2\n\u251c\u2500\u2500 [ 366 Apr 28 14:40]  leo_object_storage_1.report.63660577222\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_2 -  /leofs/package/leo_storage/avs/log/leo_object_storage_2.20170428.14.2\n\u251c\u2500\u2500 [ 786 Apr 28 14:40]  leo_object_storage_2.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_2.20170428.14.2\n\u251c\u2500\u2500 [ 368 Apr 28 14:40]  leo_object_storage_2.report.63660577223\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_3 -  /leofs/package/leo_storage/avs/log/leo_object_storage_3.20170428.14.2\n\u251c\u2500\u2500 [1.3K Apr 28 14:40]  leo_object_storage_3.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_3.20170428.14.2\n\u251c\u2500\u2500 [ 366 Apr 28 14:40]  leo_object_storage_3.report.63660577223\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_4 -  /leofs/package/leo_storage/avs/log/leo_object_storage_4.20170428.14.2\n\u251c\u2500\u2500 [1.3K Apr 28 14:40]  leo_object_storage_4.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_4.20170428.14.2\n\u251c\u2500\u2500 [ 368 Apr 28 14:40]  leo_object_storage_4.report.63660577224\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_5 -  /leofs/package/leo_storage/avs/log/leo_object_storage_5.20170428.14.2\n\u251c\u2500\u2500 [ 943 Apr 28 14:40]  leo_object_storage_5.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_5.20170428.14.2\n\u251c\u2500\u2500 [ 366 Apr 28 14:40]  leo_object_storage_5.report.63660577225\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_6 -  /leofs/package/leo_storage/avs/log/leo_object_storage_6.20170428.14.2\n\u251c\u2500\u2500 [1.5K Apr 28 14:40]  leo_object_storage_6.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_6.20170428.14.2\n\u251c\u2500\u2500 [ 368 Apr 28 14:40]  leo_object_storage_6.report.63660577225\n\u251c\u2500\u2500 [ 106 Apr 28 14:40]  leo_object_storage_7 -  /leofs/package/leo_storage/avs/log/leo_object_storage_7.20170428.14.2\n\u251c\u2500\u2500 [1.1K Apr 28 14:40]  leo_object_storage_7.20170428.14.1\n\u251c\u2500\u2500 [   0 Apr 28 14:40]  leo_object_storage_7.20170428.14.2\n\u2514\u2500\u2500 [ 368 Apr 28 14:40]  leo_object_storage_7.report.63660577226\n\n0 directories, 32 files", 
            "title": "LeoStorage's Log Files"
        }, 
        {
            "location": "/admin/system_admin/log_management/#example_1", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 ## Example:\n------+------------------------------------------+------------------------------------------------------------+-----------+------------+------------------+--------------------------+----\nOffset| RING s address-id                        | Filename                                                   | Child num | File Size  | Unixtime         | Localtime                |del?\n------+------------------------------------------+------------------------------------------------------------+-----------+------------+------------------+--------------------------+----\n194     296754181484029444656944009564610621293   photo/leo_redundant_manager/Makefile                             0       2034        1413348050768344   2014-10-15 13:40:50 +0900   0\n2400    185993533055981727582172380494809056426   photo/leo_redundant_manager/ebin/leo_redundant_manager.beam      0       24396       1413348050869454   2014-10-15 13:40:50 +0900   0\n38446   53208912738248114804281793572563205919    photo/leo_rpc/.git/refs/remotes/origin/HEAD                      0       33          1413348057441546   2014-10-15 13:40:57 +0900   0\n38658   57520977797167422772945547576980778561    photo/leo_rpc/ebin/leo_rpc_client_utils.beam                     0       2576        1413348057512261   2014-10-15 13:40:57 +0900   0\n69506   187294034498591995039607573685274229706   photo/leo_backend_db/src/leo_backend_db_server.erl               0       13911       1413348068031188   2014-10-15 13:41:08 +0900   0\n83603   316467020376888598364250682951088839795   photo/leo_backend_db/test/leo_backend_db_api_prop.erl            0       3507        1413348068052219   2014-10-15 13:41:08 +0900   1", 
            "title": "Example"
        }, 
        {
            "location": "/admin/system_admin/log_management/#format_1", 
            "text": "A diagnose-log's format is TSV,  Tab Separated Values .     Column Number  Description      1  Offset of the AVS-file    2  RING\u2019s address id (routing-table)    3  Filename    4  Child number of a file    5  File Size (byte)    6  Timestamp - unixtime    7  Timestamp - localtime    8  Removed file?", 
            "title": "Format"
        }, 
        {
            "location": "/admin/system_admin/log_management/#related-links", 
            "text": "For Administrators / Settings / LeoGateway Settings  For Administrators / Settings / LeoStroage Settings  For Administrators / System Operations / Data Operations - Diagnosis", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/system_admin/migration/", 
            "text": "System Migration\n\n\nUpgrade an old version LeoFS to v1.3.3\n\n\nOperation Flow\n\n\nIf you would like to migrate a LeoFS system, you can achieve that by following the operation flow.\n\n\n\n\n\n\nSee the large diagram\n\n\n\n\nTakeover and Adjust Confugurations\n\n\nBefore getting started with the migration of a LeoFS system, you need to take over the configuration, then adjust the paths and set the new configurations.\n\n\nAdded Or Changed LeoFS' Configurations\n\n\nLeoManager Master\n\n\n[since v1.3.3]\n LeoFS' MDC replication feature was improved. Some configuration are added in \nthe configuration of LeoManager's master\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n## --------------------------------------------------------------------\n\n\n## MANAGER - Multi DataCenter Settings\n\n\n## --------------------------------------------------------------------\n\n\n## A number of replication targets\n\n\nmdc_replication.max_targets\n \n=\n \n2\n\n\n\n## A number of replicas per a datacenter\n\n\n## [note] A local LeoFS sends a stacked object which contains an items of a replication method:\n\n\n##          - [L1_N] A number of replicas\n\n\n##          - [L1_W] A number of replicas needed for a successful WRITE operation\n\n\n##          - [L1_R] A number of replicas needed for a successful READ operation\n\n\n##          - [L1_D] A number of replicas needed for a successful DELETE operation\n\n\n##       A remote cluster of a LeoFS system which receives its object,\n\n\n##       and then replicates it by its contained reoplication method.\n\n\nmdc_replication.num_of_replicas_a_dc\n \n=\n \n1\n\n\n\n## MDC replication / A number of replicas needed for a successful WRITE operation\n\n\nmdc_replication.consistency.write\n \n=\n \n1\n\n\n\n## MDC replication / A number of replicas needed for a successful READ operation\n\n\nmdc_replication.consistency.read\n \n=\n \n1\n\n\n\n## MDC replication / A number of replicas needed for a successful DELETE operation\n\n\nmdc_replication.consistency.delete\n \n=\n \n1\n\n\n\n\n\n\n\nLeoStorage\n\n\n[since v1.3.3]\n Data synchronization configuration are added in \nthe configuration of LeoStorage\n.\n\n\n1\n2\n3\n4\n5\n6\n## Mode of the data synchronization - [none, periodic, writethrough]\n\n\n## - default:none\n\n\nobj_containers.sync_mode\n \n=\n \nnone\n\n\n\n## Interval in ms of the data synchronization - default: 1000ms\n\n\nobj_containers.sync_interval_in_ms\n \n=\n \n1000\n\n\n\n\n\n\n\nAbout LeoFS' Package\n\n\nStarting from v1.3.3, all LeoFS nodes are running as non-privileged user \nleofs\n in \nthe official Linux packages\n. It should work out of the box for new installations and for new nodes on existing installations. However, for existing nodes upgrading to v1.3.3 \n(or later)\n from previous versions, the change might be not seamless.\n\n\nExtra Steps\n\n\nRunning LeoFS with Default Paths\n\n\nFor those who have LeoFS configured with:\n\n\n\n\nqueue\n and \nmnesia\n in \n/usr/local/leofs/\nversion\n/leo_*/work\n\n\nlog files\n in \n/usr/local/leofs/\nversion\n/leo_*/log\n\n\nLeoStorage data files in \n/usr/local/leofs/\nversion\n/leo_storage/avs\n\n\n\n\nProcedures\n\n\nMigrate Files and Directories\n\n\nDuring upgrade of node \n(of any type)\n, \nafter\n stopping the old version and copying or moving every files to be moved into the new directories, change the owner with the commands below. It has to be done \nbefore\n launching the\nnew version.\n\n\n1\n2\n3\n4\n# chown -R leofs:leofs /usr/local/leofs/%version/leo_storage/avs\n# chown -R leofs:leofs /usr/local/leofs/%version/leo_gateway/cache\n# chown -R leofs:leofs /usr/local/leofs/%version/leo_*/log\n# chown -R leofs:leofs /usr/local/leofs/%version/leo_*/work\n\n\n\n\n\n\nRemove Unnecessary Directories\n\n\nRemove old temporary directory used by launch scripts. This step is needed because when earlier version was launched with \nroot\n permissions, it creates a set of temporary directories in \n/tmp\n which cannot be re-used by non-privileged user as is, and launch scripts will fail with obscure messages - or with no message at all, except for an error in syslog \n(usually \n/var/log/messages\n)\n.\n\n\n1\n# rm -rf /tmp/usr\n\n\n\n\n\n\nRe-launch the System\n\n\nStart the node through its launch script, as per upgrade flow diagram.\n\n\nRunning LeoFS with customized paths\n\n\nFor those who have LeoFS configured with, for example:\n\n\n\n\nqueue\n and \nmnesia\n in \n/mnt/work\n\n\nlog files\n in \n/var/log/leofs\n\n\n\n\nLeoStorage data files in \n/mnt/avs\n\n\n\n\n\n\nBefore starting new version of a node, execute \nchown -R leofs:leofs \n..\n for all these external directories\n\n\n\n\n\n\nDon't forget to remove temporary directory \n(\nrm -rf /tmp/usr\n)\n as well for the reasons described above.\n\n\n\n\n\n\nThese users might be interested in new features of \nenvironment config files\n, which allow to redefine some environment variables like paths in launch script.\n\n\nRefer \nFor Administrators / Settings / Environment Configuration\n for more information.\n\n\nRunning LeoFS with customized launch scripts\n\n\nFor those who have LeoFS already running as non-privileged user.\n\n\n\n\n\n\nScripts that are provided by packages generally should be enough to run on most configurations without changes. If needed, change user from \nleofs\n to some other in \"environment\" config files \n(e.g. \nRUNNER_USER=localuser\n)\n. Refer to the later section for more details about environment config files.\n\n\n\n\n\n\nPossible pitfall includes ownership of \n/usr/local/leofs/.erlang.cookie\n file, which is set to \nleofs\n during package installation. This should only be a problem when trying to run LeoFS nodes with permissions of some user which is not called \nleofs\n, but has home directory set to \n/usr/local/leofs\n. This is not supported due to technical reasons. Home directory of that user must be set to something else.\n\n\n\n\n\n\nRunning LeoFS in any form and keeping LeoFS running as \nroot\n\n\nFor those who want to keep maximum compatibility with the previous installation.\nIn \nenvironment config file\n, set \nRUNNER_USER\n:\n\n\n1\nRUNNER_USER\n=\nroot\n\n\n\n\n\n\n\nNote that switching this node to run as non-privileged user later will require extra steps to carefully change all permissions. This is not recommended, but possible \n(at very least, in addition to \nchown\n commands from before, permissions of \nleo_*/etc\n and \nleo_*/snmp/*/db\n will have to be changed recursively as well)\n.\n\n\nNote for Developers\n\n\nAs described at the previous section, the default user running LeoFS processes has changed to \nleofs\n so that requires developers to\n\n\n\n\nTweak environment config files to set \nRUNNER_USER\n to the user you have logged in while developing with \nmake release/bootstrap.sh/mdcr.sh\n.\n\n\nRemove all files under \n$PIPE_DIR\n before starting any LeoFS processes.\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Settings / Environment Configuration\n\n\nFor Administrators / Settings / LeoManager Settings\n\n\nFor Administrators / Settings / LeoStorage Settings\n\n\nFor Administrators / Settings / LeoGateway Settings", 
            "title": "System Migration"
        }, 
        {
            "location": "/admin/system_admin/migration/#system-migration", 
            "text": "", 
            "title": "System Migration"
        }, 
        {
            "location": "/admin/system_admin/migration/#upgrade-an-old-version-leofs-to-v133", 
            "text": "", 
            "title": "Upgrade an old version LeoFS to v1.3.3"
        }, 
        {
            "location": "/admin/system_admin/migration/#operation-flow", 
            "text": "If you would like to migrate a LeoFS system, you can achieve that by following the operation flow.    See the large diagram", 
            "title": "Operation Flow"
        }, 
        {
            "location": "/admin/system_admin/migration/#takeover-and-adjust-confugurations", 
            "text": "Before getting started with the migration of a LeoFS system, you need to take over the configuration, then adjust the paths and set the new configurations.", 
            "title": "Takeover and Adjust Confugurations"
        }, 
        {
            "location": "/admin/system_admin/migration/#added-or-changed-leofs-configurations", 
            "text": "", 
            "title": "Added Or Changed LeoFS' Configurations"
        }, 
        {
            "location": "/admin/system_admin/migration/#leomanager-master", 
            "text": "[since v1.3.3]  LeoFS' MDC replication feature was improved. Some configuration are added in  the configuration of LeoManager's master .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24 ## --------------------------------------------------------------------  ## MANAGER - Multi DataCenter Settings  ## --------------------------------------------------------------------  ## A number of replication targets  mdc_replication.max_targets   =   2  ## A number of replicas per a datacenter  ## [note] A local LeoFS sends a stacked object which contains an items of a replication method:  ##          - [L1_N] A number of replicas  ##          - [L1_W] A number of replicas needed for a successful WRITE operation  ##          - [L1_R] A number of replicas needed for a successful READ operation  ##          - [L1_D] A number of replicas needed for a successful DELETE operation  ##       A remote cluster of a LeoFS system which receives its object,  ##       and then replicates it by its contained reoplication method.  mdc_replication.num_of_replicas_a_dc   =   1  ## MDC replication / A number of replicas needed for a successful WRITE operation  mdc_replication.consistency.write   =   1  ## MDC replication / A number of replicas needed for a successful READ operation  mdc_replication.consistency.read   =   1  ## MDC replication / A number of replicas needed for a successful DELETE operation  mdc_replication.consistency.delete   =   1", 
            "title": "LeoManager Master"
        }, 
        {
            "location": "/admin/system_admin/migration/#leostorage", 
            "text": "[since v1.3.3]  Data synchronization configuration are added in  the configuration of LeoStorage .  1\n2\n3\n4\n5\n6 ## Mode of the data synchronization - [none, periodic, writethrough]  ## - default:none  obj_containers.sync_mode   =   none  ## Interval in ms of the data synchronization - default: 1000ms  obj_containers.sync_interval_in_ms   =   1000", 
            "title": "LeoStorage"
        }, 
        {
            "location": "/admin/system_admin/migration/#about-leofs-package", 
            "text": "Starting from v1.3.3, all LeoFS nodes are running as non-privileged user  leofs  in  the official Linux packages . It should work out of the box for new installations and for new nodes on existing installations. However, for existing nodes upgrading to v1.3.3  (or later)  from previous versions, the change might be not seamless.", 
            "title": "About LeoFS' Package"
        }, 
        {
            "location": "/admin/system_admin/migration/#extra-steps", 
            "text": "", 
            "title": "Extra Steps"
        }, 
        {
            "location": "/admin/system_admin/migration/#running-leofs-with-default-paths", 
            "text": "For those who have LeoFS configured with:   queue  and  mnesia  in  /usr/local/leofs/ version /leo_*/work  log files  in  /usr/local/leofs/ version /leo_*/log  LeoStorage data files in  /usr/local/leofs/ version /leo_storage/avs", 
            "title": "Running LeoFS with Default Paths"
        }, 
        {
            "location": "/admin/system_admin/migration/#procedures", 
            "text": "", 
            "title": "Procedures"
        }, 
        {
            "location": "/admin/system_admin/migration/#migrate-files-and-directories", 
            "text": "During upgrade of node  (of any type) ,  after  stopping the old version and copying or moving every files to be moved into the new directories, change the owner with the commands below. It has to be done  before  launching the\nnew version.  1\n2\n3\n4 # chown -R leofs:leofs /usr/local/leofs/%version/leo_storage/avs\n# chown -R leofs:leofs /usr/local/leofs/%version/leo_gateway/cache\n# chown -R leofs:leofs /usr/local/leofs/%version/leo_*/log\n# chown -R leofs:leofs /usr/local/leofs/%version/leo_*/work", 
            "title": "Migrate Files and Directories"
        }, 
        {
            "location": "/admin/system_admin/migration/#remove-unnecessary-directories", 
            "text": "Remove old temporary directory used by launch scripts. This step is needed because when earlier version was launched with  root  permissions, it creates a set of temporary directories in  /tmp  which cannot be re-used by non-privileged user as is, and launch scripts will fail with obscure messages - or with no message at all, except for an error in syslog  (usually  /var/log/messages ) .  1 # rm -rf /tmp/usr", 
            "title": "Remove Unnecessary Directories"
        }, 
        {
            "location": "/admin/system_admin/migration/#re-launch-the-system", 
            "text": "Start the node through its launch script, as per upgrade flow diagram.", 
            "title": "Re-launch the System"
        }, 
        {
            "location": "/admin/system_admin/migration/#running-leofs-with-customized-paths", 
            "text": "For those who have LeoFS configured with, for example:   queue  and  mnesia  in  /mnt/work  log files  in  /var/log/leofs   LeoStorage data files in  /mnt/avs    Before starting new version of a node, execute  chown -R leofs:leofs  ..  for all these external directories    Don't forget to remove temporary directory  ( rm -rf /tmp/usr )  as well for the reasons described above.    These users might be interested in new features of  environment config files , which allow to redefine some environment variables like paths in launch script.  Refer  For Administrators / Settings / Environment Configuration  for more information.", 
            "title": "Running LeoFS with customized paths"
        }, 
        {
            "location": "/admin/system_admin/migration/#running-leofs-with-customized-launch-scripts", 
            "text": "For those who have LeoFS already running as non-privileged user.    Scripts that are provided by packages generally should be enough to run on most configurations without changes. If needed, change user from  leofs  to some other in \"environment\" config files  (e.g.  RUNNER_USER=localuser ) . Refer to the later section for more details about environment config files.    Possible pitfall includes ownership of  /usr/local/leofs/.erlang.cookie  file, which is set to  leofs  during package installation. This should only be a problem when trying to run LeoFS nodes with permissions of some user which is not called  leofs , but has home directory set to  /usr/local/leofs . This is not supported due to technical reasons. Home directory of that user must be set to something else.", 
            "title": "Running LeoFS with customized launch scripts"
        }, 
        {
            "location": "/admin/system_admin/migration/#running-leofs-in-any-form-and-keeping-leofs-running-as-root", 
            "text": "For those who want to keep maximum compatibility with the previous installation.\nIn  environment config file , set  RUNNER_USER :  1 RUNNER_USER = root    Note that switching this node to run as non-privileged user later will require extra steps to carefully change all permissions. This is not recommended, but possible  (at very least, in addition to  chown  commands from before, permissions of  leo_*/etc  and  leo_*/snmp/*/db  will have to be changed recursively as well) .", 
            "title": "Running LeoFS in any form and keeping LeoFS running as root"
        }, 
        {
            "location": "/admin/system_admin/migration/#note-for-developers", 
            "text": "As described at the previous section, the default user running LeoFS processes has changed to  leofs  so that requires developers to   Tweak environment config files to set  RUNNER_USER  to the user you have logged in while developing with  make release/bootstrap.sh/mdcr.sh .  Remove all files under  $PIPE_DIR  before starting any LeoFS processes.", 
            "title": "Note for Developers"
        }, 
        {
            "location": "/admin/system_admin/migration/#related-links", 
            "text": "For Administrators / Settings / Environment Configuration  For Administrators / Settings / LeoManager Settings  For Administrators / Settings / LeoStorage Settings  For Administrators / Settings / LeoGateway Settings", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/system_admin/integration/", 
            "text": "System Integration\n\n\nCDN Integration\n\n\nHow To Integrate LeoFS With CDN\n\n\nThere is nothing special to do for integrating LeoFS with CDN\n1\n. Since almost CDN service providers take care of a CacheControl Header received from an origin to determine how long a file should be cached on their edge servers, so if you want to modify TTL according to URLs, you can do it by using \nhttp_custom_header.conf\n.\n\n\nHow To Use \nhttp_custom_header.conf\n\n\nAppend the following line to \nleo_gateway.conf\n which contains a LeoGateway's directory. Arrange the \nhttp_custom_header.conf\n into the path specified at \nleo_gateway.conf\n.\n\n\n1\n2\n## HTTP custom header configuration file path\n\n\nhttp.headers_config_file\n \n=\n \n./etc/http_custom_header.conf\n\n\n\n\n\n\n\nHow To Write \nhttp_custom_header.conf\n\n\nThe syntax is a subset of \nNginx\n2\n configuration\n. You can use location contexts to specify TTL and add any headers to the path.\n\n\n1\n2\n3\n4\n5\nlocation bucket/static {\n    expires    12h;\n    add_header Cache-Control public;\n    add_header X-OriginalHeader OriginalValue;\n}\n\n\n\n\n\n\nIn this case, assuming that a CDN service already has been enabled, and there is a file at \nbucket/static/path_to_file\n, if a user browses that file via the CDN. The CDN will receive a response from a LeoFS system with customized Http headers.\n\n\n1\n2\nCache-Control\n:\n \npublic\n,\n \nmax-age\n=\n43200\n;\n\n\nX-OriginalHeader\n:\n \nOriginalValue\n;\n\n\n\n\n\n\n\nUse Cases\n\n\nSpecify TTL by the bucket.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\nlocation bucket1 {\n    expires    1h;\n    add_header Cache-Control public;\n}\nlocation bucket2 {\n    expires    1d;\n    add_header Cache-Control public;\n}\nlocation bucket3 {\n    expires    1h30m;\n    add_header Cache-Control private;\n}\n\n\n\n\n\n\nAppendix\n\n\nSyntax for the expire field\n\n\nLeoFS supports a part of measurement units which can be used in Nginx configuration. Following time intervals can be specified.\n\n\n1\n2\n3\n4\ns\n:\n \nseconds\n\n\nm\n:\n \nminutes\n\n\nh\n:\n \nhours\n\n\nd\n:\n \ndays\n\n\n\n\n\n\n\nList of verified CDN services\n\n\nLeoFS Team tested the following CDN services with LeoFS. We recognize that other CDN services also should work.\n\n\n\n\nAmazon CloudFront \u2013 Content Delivery Network (CDN)\n\n\n\n\nRelated Links\n\n\n\n\nFor Administrators / Settings / LeoGateway Settings\n\n\n\n\n\n\n\n\n\n\n\n\nCDN, Content delivery network\n\n\n\n\n\n\nNginx, An HTTP and reverse proxy server", 
            "title": "System Integration"
        }, 
        {
            "location": "/admin/system_admin/integration/#system-integration", 
            "text": "", 
            "title": "System Integration"
        }, 
        {
            "location": "/admin/system_admin/integration/#cdn-integration", 
            "text": "", 
            "title": "CDN Integration"
        }, 
        {
            "location": "/admin/system_admin/integration/#how-to-integrate-leofs-with-cdn", 
            "text": "There is nothing special to do for integrating LeoFS with CDN 1 . Since almost CDN service providers take care of a CacheControl Header received from an origin to determine how long a file should be cached on their edge servers, so if you want to modify TTL according to URLs, you can do it by using  http_custom_header.conf .", 
            "title": "How To Integrate LeoFS With CDN"
        }, 
        {
            "location": "/admin/system_admin/integration/#how-to-use-http_custom_headerconf", 
            "text": "Append the following line to  leo_gateway.conf  which contains a LeoGateway's directory. Arrange the  http_custom_header.conf  into the path specified at  leo_gateway.conf .  1\n2 ## HTTP custom header configuration file path  http.headers_config_file   =   ./etc/http_custom_header.conf", 
            "title": "How To Use http_custom_header.conf"
        }, 
        {
            "location": "/admin/system_admin/integration/#how-to-write-http_custom_headerconf", 
            "text": "The syntax is a subset of  Nginx 2  configuration . You can use location contexts to specify TTL and add any headers to the path.  1\n2\n3\n4\n5 location bucket/static {\n    expires    12h;\n    add_header Cache-Control public;\n    add_header X-OriginalHeader OriginalValue;\n}   In this case, assuming that a CDN service already has been enabled, and there is a file at  bucket/static/path_to_file , if a user browses that file via the CDN. The CDN will receive a response from a LeoFS system with customized Http headers.  1\n2 Cache-Control :   public ,   max-age = 43200 ;  X-OriginalHeader :   OriginalValue ;", 
            "title": "How To Write http_custom_header.conf"
        }, 
        {
            "location": "/admin/system_admin/integration/#use-cases", 
            "text": "Specify TTL by the bucket.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 location bucket1 {\n    expires    1h;\n    add_header Cache-Control public;\n}\nlocation bucket2 {\n    expires    1d;\n    add_header Cache-Control public;\n}\nlocation bucket3 {\n    expires    1h30m;\n    add_header Cache-Control private;\n}", 
            "title": "Use Cases"
        }, 
        {
            "location": "/admin/system_admin/integration/#appendix", 
            "text": "", 
            "title": "Appendix"
        }, 
        {
            "location": "/admin/system_admin/integration/#syntax-for-the-expire-field", 
            "text": "LeoFS supports a part of measurement units which can be used in Nginx configuration. Following time intervals can be specified.  1\n2\n3\n4 s :   seconds  m :   minutes  h :   hours  d :   days", 
            "title": "Syntax for the expire field"
        }, 
        {
            "location": "/admin/system_admin/integration/#list-of-verified-cdn-services", 
            "text": "LeoFS Team tested the following CDN services with LeoFS. We recognize that other CDN services also should work.   Amazon CloudFront \u2013 Content Delivery Network (CDN)", 
            "title": "List of verified CDN services"
        }, 
        {
            "location": "/admin/system_admin/integration/#related-links", 
            "text": "For Administrators / Settings / LeoGateway Settings       CDN, Content delivery network    Nginx, An HTTP and reverse proxy server", 
            "title": "Related Links"
        }, 
        {
            "location": "/admin/index_of_commands/", 
            "text": "Index of leofs-adm Command Lines\n\n\nleofs-adm\n easily makes administrative operations of LeoFS, the commands of which include as below:\n\n\n\n\nGeneral Commands\n\n\nLeoStorage Cluster Operation\n\n\nLeoStorage MQ Operation\n\n\nRecover Commands\n\n\nData Compaction Commands\n\n\nDisk Usage Commands\n\n\nLeoGateway Operation\n\n\nLeoManager Maintenance Commands\n\n\nS3-API Related Commands\n\n\nMulti Data Center Operation\n\n\n\n\n\n\n\n\n\n\nCommand\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGeneral Commands:\n\n\n\n\n\n\n\n\nstatus \n[\nnode\n]\n\n\nRetrieve status of every node (default)\n Retrieve status of a specified node\n\n\n\n\n\n\nwhereis \nfile-path\n\n\nRetrieve an assigned object by a file path\n\n\n\n\n\n\nStorage Operation:\n\n\n\n\n\n\n\n\ndetach \nstorage-node\n\n\nRemove a storage node in a LeoFS' storage cluster\nCurrent status: \nrunning\n OR \nstop\n\n\n\n\n\n\nsuspend \nstorage-node\n\n\nSuspend a storage node for maintenance\nThis command does NOT detach a node from a LeoFS' storage cluster\nWhile suspending, it rejects any requests\nCurrent status: \nrunning\n\n\n\n\n\n\nresume \nstorage-node\n\n\nResume a storage node for finished maintenance\nCurrent status: \nsuspended\n OR \nrestarted\n\n\n\n\n\n\nstart\n\n\nStart LeoFS after distributing a RING from LeoFS Manager to LeoFS Storage and LeoFS Gateway\n\n\n\n\n\n\nrebalance\n\n\nCommit detached and attached nodes to join a cluster\nRebalance objects in a cluster based on updated cluster topology\n\n\n\n\n\n\nmq-stats \nstorage-node\n\n\nSee statuses of message queues used in a LeoStorage node\n\n\n\n\n\n\nmq-suspend \nstorage-node\n \nmq-id\n\n\nSuspend a process consuming a message queue\nActive message queues only can be suspended\nWhile suspending, no messages are consumed\n\n\n\n\n\n\nmq-resume \nstorage-node\n \nmq-id\n\n\nResume a process consuming a message queue\n\n\n\n\n\n\nRecover Commands:\n\n\n\n\n\n\n\n\nrecover-file \nfile-path\n\n\nRecover an inconsistent object specified by a file-path\n\n\n\n\n\n\nrecover-node \nstorage-node\n\n\nRecover all inconsistent objects in a specified node\n\n\n\n\n\n\nrecover-ring \nstorage-node\n\n\nRecover \nRING\n, a routing table of a specified node\n\n\n\n\n\n\nrecover-cluster \ncluster-id\n\n\nRecover all inconsistent objects in a specified cluster in case of using the multi datacenter replication\n\n\n\n\n\n\nCompaction Commands:\n\n\n\n\n\n\n\n\ncompact-start \nnode\n \nnum-of-targets\n [\nnumber-of-compaction-procs\n]\n\n\nRemove unnecessary objects from a specified node\n \nnum-of-targets\n: It controls a number of containers in parallel \n \nnum-of-compaction-procs\n: It controls a number of procs to execute the data compaction in parallel\n\n\n\n\n\n\ncompact-suspend \nstorage-node\n\n\nSuspend a data compaction processing\n\n\n\n\n\n\ncompact-resume \nstorage-node\n\n\nResume a data compaction processing\n\n\n\n\n\n\ncompact-status \nstorage-node\n\n\nSee current compaction status\n Compaction\u2019s status: \nidle\n, \nrunning\n, \nsuspend\n\n\n\n\n\n\ndiagnose-start \nnode\n\n\nDiagnose data of a specified storage node\n\n\n\n\n\n\nDisk Usage Commands:\n\n\n\n\n\n\n\n\ndu \nstorage-node\n\n\nSee current disk usages\n\n\n\n\n\n\ndu detail \nstorage-node\n\n\nSee current disk usages in detail\n\n\n\n\n\n\nGateway Operation:\n\n\n\n\n\n\n\n\npurge-cache \nfile-path\n\n\nRemove a cache from each LeoFS gateway\n\n\n\n\n\n\nremove-gateway \ngateway-node\n\n\nRemove a specified LeoGateway node, which is already stopped\n\n\n\n\n\n\nManager Maintenance:\n\n\n\n\n\n\n\n\nbackup-mnesia \nbackup-filepath\n\n\nCopy LeoFS\u2019s Manager data to a filepath\n\n\n\n\n\n\nrestore-mnesia \nbackup-filepath\n\n\nRestore LeoFS\u2019s Manager data from a backup file\n\n\n\n\n\n\nupdate-managers \nmanager-master\n \nmanager-slave\n\n\nUpdate LeoFS Manager nodes\n Destribute a new LeoManager nodes to LeoStorage and LeoGateway\n\n\n\n\n\n\ndump-ring \nnode\n\n\nDump RING, a routing table to a local disk\n\n\n\n\n\n\nupdate-log-level \ngateway/storage-node\n \nlog-level\n\n\nUpdate log level of a specified node\n log-level: debug, info, warn, error\n\n\n\n\n\n\nupdate-consistency-level \nwrite-quorum\n \nread-quorum\n \ndelete-quorum\n\n\nUpdate current consistency level of \nR-quorum\n, \nW-quorum\n and \nD-quorum\n\n\n\n\n\n\nWatchdog Operation:\n\n\n\n\n\n\n\n\nupdate-property \nnode\n \nproperty-name\n \nproperty-value\n\n\nUpdate watchdog properties of a specifid node, which includes as below:\n- watchdog.cpu_enabled \nboolean\n- watchdog.cpu_raised_error_times \ninteger\n- watchdog.cpu_interval \ninteger\n- watchdog.cpu_threshold_load_avg \nfloat\n- watchdog.cpu_threshold_util \ninteger\n- watchdog.disk_enabled \nboolean\n- watchdog.disk_raised_error_times \ninteger\n- watchdog.disk_interval \ninteger\n- watchdog.disk_threshold_use \ninteger\n- watchdog.disk_threshold_util \ninteger\n- watchdog.disk_threshold_rkb \ninteger\n- watchdog.disk_threshold_wkb \ninteger\n- watchdog.cluster_enabled \nboolean\n- watchdog.cluster_interval \ninteger\n\n\n\n\n\n\nS3-related Commands:\n\n\n\n\n\n\n\n\ncreate-user \nuser-id\n \n[\npassword\n]\n\n\nRegister a new user\n Generate an S3 key pair, \nAccess Key ID\n and \nSecret Access Key\n\n\n\n\n\n\nimport-user \nuser-id\n \naccess-key-id\n \nsecret-access-key\n\n\nImport a new user with S3 key pair, \nAccess Key ID\n and \nSecret Access Key\n\n\n\n\n\n\ndelete-user \nuser-id\n\n\nRemove a user\n\n\n\n\n\n\nget-users\n\n\nRetrieve a list of users\n\n\n\n\n\n\nupdate-user-role \nuser-id\n \nrole-id\n\n\nUpdate a user\u2019s role\n Currently, we are supporting two kinds of roles, [1: \nGeneral user\n], [9: \nAdministrator\n]\n\n\n\n\n\n\nadd-endpoint \nendpoint\n\n\nRegister a new S3 Endpoint\n LeoFS\u2019 domains are ruled by \nNaming rule of AWS S3 Bucket\n\n\n\n\n\n\ndelete-endpoint \nendpoint\n\n\nRemove an endpoint\n\n\n\n\n\n\nget-endpoints\n\n\nRetrieve a list of endpoints\n\n\n\n\n\n\nadd-bucket \nbucket\n \naccess-key-id\n\n\nCreate a new bucket\n\n\n\n\n\n\ndelete-bucket \nbucket\n \naccess-key-id\n\n\nRemove a bucket and all files stored in the bucket\n\n\n\n\n\n\nget-bucket \naccess-key-id\n\n\nRetrieve a list of buckets owned by a specified user\n\n\n\n\n\n\nget-buckets\n\n\nRetrieve a list of all buckets registered\n\n\n\n\n\n\nchown-bucket \nbucket\n \naccess-key-id\n\n\nChange an owner of a bucket\n\n\n\n\n\n\nupdate-acl \nbucket\n \naccess-key-id\n \nacl\n\n\nUpdate ACL, Access Control List for a bucket\n- \nprivate (default)\n: No one except an owner has access rights\n- \npublic-read\n: All users have READ access\n- \npublic-read-write\n: All users have READ and WRITE access\n\n\n\n\n\n\ngen-nfs-mnt-key \nbucket\n \naccess-key-id\n \nclient-ip-address\n\n\nGenerate a key for NFS mount\n\n\n\n\n\n\nMulti Data Center Operation:\n\n\n\n\n\n\n\n\njoin-cluster \nremote-manager-master\n \nremote-manager-slave\n\n\nBegin to communicate between a local cluster and a remote cluster\n\n\n\n\n\n\nremove-cluster \nremote-manager-master\n \nremote-manager-slave\n\n\nTerminate to communicate between a local cluster and a remote cluster\n\n\n\n\n\n\ncluster-status\n\n\nSee a current state of cluster(s)", 
            "title": "Index of LeoFS' Commands"
        }, 
        {
            "location": "/admin/index_of_commands/#index-of-leofs-adm-command-lines", 
            "text": "leofs-adm  easily makes administrative operations of LeoFS, the commands of which include as below:   General Commands  LeoStorage Cluster Operation  LeoStorage MQ Operation  Recover Commands  Data Compaction Commands  Disk Usage Commands  LeoGateway Operation  LeoManager Maintenance Commands  S3-API Related Commands  Multi Data Center Operation      Command  Description      General Commands:     status  [ node ]  Retrieve status of every node (default)  Retrieve status of a specified node    whereis  file-path  Retrieve an assigned object by a file path    Storage Operation:     detach  storage-node  Remove a storage node in a LeoFS' storage cluster Current status:  running  OR  stop    suspend  storage-node  Suspend a storage node for maintenance This command does NOT detach a node from a LeoFS' storage cluster While suspending, it rejects any requests Current status:  running    resume  storage-node  Resume a storage node for finished maintenance Current status:  suspended  OR  restarted    start  Start LeoFS after distributing a RING from LeoFS Manager to LeoFS Storage and LeoFS Gateway    rebalance  Commit detached and attached nodes to join a cluster Rebalance objects in a cluster based on updated cluster topology    mq-stats  storage-node  See statuses of message queues used in a LeoStorage node    mq-suspend  storage-node   mq-id  Suspend a process consuming a message queue Active message queues only can be suspended While suspending, no messages are consumed    mq-resume  storage-node   mq-id  Resume a process consuming a message queue    Recover Commands:     recover-file  file-path  Recover an inconsistent object specified by a file-path    recover-node  storage-node  Recover all inconsistent objects in a specified node    recover-ring  storage-node  Recover  RING , a routing table of a specified node    recover-cluster  cluster-id  Recover all inconsistent objects in a specified cluster in case of using the multi datacenter replication    Compaction Commands:     compact-start  node   num-of-targets  [ number-of-compaction-procs ]  Remove unnecessary objects from a specified node   num-of-targets : It controls a number of containers in parallel    num-of-compaction-procs : It controls a number of procs to execute the data compaction in parallel    compact-suspend  storage-node  Suspend a data compaction processing    compact-resume  storage-node  Resume a data compaction processing    compact-status  storage-node  See current compaction status  Compaction\u2019s status:  idle ,  running ,  suspend    diagnose-start  node  Diagnose data of a specified storage node    Disk Usage Commands:     du  storage-node  See current disk usages    du detail  storage-node  See current disk usages in detail    Gateway Operation:     purge-cache  file-path  Remove a cache from each LeoFS gateway    remove-gateway  gateway-node  Remove a specified LeoGateway node, which is already stopped    Manager Maintenance:     backup-mnesia  backup-filepath  Copy LeoFS\u2019s Manager data to a filepath    restore-mnesia  backup-filepath  Restore LeoFS\u2019s Manager data from a backup file    update-managers  manager-master   manager-slave  Update LeoFS Manager nodes  Destribute a new LeoManager nodes to LeoStorage and LeoGateway    dump-ring  node  Dump RING, a routing table to a local disk    update-log-level  gateway/storage-node   log-level  Update log level of a specified node  log-level: debug, info, warn, error    update-consistency-level  write-quorum   read-quorum   delete-quorum  Update current consistency level of  R-quorum ,  W-quorum  and  D-quorum    Watchdog Operation:     update-property  node   property-name   property-value  Update watchdog properties of a specifid node, which includes as below: - watchdog.cpu_enabled  boolean - watchdog.cpu_raised_error_times  integer - watchdog.cpu_interval  integer - watchdog.cpu_threshold_load_avg  float - watchdog.cpu_threshold_util  integer - watchdog.disk_enabled  boolean - watchdog.disk_raised_error_times  integer - watchdog.disk_interval  integer - watchdog.disk_threshold_use  integer - watchdog.disk_threshold_util  integer - watchdog.disk_threshold_rkb  integer - watchdog.disk_threshold_wkb  integer - watchdog.cluster_enabled  boolean - watchdog.cluster_interval  integer    S3-related Commands:     create-user  user-id   [ password ]  Register a new user  Generate an S3 key pair,  Access Key ID  and  Secret Access Key    import-user  user-id   access-key-id   secret-access-key  Import a new user with S3 key pair,  Access Key ID  and  Secret Access Key    delete-user  user-id  Remove a user    get-users  Retrieve a list of users    update-user-role  user-id   role-id  Update a user\u2019s role  Currently, we are supporting two kinds of roles, [1:  General user ], [9:  Administrator ]    add-endpoint  endpoint  Register a new S3 Endpoint  LeoFS\u2019 domains are ruled by  Naming rule of AWS S3 Bucket    delete-endpoint  endpoint  Remove an endpoint    get-endpoints  Retrieve a list of endpoints    add-bucket  bucket   access-key-id  Create a new bucket    delete-bucket  bucket   access-key-id  Remove a bucket and all files stored in the bucket    get-bucket  access-key-id  Retrieve a list of buckets owned by a specified user    get-buckets  Retrieve a list of all buckets registered    chown-bucket  bucket   access-key-id  Change an owner of a bucket    update-acl  bucket   access-key-id   acl  Update ACL, Access Control List for a bucket -  private (default) : No one except an owner has access rights -  public-read : All users have READ access -  public-read-write : All users have READ and WRITE access    gen-nfs-mnt-key  bucket   access-key-id   client-ip-address  Generate a key for NFS mount    Multi Data Center Operation:     join-cluster  remote-manager-master   remote-manager-slave  Begin to communicate between a local cluster and a remote cluster    remove-cluster  remote-manager-master   remote-manager-slave  Terminate to communicate between a local cluster and a remote cluster    cluster-status  See a current state of cluster(s)", 
            "title": "Index of leofs-adm Command Lines"
        }, 
        {
            "location": "/benchmark/README/", 
            "text": "Benchmark\n\n\nSetting up basho_bench\n\n\nInstallation\n\n\n\n\nBasho basho_bench\u2019s repository\n\n\nBasho basho_bench\u2019s documentation\n\n\nUse the following commands to set up basho_bench.\n\n\n\n\n1\n2\n3\n4\n5\n6\n$ git clone git://github.com/basho/basho_bench.git\n$ git clone https://github.com/leo-project/leofs.git\n$ \ncd\n basho_bench\n$ cp -i ../leofs/test/src/*.erl src/\n$ cp -i ../leofs/test/include/*.hrl include/\n$ make all\n\n\n\n\n\n\nPreparations before testing\n\n\nCreate a test bucket\n\n\nAfter starting a LeoFS system, you need to create a bucket before getting started with benchmarks. In this example, the bucket name is \ntest\n. It is owned by the user \n_test_leofs\n that is already registered internally by LeoFS.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n$ leofs-adm add-endpoint \ngateway-ip-address\n\nOK\n\n$ leofs-adm add-bucket \ntest\n \n05236\n\nOK\n\n$ leofs-adm get-buckets\nbucket   \n|\n owner       \n|\n created at\n---------+-------------+---------------------------\n\ntest\n     \n|\n _test_leofs \n|\n \n2013\n-02-27 \n14\n:06:54 +0900\n\n$ leofs-adm update-acl \ntest\n \n05236\n public-read\nOK\n\n\n\n\n\n\nConfiguration file for basho_bench\n\n\nAn Example\n\n\nSome examples are included in LeoFS' repository at \nleo-project / leofs / test / conf\n. If you would like to learn basho_bench's configuration, you can see \nBashoBench's Configuration\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n{\nmode\n,\n      \nmax\n}.\n\n\n{\nduration\n,\n   \n10\n}.\n\n\n{\nconcurrent\n,\n \n50\n}.\n\n\n\n{\ndriver\n,\n \nbasho_bench_driver_leofs\n}.\n\n\n{\ncode_paths\n,\n \n[\ndeps/ibrowse\n]}.\n\n\n\n{\nhttp_raw_ips\n,\n \n[\n${HOST_NAME_OF_LEOFS_GATEWAY}\n]}.\n\n\n{\nhttp_raw_port\n,\n \n8080\n}.\n\n\n{\nhttp_raw_path\n,\n \n/test\n}.\n\n\n%% {http_raw_path, \n/${BUCKET}\n}.\n\n\n\n{\nkey_generator\n,\n   \n{\npartitioned_sequential_int\n,\n \n1000000\n}}.\n\n\n{\nvalue_generator\n,\n \n{\nfixed_bin\n,\n \n16384\n}}.\n \n%% 16KB\n\n\n{\noperations\n,\n \n[{\nput\n,\n1\n}]}.\n               \n%% PUT:100%\n\n\n%%{operations, [{put,1}, {get, 4}]}.   %% PUT:20%, GET:80%\n\n\n\n{\ncheck_integrity\n,\n \nfalse\n}.\n\n\n\n\n\n\n\nDescription\n\n\n\n\n\n\n\n\nKey\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nhttp_raw_ips\n\n\nThe LeoFS Gateway nodes we want to benchmark\n\n\n\n\n\n\nhttp_raw_port\n\n\nThe port used by the LeoFS Gateway nodes\n\n\n\n\n\n\nhttp_raw_path\n\n\nURL path prefix. The first path segment MUST be a BUCKET name\n\n\n\n\n\n\ncheck_integrity (default:false)\n\n\nCheck integrity of registered object - compare an original MD5 with a retrieved object\u2019s MD5 (Only for developers)\n\n\n\n\n\n\n\n\nRunning basho_bench (1)\n\n\nIn this example, LeoFS and \nbasho_bench\n are installed locally. The following commands can be used to run \nbasho_bench\n.\n\n\n1\n2\n3\n### Loading 1M records of size 16KB\n\n\ncd\n basho_bench\n./basho_bench ../leofs/test/conf/leofs_16K_LOAD1M.config\n\n\n\n\n\n\nRunning basho_bench (2)\n\n\nIn this example, LeoFS and \nbasho_bench\n are installed on different hosts.\n\n\nConfigure the endpoint on LeoManager's console\n\n\nAllows basho_bench\u2019s requests to reach \n${\nHOST_NAME_OF_LEOFS_GATEWAY\n}\n.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n$ leofs-adm add-endpoint \nhost-name-of-leofs-gateway\n\nOK\n\n$ leofs-adm get-endpoints\nendpoint                      \n|\n created at\n------------------------------+---------------------------\nlocalhost                     \n|\n \n2013\n-03-01 \n00\n:14:04 +0000\ns3.amazonaws.com              \n|\n \n2013\n-03-01 \n00\n:14:04 +0000\n\n${\nHOST_NAME_OF_LEOFS_GATEWAY\n}\n \n|\n \n2013\n-03-01 \n00\n:14:04 +0000\n\n\n\n\n\n\nEdit the benchmark\u2019s configuration file\n\n\nYou need to modify the values for \nhttp_raw_ips\n and \nhttp_raw_port\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n{\nmode,      max\n}\n.\n\n{\nduration,   \n10\n}\n.\n\n{\nconcurrent, \n50\n}\n.\n\n\n{\ndriver, basho_bench_driver_leofs\n}\n.\n\n{\ncode_paths, \n[\ndeps/ibrowse\n]}\n.\n\n\n{\nhttp_raw_ips, \n[\n${\nHOST_NAME_OF_LEOFS_GATEWAY\n}\n]}\n. %% able to \nset\n plural nodes\n\n{\nhttp_raw_port, \n${\nPORT\n}\n}\n. %% default: \n8080\n\n\n{\nhttp_raw_path, \n/test\n}\n.\n%% \n{\nhttp_raw_path, \n/\n${\nBUCKET\n}\n}\n.\n\n\n{\nkey_generator,   \n{\npartitioned_sequential_int, \n1000000\n}}\n.\n\n{\nvalue_generator, \n{\nfixed_bin, \n16384\n}}\n. %% 16KB\n\n{\noperations, \n[{\nput,1\n}]}\n.               %% PUT:100%\n%%\n{\noperations, \n[{\nput,1\n}\n, \n{\nget, \n4\n}]}\n.   %% PUT:20%, GET:80%\n\n\n{\ncheck_integrity, false\n}\n.\n\n\n\n\n\n\nRunning basho_bench\n\n\n1\n2\n3\n### Loading 1M records each size is 16KB\n\n\ncd\n basho_bench\n./basho_bench ../leofs/test/conf/leofs_16K_LOAD1M.config\n\n\n\n\n\n\nRelated Links\n\n\n\n\nGetting Started / Quick Installation and Setup\n\n\nGetting Started / Building a LeoFS' cluster with Ansible\n\n\nFor Administrators / System Operations / S3-API related Operations", 
            "title": "Benchmark"
        }, 
        {
            "location": "/benchmark/README/#benchmark", 
            "text": "", 
            "title": "Benchmark"
        }, 
        {
            "location": "/benchmark/README/#setting-up-basho_bench", 
            "text": "", 
            "title": "Setting up basho_bench"
        }, 
        {
            "location": "/benchmark/README/#installation", 
            "text": "Basho basho_bench\u2019s repository  Basho basho_bench\u2019s documentation  Use the following commands to set up basho_bench.   1\n2\n3\n4\n5\n6 $ git clone git://github.com/basho/basho_bench.git\n$ git clone https://github.com/leo-project/leofs.git\n$  cd  basho_bench\n$ cp -i ../leofs/test/src/*.erl src/\n$ cp -i ../leofs/test/include/*.hrl include/\n$ make all", 
            "title": "Installation"
        }, 
        {
            "location": "/benchmark/README/#preparations-before-testing", 
            "text": "", 
            "title": "Preparations before testing"
        }, 
        {
            "location": "/benchmark/README/#create-a-test-bucket", 
            "text": "After starting a LeoFS system, you need to create a bucket before getting started with benchmarks. In this example, the bucket name is  test . It is owned by the user  _test_leofs  that is already registered internally by LeoFS.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 $ leofs-adm add-endpoint  gateway-ip-address \nOK\n\n$ leofs-adm add-bucket  test   05236 \nOK\n\n$ leofs-adm get-buckets\nbucket    |  owner        |  created at\n---------+-------------+--------------------------- test       |  _test_leofs  |   2013 -02-27  14 :06:54 +0900\n\n$ leofs-adm update-acl  test   05236  public-read\nOK", 
            "title": "Create a test bucket"
        }, 
        {
            "location": "/benchmark/README/#configuration-file-for-basho_bench", 
            "text": "", 
            "title": "Configuration file for basho_bench"
        }, 
        {
            "location": "/benchmark/README/#an-example", 
            "text": "Some examples are included in LeoFS' repository at  leo-project / leofs / test / conf . If you would like to learn basho_bench's configuration, you can see  BashoBench's Configuration .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 { mode ,        max }.  { duration ,     10 }.  { concurrent ,   50 }.  { driver ,   basho_bench_driver_leofs }.  { code_paths ,   [ deps/ibrowse ]}.  { http_raw_ips ,   [ ${HOST_NAME_OF_LEOFS_GATEWAY} ]}.  { http_raw_port ,   8080 }.  { http_raw_path ,   /test }.  %% {http_raw_path,  /${BUCKET} }.  { key_generator ,     { partitioned_sequential_int ,   1000000 }}.  { value_generator ,   { fixed_bin ,   16384 }}.   %% 16KB  { operations ,   [{ put , 1 }]}.                 %% PUT:100%  %%{operations, [{put,1}, {get, 4}]}.   %% PUT:20%, GET:80%  { check_integrity ,   false }.", 
            "title": "An Example"
        }, 
        {
            "location": "/benchmark/README/#description", 
            "text": "Key  Value      http_raw_ips  The LeoFS Gateway nodes we want to benchmark    http_raw_port  The port used by the LeoFS Gateway nodes    http_raw_path  URL path prefix. The first path segment MUST be a BUCKET name    check_integrity (default:false)  Check integrity of registered object - compare an original MD5 with a retrieved object\u2019s MD5 (Only for developers)", 
            "title": "Description"
        }, 
        {
            "location": "/benchmark/README/#running-basho_bench-1", 
            "text": "In this example, LeoFS and  basho_bench  are installed locally. The following commands can be used to run  basho_bench .  1\n2\n3 ### Loading 1M records of size 16KB  cd  basho_bench\n./basho_bench ../leofs/test/conf/leofs_16K_LOAD1M.config", 
            "title": "Running basho_bench (1)"
        }, 
        {
            "location": "/benchmark/README/#running-basho_bench-2", 
            "text": "In this example, LeoFS and  basho_bench  are installed on different hosts.", 
            "title": "Running basho_bench (2)"
        }, 
        {
            "location": "/benchmark/README/#configure-the-endpoint-on-leomanagers-console", 
            "text": "Allows basho_bench\u2019s requests to reach  ${ HOST_NAME_OF_LEOFS_GATEWAY } .  1\n2\n3\n4\n5\n6\n7\n8\n9 $ leofs-adm add-endpoint  host-name-of-leofs-gateway \nOK\n\n$ leofs-adm get-endpoints\nendpoint                       |  created at\n------------------------------+---------------------------\nlocalhost                      |   2013 -03-01  00 :14:04 +0000\ns3.amazonaws.com               |   2013 -03-01  00 :14:04 +0000 ${ HOST_NAME_OF_LEOFS_GATEWAY }   |   2013 -03-01  00 :14:04 +0000", 
            "title": "Configure the endpoint on LeoManager's console"
        }, 
        {
            "location": "/benchmark/README/#edit-the-benchmarks-configuration-file", 
            "text": "You need to modify the values for  http_raw_ips  and  http_raw_port .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 { mode,      max } . { duration,    10 } . { concurrent,  50 } . { driver, basho_bench_driver_leofs } . { code_paths,  [ deps/ibrowse ]} . { http_raw_ips,  [ ${ HOST_NAME_OF_LEOFS_GATEWAY } ]} . %% able to  set  plural nodes { http_raw_port,  ${ PORT } } . %% default:  8080  { http_raw_path,  /test } .\n%%  { http_raw_path,  / ${ BUCKET } } . { key_generator,    { partitioned_sequential_int,  1000000 }} . { value_generator,  { fixed_bin,  16384 }} . %% 16KB { operations,  [{ put,1 }]} .               %% PUT:100%\n%% { operations,  [{ put,1 } ,  { get,  4 }]} .   %% PUT:20%, GET:80% { check_integrity, false } .", 
            "title": "Edit the benchmark\u2019s configuration file"
        }, 
        {
            "location": "/benchmark/README/#running-basho_bench", 
            "text": "1\n2\n3 ### Loading 1M records each size is 16KB  cd  basho_bench\n./basho_bench ../leofs/test/conf/leofs_16K_LOAD1M.config", 
            "title": "Running basho_bench"
        }, 
        {
            "location": "/benchmark/README/#related-links", 
            "text": "Getting Started / Quick Installation and Setup  Getting Started / Building a LeoFS' cluster with Ansible  For Administrators / System Operations / S3-API related Operations", 
            "title": "Related Links"
        }, 
        {
            "location": "/faq/fundamentals/", 
            "text": "FAQ: LeoFS Fundamentals\n\n\n\n\n\nWhat kind of storage is leofs?\n\n\nLeoFS is a highly scalable, fault-tolerant \nobject_storage\n for the Web. Significantly, LeoFS supports huge amount and various kind unstructured data such as photo, movie, document, log data and so on.\n\n\nOperationally, LeoFS features multi-master replication with automated failover and built-in horizontal scaling via ConsistentHashing.\n\n\n\n\nRelated Links:\n\n\nLeoFS Introduction\n\n\n\n\n\n\n\n\nWhat are typical uses for LeoFS?\n\n\nIf you are searching a storage system that is able to store huge amount and various kind of files such as photo, movie, log data and so on, LeoFS is suitable for that.\n\n\nThis is because LeoFS is a highly available, distributed storage system. Also, LeoFS can be used to store a lot of data efficiently, safely, and inexpensively.\n\n\n\n\nRelated Links:\n\n\nLeoFS Introduction\n\n\n\n\n\n\n\n\nWhat is benefit for LeoFS users?\n\n\nAdvantage\n\n\nLeoFS is supporting the following features:\n\n\n\n\nS3-API Support\n\n\nLeoFS is an Amazon S3 compatible storage system.\n\n\nSwitch to LeoFS to decrease your cost from more expensive public-cloud solutions.\n\n\n\n\n\n\nLarge Object Support\n\n\nLeoFS can handle files with more than GB\n\n\n\n\n\n\nMulti Data Center Replication\n\n\nLeoFS is a highly scalable, fault-tolerant distributed file system without |SPOF|.\n\n\nLeoFS's cluster can be viewed as ONE-HUGE storage. It consists of a set of loosely connected nodes.\n\n\nWe can build a global scale storage system with easy operations\n\n\n\n\n\n\nHigh Performance without |SPOF|\n\n\nAccording to the original cache mechanism and sophisticated innternal architecture, LeoFS keeps high performance regardless of amount and kind of data without |SPOF|.\n\n\n\n\n\n\n\n\nIn near future, LeoFS is going to provide the powerful features with LeoFS v1.4, v1.5 and v2.0\n\n\n\n\nHybrid Storage\n\n\n[v1.0]\n S3-API and REST-API Support\n\n\n[v1.2]\n NFS v3 Support\n\n\n[v1.5]\n LeoFS' Native Client\n\n\n\n\n\n\nReduction of Storage Costs\n\n\n[v1.5]\n Erasure Code\n\n\n[v2.2]\n Data Deduplication\n\n\n\n\n\n\n\n\nFor Business Managers\n\n\n\n\nStoring confidential/sensitive data internally\n\n\nSaving cost to use commodity servers\n\n\nIncreasing service level with speedy response\n\n\nExpanding business globally\n\n\n\n\nFor Administrators\n\n\n\n\nEasy to install with the packages\n\n\nEasy to operate with |LeoCenterDocs|\n\n\n\n\nWhat is architecture of LeoFS?\n\n\nWe've been mainly focusing on \nHigh Availability\n, \nHigh Scalability\n and \nHigh Cost Performance Ratio\n since unstructured data such as images, movies and logs have been exponentially increasing day by day, and we needed to build a cloud storage that can handle all them.\n\n\nLeoFS consists of 3 core components, \nLeoGateway\n, \nLeoStorage\n and \nLeoManager\n. The role of each component is clearly defined.\n\n\n\n\nLeoGateway\n handles http-requests and http-responses from clients when using REST-API OR S3-API. Also, it has the built-in object-cache system.\n\n\nLeoStorage\n handles \nGET\n, \nPUT\n and \nDELETE\n, Also it has replicator and recoverer in order to keep running and consistency.\n\n\nLeoManager\n always monitors Gateway(s) and Storage(s). Manger monitors node-status and RING(logical routing-table) checksum to keep running and consistency.\n\n\nAlso, what we payed attention when we desined LeoFS are the following 3 things:\n\n\n\n\nTo keep always running and No |SPOF|\n\n\nTo keep high-performance, regardless of the kind and amount of data\n\n\nTo provide easy administration, we already provide LeoFS CUI and GUI console.\n\n\n\n\nIs there the roadmap of LeoFS?\n\n\nWe've published LeoFS milestones on \nLeoFS' GitHub\n. We may revise the milestones occasionally because there is a possibility to add new features or change priority of implementation. We'll keep them always updated.\n\n\nWhat language is LeoFS written in?\n\n\nLeoFS depends on \nErlang\n,  and some LeoFS' libraries are implemented by C/C++.\n\n\n\n\nRelated Links:\n\n\nLeoFS Introduction\n\n\nLeoFS Repository on GitHub", 
            "title": "LeoFS Fundamentals"
        }, 
        {
            "location": "/faq/fundamentals/#faq-leofs-fundamentals", 
            "text": "", 
            "title": "FAQ: LeoFS Fundamentals"
        }, 
        {
            "location": "/faq/fundamentals/#what-kind-of-storage-is-leofs", 
            "text": "LeoFS is a highly scalable, fault-tolerant  object_storage  for the Web. Significantly, LeoFS supports huge amount and various kind unstructured data such as photo, movie, document, log data and so on.  Operationally, LeoFS features multi-master replication with automated failover and built-in horizontal scaling via ConsistentHashing.   Related Links:  LeoFS Introduction", 
            "title": "What kind of storage is leofs?"
        }, 
        {
            "location": "/faq/fundamentals/#what-are-typical-uses-for-leofs", 
            "text": "If you are searching a storage system that is able to store huge amount and various kind of files such as photo, movie, log data and so on, LeoFS is suitable for that.  This is because LeoFS is a highly available, distributed storage system. Also, LeoFS can be used to store a lot of data efficiently, safely, and inexpensively.   Related Links:  LeoFS Introduction", 
            "title": "What are typical uses for LeoFS?"
        }, 
        {
            "location": "/faq/fundamentals/#what-is-benefit-for-leofs-users", 
            "text": "", 
            "title": "What is benefit for LeoFS users?"
        }, 
        {
            "location": "/faq/fundamentals/#advantage", 
            "text": "LeoFS is supporting the following features:   S3-API Support  LeoFS is an Amazon S3 compatible storage system.  Switch to LeoFS to decrease your cost from more expensive public-cloud solutions.    Large Object Support  LeoFS can handle files with more than GB    Multi Data Center Replication  LeoFS is a highly scalable, fault-tolerant distributed file system without |SPOF|.  LeoFS's cluster can be viewed as ONE-HUGE storage. It consists of a set of loosely connected nodes.  We can build a global scale storage system with easy operations    High Performance without |SPOF|  According to the original cache mechanism and sophisticated innternal architecture, LeoFS keeps high performance regardless of amount and kind of data without |SPOF|.     In near future, LeoFS is going to provide the powerful features with LeoFS v1.4, v1.5 and v2.0   Hybrid Storage  [v1.0]  S3-API and REST-API Support  [v1.2]  NFS v3 Support  [v1.5]  LeoFS' Native Client    Reduction of Storage Costs  [v1.5]  Erasure Code  [v2.2]  Data Deduplication", 
            "title": "Advantage"
        }, 
        {
            "location": "/faq/fundamentals/#for-business-managers", 
            "text": "Storing confidential/sensitive data internally  Saving cost to use commodity servers  Increasing service level with speedy response  Expanding business globally", 
            "title": "For Business Managers"
        }, 
        {
            "location": "/faq/fundamentals/#for-administrators", 
            "text": "Easy to install with the packages  Easy to operate with |LeoCenterDocs|", 
            "title": "For Administrators"
        }, 
        {
            "location": "/faq/fundamentals/#what-is-architecture-of-leofs", 
            "text": "We've been mainly focusing on  High Availability ,  High Scalability  and  High Cost Performance Ratio  since unstructured data such as images, movies and logs have been exponentially increasing day by day, and we needed to build a cloud storage that can handle all them.  LeoFS consists of 3 core components,  LeoGateway ,  LeoStorage  and  LeoManager . The role of each component is clearly defined.   LeoGateway  handles http-requests and http-responses from clients when using REST-API OR S3-API. Also, it has the built-in object-cache system.  LeoStorage  handles  GET ,  PUT  and  DELETE , Also it has replicator and recoverer in order to keep running and consistency.  LeoManager  always monitors Gateway(s) and Storage(s). Manger monitors node-status and RING(logical routing-table) checksum to keep running and consistency.  Also, what we payed attention when we desined LeoFS are the following 3 things:   To keep always running and No |SPOF|  To keep high-performance, regardless of the kind and amount of data  To provide easy administration, we already provide LeoFS CUI and GUI console.", 
            "title": "What is architecture of LeoFS?"
        }, 
        {
            "location": "/faq/fundamentals/#is-there-the-roadmap-of-leofs", 
            "text": "We've published LeoFS milestones on  LeoFS' GitHub . We may revise the milestones occasionally because there is a possibility to add new features or change priority of implementation. We'll keep them always updated.", 
            "title": "Is there the roadmap of LeoFS?"
        }, 
        {
            "location": "/faq/fundamentals/#what-language-is-leofs-written-in", 
            "text": "LeoFS depends on  Erlang ,  and some LeoFS' libraries are implemented by C/C++.   Related Links:  LeoFS Introduction  LeoFS Repository on GitHub", 
            "title": "What language is LeoFS written in?"
        }, 
        {
            "location": "/faq/limits/", 
            "text": "FAQ: LeoFS Limits\n\n\n\n\n\nFeatures\n\n\n\n\nLeoFS have covered almost major \nAWS S3-API\n but not all APIs.\n\n\nIf you use \nS3's Multi Part Upload API\n, the size of a part of an object must be less than the size of a chunked object in LeoFS.\n\n\n\n\nWhen using the multi datacenter replication feature, we have supported up to 2 clusters with \nLeoFS v1.2\n, but we're going to support over 3 clusters replication with \nLeoFS v2.0\n.\n\n\n\n\n\n\nRelated Links:\n\n\n\n\nAmazon S3 REST API Introduction\n\n\nLeoFS' ISSUE_177\n\n\nLeoFS' ISSUE_338\n\n\n\n\n\n\n\n\nOperations\n\n\n\n\n\n\nWhen you upgrade your LeoFS, you can NOT change the metadata storage as |KVS| - \nbitcask\n or \nleveldb\n can be used in LeoFS - used by LeoFS Storage. We recommend users to replace \nbitcask\n with \nleveldb\n by using |leofs_utils/tools/b2l|.\n\n\n\n\n\n\nRelated Links:\n\n\n\n\nConfiguration\n\n\n\n\n\n\n\n\nNFS Support\n\n\n\n\nNFS implemantation with \nLeoFS v1.1\n is a subset of \nNFS v3\n. \nAuthentication\n, and \nOwner/Permission\n management are NOT covered.\n\n\nThe \nls\n command may take too much time when the target directory have lots of child. We're planning to provide better performance with LeoFS v.2.0.\n\n\nIf you use LeoFS with NFS, you should set the size of a chunked object in LeoFS to 1MB (1048576Bytes) - \nlarge_object.chunked_obj_len = 1048576\n \n(leo_gateway.conf)\n, otherwise the efficiency of disk utilization can be decreased.\n\n\n\n\nWhat are the requirements to run LeoFS with NFS?\n\n\n\n\nLeoGateway (NFS Server):\n\n\nWe've supporeted the targets Debian 6, Ubuntu-Server 14.04 LTS or Higher and CentOS 6.5/7.0 as LeoFS does, but should work on most linux platforms. In addition, We've confirmed LeoFS with NFS works properly on the latest FreeBSD and SmartOS by using \nNFS Integration Test Tool\n.\n\n\n\n\n\n\n\n\n\n\n\n\nRelated Links:\n\n\n\n\nConfiguration", 
            "title": "LeoFS Limits"
        }, 
        {
            "location": "/faq/limits/#faq-leofs-limits", 
            "text": "", 
            "title": "FAQ: LeoFS Limits"
        }, 
        {
            "location": "/faq/limits/#features", 
            "text": "LeoFS have covered almost major  AWS S3-API  but not all APIs.  If you use  S3's Multi Part Upload API , the size of a part of an object must be less than the size of a chunked object in LeoFS.   When using the multi datacenter replication feature, we have supported up to 2 clusters with  LeoFS v1.2 , but we're going to support over 3 clusters replication with  LeoFS v2.0 .    Related Links:   Amazon S3 REST API Introduction  LeoFS' ISSUE_177  LeoFS' ISSUE_338", 
            "title": "Features"
        }, 
        {
            "location": "/faq/limits/#operations", 
            "text": "When you upgrade your LeoFS, you can NOT change the metadata storage as |KVS| -  bitcask  or  leveldb  can be used in LeoFS - used by LeoFS Storage. We recommend users to replace  bitcask  with  leveldb  by using |leofs_utils/tools/b2l|.    Related Links:   Configuration", 
            "title": "Operations"
        }, 
        {
            "location": "/faq/limits/#nfs-support", 
            "text": "NFS implemantation with  LeoFS v1.1  is a subset of  NFS v3 .  Authentication , and  Owner/Permission  management are NOT covered.  The  ls  command may take too much time when the target directory have lots of child. We're planning to provide better performance with LeoFS v.2.0.  If you use LeoFS with NFS, you should set the size of a chunked object in LeoFS to 1MB (1048576Bytes) -  large_object.chunked_obj_len = 1048576   (leo_gateway.conf) , otherwise the efficiency of disk utilization can be decreased.   What are the requirements to run LeoFS with NFS?   LeoGateway (NFS Server):  We've supporeted the targets Debian 6, Ubuntu-Server 14.04 LTS or Higher and CentOS 6.5/7.0 as LeoFS does, but should work on most linux platforms. In addition, We've confirmed LeoFS with NFS works properly on the latest FreeBSD and SmartOS by using  NFS Integration Test Tool .       Related Links:   Configuration", 
            "title": "NFS Support"
        }, 
        {
            "location": "/faq/administration/", 
            "text": "FAQ: LeoFS Administration\n\n\n\n\n\nWhere can I get the packages for LeoFS?\n\n\nYou can get the packages by platform from \nLeoFS website\n, and LeoFS package for FreeBSD has been published at \nFreeBSD Fresh Ports\n.\n\n\nHow can I run my LeoFS cluster automatically?\n\n\nCurrently after restarting a storage node,  you need to issue the resume command with leofs-adm script manually like this.\n\n\n1\n$ leofs-adm resume storage_0@127.0.0.1\n\n\n\n\n\n\nWe're planing to provide LeoFS's automated operations like \nauto-compaction\n, \nauto-rebalance\n and others with LeoFS v1.2, v1.4 and v2.0. Actually, \nauto-compaction\n which was already supported with v1.2.\n\n\nHow do multiple users login into LeoFS Manager's console at the same time?\n\n\nAnswer 1:\n\n\nActually, there is no login status in LeoFS, but the number of listening tcp connections is limited by \nleo_manger.conf\n \ndefault\n:\n \nconsole\n.\nacceptors\n.\ncui\n \n=\n \n3\n.\nSo while using default settings, 3 connections can be connected to a manager in parallel.\n\n\nAnswer 2:\n\n\nSince \nLeoFS v1.1.0\n, LeoFS have the more powerful alternative option \nleofs-adm\n command.\n\n\nThis command have not only same functionalities with the existing telnet way but also do NOT keep an tcp connection established for long time (connect only when issueing a command). You don't need to worry about the number of tcp connections.\n\n\nThe result of the du command can be different with the actual disk-usage\n\n\nIn order to reduce system resource usage for calculating the result of the \ndu\n command, LeoFS keep that information on memory and when stopping itself, LeoFS save those data into a local file. Then when restarting, LeoFS load those on memory.\n\n\nSo if LeoFS is stopped unintentionally like killing by OOM killer, those data can become inconsistent with actual usage.\n\n\nIf you get into this situation, you can recover those data by issueing the \ncompact-start\n command to the node. after finishing the compaction, the result of the du command will be consitent with actual usage.\n\n\nWhen issueing the recover node command the LeoFS can get into high load\n\n\nSince the \nrecover-node\n command can lead to issue lots of disk I/O and consume network bandwidth, if you face to issue the recover-node to multiple nodes at once, LeoFS can get into high load and become unresponsive. So we recommend you execute the recover-node command to target nodes one by one.\n\n\nIf this solution could not work for you, you're able to control how much recover-node consume system resources by changing \nthe MQ-related parameters in leo_storage.conf\n.\n\n\nWhat should I do when Too many processes errors happen?\n\n\nLeoFS usually try to keep the number of Erlang processes as minimum as possible, but there are some exceptions when doing something asynchronously.\n\n\n\n\nReplicating an object to the non-primary assigned nodes\n\n\nRetrying to replicate an object when the previous attempt failed\n\n\n\n\nGiven that LeoFS suffered from very high load AND there are some nodes downed for some reason, The number of Erlang processes gradually have increased and might have reached the sysmte limit.\n\n\nWe recommend users to set an appropriate value which depends on your workload to the \n+P option\n. Also if the \n+P option\n does NOT work for you, there are some possibilities that some external system resources like disk, network equipments have broken, Please check out the dmesg/syslog on your sysmtem.\n\n\nHow do I set \"a number of containers\" at LeoFS Storage configuration?\n\n\nObjects/files are stored into LeoFS Storage containers which are log-structured files. LeoFS has the \ndata-compaction\n mechanism in order to remove unncessary objects/files from the object-containers.\n\n\nLeoFS's performance is affected by the data-compaction. And also, LeoFS Storage temporally creates a new file of a object-container corresponding to the compaction target container, which means during the data-compaction needs disk space for the new file of object-container(s).\n\n\nIf \nwrite, update and delete operation\n is a lot, we recommend that the number of containers is 32 OR 64 because it's possible to make effect of the data-compaction at a minimum as much as possible.\n\n\nIn conclusiton:\n\n\n\n\nRead operation \n Write operation:\n\n\nA total number of containers\n = 8\n\n\n\n\n\n\nA lot of write, update and delete operation:\n\n\nA total number of containers\n = 32 OR 64 \n(depends on the disk capacity and performance)\n\n\n\n\n\n\n\n\nleo_storage can not start due to \"enif_send_failed on non smp vm\"\n\n\nWhen starting \nleo_storage\n on a single core machine which crashes with an erl_nif error.\n\n\n1\n2\n3\n## Error log\n\nenif_send: \nenv\n==\nNULL on non-SMP VM\nAborted \n(\ncore dumped\n)\n\n\n\n\n\n\n\nIn this case, you have faced with \nLeoFS Issue#320\n. You need to set a Erlang's VM flag, \n-smp\n in your \nleo_storage.conf\n as follows:\n\n\n1\n2\n## leo_storage.conf\n\nerlang.smp \n=\n \nenable\n\n\n\n\n\n\n\nWhy is the speed of rebalance/recover command too slow?\n\n\nYou must hit \nLeoFS Issue#359\n. Since this issue has been fixed with \nLeoFS v1.2.9\n, we'd recommend you upgrading to the v1.2.9 or higher one, and set an appropriate value for your environment to \nmq.num_of_mq_procs\n in your leo_storage.conf.\n\n\nWhy does LeoFS's SNMP I/F give me wrong values(0) instead of correct values?\n\n\nYou must hit \nLeoFS Issue#361\n. Since this issue has been fixed with \nLeoFS v1.2.9\n, we'd recommend you upgrading to the v1.2.9 or higher one.\n\n\nWhen adding a new storage node, that node doesn't appear with \nleofs-adm status\n, Why?\n\n\nIf you changed a WRONG node name before stopping the daemon, As a result, when a new daemon was starting, it failed to detect that the previous was still running and\nstop command did not work too.\n\n\nSince you can notice this kind of mistake in \nerror.log\n with \nLeoFS v1.2.9\n, we'd recommend you upgrading to the v1.2.9 or higher one.", 
            "title": "LeoFS Administration"
        }, 
        {
            "location": "/faq/administration/#faq-leofs-administration", 
            "text": "", 
            "title": "FAQ: LeoFS Administration"
        }, 
        {
            "location": "/faq/administration/#where-can-i-get-the-packages-for-leofs", 
            "text": "You can get the packages by platform from  LeoFS website , and LeoFS package for FreeBSD has been published at  FreeBSD Fresh Ports .", 
            "title": "Where can I get the packages for LeoFS?"
        }, 
        {
            "location": "/faq/administration/#how-can-i-run-my-leofs-cluster-automatically", 
            "text": "Currently after restarting a storage node,  you need to issue the resume command with leofs-adm script manually like this.  1 $ leofs-adm resume storage_0@127.0.0.1   We're planing to provide LeoFS's automated operations like  auto-compaction ,  auto-rebalance  and others with LeoFS v1.2, v1.4 and v2.0. Actually,  auto-compaction  which was already supported with v1.2.", 
            "title": "How can I run my LeoFS cluster automatically?"
        }, 
        {
            "location": "/faq/administration/#how-do-multiple-users-login-into-leofs-managers-console-at-the-same-time", 
            "text": "", 
            "title": "How do multiple users login into LeoFS Manager's console at the same time?"
        }, 
        {
            "location": "/faq/administration/#answer-1", 
            "text": "Actually, there is no login status in LeoFS, but the number of listening tcp connections is limited by  leo_manger.conf   default :   console . acceptors . cui   =   3 .\nSo while using default settings, 3 connections can be connected to a manager in parallel.", 
            "title": "Answer 1:"
        }, 
        {
            "location": "/faq/administration/#answer-2", 
            "text": "Since  LeoFS v1.1.0 , LeoFS have the more powerful alternative option  leofs-adm  command.  This command have not only same functionalities with the existing telnet way but also do NOT keep an tcp connection established for long time (connect only when issueing a command). You don't need to worry about the number of tcp connections.", 
            "title": "Answer 2:"
        }, 
        {
            "location": "/faq/administration/#the-result-of-the-du-command-can-be-different-with-the-actual-disk-usage", 
            "text": "In order to reduce system resource usage for calculating the result of the  du  command, LeoFS keep that information on memory and when stopping itself, LeoFS save those data into a local file. Then when restarting, LeoFS load those on memory.  So if LeoFS is stopped unintentionally like killing by OOM killer, those data can become inconsistent with actual usage.  If you get into this situation, you can recover those data by issueing the  compact-start  command to the node. after finishing the compaction, the result of the du command will be consitent with actual usage.", 
            "title": "The result of the du command can be different with the actual disk-usage"
        }, 
        {
            "location": "/faq/administration/#when-issueing-the-recover-node-command-the-leofs-can-get-into-high-load", 
            "text": "Since the  recover-node  command can lead to issue lots of disk I/O and consume network bandwidth, if you face to issue the recover-node to multiple nodes at once, LeoFS can get into high load and become unresponsive. So we recommend you execute the recover-node command to target nodes one by one.  If this solution could not work for you, you're able to control how much recover-node consume system resources by changing  the MQ-related parameters in leo_storage.conf .", 
            "title": "When issueing the recover node command the LeoFS can get into high load"
        }, 
        {
            "location": "/faq/administration/#what-should-i-do-when-too-many-processes-errors-happen", 
            "text": "LeoFS usually try to keep the number of Erlang processes as minimum as possible, but there are some exceptions when doing something asynchronously.   Replicating an object to the non-primary assigned nodes  Retrying to replicate an object when the previous attempt failed   Given that LeoFS suffered from very high load AND there are some nodes downed for some reason, The number of Erlang processes gradually have increased and might have reached the sysmte limit.  We recommend users to set an appropriate value which depends on your workload to the  +P option . Also if the  +P option  does NOT work for you, there are some possibilities that some external system resources like disk, network equipments have broken, Please check out the dmesg/syslog on your sysmtem.", 
            "title": "What should I do when Too many processes errors happen?"
        }, 
        {
            "location": "/faq/administration/#how-do-i-set-a-number-of-containers-at-leofs-storage-configuration", 
            "text": "Objects/files are stored into LeoFS Storage containers which are log-structured files. LeoFS has the  data-compaction  mechanism in order to remove unncessary objects/files from the object-containers.  LeoFS's performance is affected by the data-compaction. And also, LeoFS Storage temporally creates a new file of a object-container corresponding to the compaction target container, which means during the data-compaction needs disk space for the new file of object-container(s).  If  write, update and delete operation  is a lot, we recommend that the number of containers is 32 OR 64 because it's possible to make effect of the data-compaction at a minimum as much as possible.", 
            "title": "How do I set \"a number of containers\" at LeoFS Storage configuration?"
        }, 
        {
            "location": "/faq/administration/#in-conclusiton", 
            "text": "Read operation   Write operation:  A total number of containers  = 8    A lot of write, update and delete operation:  A total number of containers  = 32 OR 64  (depends on the disk capacity and performance)", 
            "title": "In conclusiton:"
        }, 
        {
            "location": "/faq/administration/#leo_storage-can-not-start-due-to-enif_send_failed-on-non-smp-vm", 
            "text": "When starting  leo_storage  on a single core machine which crashes with an erl_nif error.  1\n2\n3 ## Error log \nenif_send:  env == NULL on non-SMP VM\nAborted  ( core dumped )    In this case, you have faced with  LeoFS Issue#320 . You need to set a Erlang's VM flag,  -smp  in your  leo_storage.conf  as follows:  1\n2 ## leo_storage.conf \nerlang.smp  =   enable", 
            "title": "leo_storage can not start due to \"enif_send_failed on non smp vm\""
        }, 
        {
            "location": "/faq/administration/#why-is-the-speed-of-rebalancerecover-command-too-slow", 
            "text": "You must hit  LeoFS Issue#359 . Since this issue has been fixed with  LeoFS v1.2.9 , we'd recommend you upgrading to the v1.2.9 or higher one, and set an appropriate value for your environment to  mq.num_of_mq_procs  in your leo_storage.conf.", 
            "title": "Why is the speed of rebalance/recover command too slow?"
        }, 
        {
            "location": "/faq/administration/#why-does-leofss-snmp-if-give-me-wrong-values0-instead-of-correct-values", 
            "text": "You must hit  LeoFS Issue#361 . Since this issue has been fixed with  LeoFS v1.2.9 , we'd recommend you upgrading to the v1.2.9 or higher one.", 
            "title": "Why does LeoFS's SNMP I/F give me wrong values(0) instead of correct values?"
        }, 
        {
            "location": "/faq/administration/#when-adding-a-new-storage-node-that-node-doesnt-appear-with-leofs-adm-status-why", 
            "text": "If you changed a WRONG node name before stopping the daemon, As a result, when a new daemon was starting, it failed to detect that the previous was still running and\nstop command did not work too.  Since you can notice this kind of mistake in  error.log  with  LeoFS v1.2.9 , we'd recommend you upgrading to the v1.2.9 or higher one.", 
            "title": "When adding a new storage node, that node doesn't appear with leofs-adm status, Why?"
        }, 
        {
            "location": "/faq/client/", 
            "text": "FAQ: LeoFS Clients\n\n\n\n\n\nName Resolution\n\n\n\n\nThere are two ways to access LeoGateway from LeoFS' Clients.\n\n\nPath Style Access\n\n\nSub-Domain Style Access\n\n\n\n\n\n\n\n\nIf a LeoFS Client you use adopts the sub-domain style access, then the client need to resolve \nbucket.endpoint\n domain into the IP address of LeoGateway.\n\n\n\n\n\n\nRelated Links:\n\n\n\n\nA Comment on LeoFS' ISSUE_748\n\n\nFor Administrators / System Operations / S3 / Endpoint", 
            "title": "LeoFS Clients"
        }, 
        {
            "location": "/faq/client/#faq-leofs-clients", 
            "text": "", 
            "title": "FAQ: LeoFS Clients"
        }, 
        {
            "location": "/faq/client/#name-resolution", 
            "text": "There are two ways to access LeoGateway from LeoFS' Clients.  Path Style Access  Sub-Domain Style Access     If a LeoFS Client you use adopts the sub-domain style access, then the client need to resolve  bucket.endpoint  domain into the IP address of LeoGateway.    Related Links:   A Comment on LeoFS' ISSUE_748  For Administrators / System Operations / S3 / Endpoint", 
            "title": "Name Resolution"
        }, 
        {
            "location": "/production_checklist/README/", 
            "text": "Production Checklist: LeoFS\n\n\n\n\n\nWhat version should we use?\n\n\nUse the latest stable one. With the version \n= v1.3.0, LeoFS had a serious issue that may cause data-lost, so that use at least \n= v1.3.1. Or in case you need to keep running LeoFS with older one for some reason, make sure that \nlarge_object.reading_chunked_obj_len\n \n= \nlarge_object.chunked_obj_len\n in \nleo_gateway.conf\n. This setting prevent LeoFS from suffering \nLeoFS Issue#531\n.\n\n\nThe last part of a large object can be broken with \nreading_chunked_obj_len\n \n \nchunked_obj_len\n in \nleo_gateway.conf\n.", 
            "title": "Production Checklist"
        }, 
        {
            "location": "/production_checklist/README/#production-checklist-leofs", 
            "text": "", 
            "title": "Production Checklist: LeoFS"
        }, 
        {
            "location": "/production_checklist/README/#what-version-should-we-use", 
            "text": "Use the latest stable one. With the version  = v1.3.0, LeoFS had a serious issue that may cause data-lost, so that use at least  = v1.3.1. Or in case you need to keep running LeoFS with older one for some reason, make sure that  large_object.reading_chunked_obj_len   =  large_object.chunked_obj_len  in  leo_gateway.conf . This setting prevent LeoFS from suffering  LeoFS Issue#531 .  The last part of a large object can be broken with  reading_chunked_obj_len     chunked_obj_len  in  leo_gateway.conf .", 
            "title": "What version should we use?"
        }
    ]
}